# -*- coding: utf-8 -*-
"""nlp_hw2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J4gJhh5NTFqvdcCNzHGepqqE9C_7qxj7
"""

import wandb
import time
import math
import numpy as np
import pandas as pd
import torch
import torch.nn
import torch.nn.utils.rnn
import torch.utils.data
import matplotlib.pyplot as plt
import seaborn as sns
import opencc
import os
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from copy import deepcopy

import time
import random

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"\n\nUsing random seed {seed}")


#set_seed(int(time.time()))
set_seed()

# Data path configuration
data_path = "./data"

# Load datasets
df_train = pd.read_csv(os.path.join(data_path, "arithmetic_train.csv"))
df_eval = pd.read_csv(os.path.join(data_path, "arithmetic_eval.csv"))

# Transform the input data to string
df_train["tgt"] = df_train["tgt"].apply(lambda x: str(x))
df_train["src"] = df_train["src"].add(df_train["tgt"])
df_train["len"] = df_train["src"].apply(lambda x: len(x))

df_eval["tgt"] = df_eval["tgt"].apply(lambda x: str(x))


# Build Dictionary
# The model cannot perform calculations directly with plain text.
# Convert all text (numbers/symbols) into numerical representations.
char_to_id = {}
id_to_char = {}

all_chars = [
    "0",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "+",
    "-",
    "*",
    "(",
    ")",
    "=",
]
special_tokens = ["<pad>", "<eos>"]
all_tokens = special_tokens + all_chars

char_to_id = {char: idx for idx, char in enumerate(all_tokens)}
id_to_char = {idx: char for idx, char in enumerate(all_tokens)}

vocab_size = len(char_to_id)
print("Vocab size: {}".format(vocab_size))


# Data Preprocessing
def data_preprocess(df: pd.DataFrame, char_to_id: dict) -> pd.DataFrame:
    df = df.copy()
    char_id_list = []
    label_id_list = []

    for sent in df["src"]:
        sent = sent.split("=")
        sent_train = sent[0]
        sent_tgt = sent[1]

        char_id = []
        label_id = []

        # Input part before '='
        for char in sent_train:
            char_id.append(char_to_id[char])
            label_id.append(char_to_id["<pad>"])

        # '=' symbol
        char_id.append(char_to_id["="])
        label_id.append(char_to_id[sent_tgt[0]])

        # Answer part - each position predicts NEXT character
        for i, char in enumerate(sent_tgt):
            char_id.append(char_to_id[char])
            if i < len(sent_tgt) - 1:
                label_id.append(char_to_id[sent_tgt[i + 1]])
            else:
                label_id.append(char_to_id["<eos>"])

        #   0     +     0   =   0   <eos>
        # <pad> <pad> <pad> 0 <eos> <pad>

        # End of sequence token
        char_id.append(char_to_id["<eos>"])
        label_id.append(char_to_id["<pad>"])

        char_id_list.append(char_id)
        label_id_list.append(label_id)

    df["char_id_list"] = char_id_list
    df["label_id_list"] = label_id_list

    return df

df_train = data_preprocess(df_train, char_to_id)

df_train.head()

# Hyperparameter configuration
default_config = {"adamw_lr": 0.001, "weight_decay": 0.01}

batch_size = 64
epochs = 2
embed_dim = 256
hidden_dim = 256
grad_clip = 1

# Initialize wandb
wandb.init(project="nlp-hw2-arithmetic", config=default_config)
config = wandb.config


# Dataset class
class Dataset(torch.utils.data.Dataset):
    def __init__(self, sequences):
        self.sequences = sequences

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, index):
        x = self.sequences.iloc[index]["char_id_list"]
        y = self.sequences.iloc[index]["label_id_list"]
        return x, y


# Collate function for DataLoader
def collate_fn(batch):
    batch_x = [torch.tensor(data[0]) for data in batch]
    batch_y = [torch.tensor(data[1]) for data in batch]
    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])
    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])

    # Pad sequences to the same length
    pad_batch_x = torch.nn.utils.rnn.pad_sequence(
        batch_x, batch_first=True, padding_value=char_to_id["<pad>"]
    )

    pad_batch_y = torch.nn.utils.rnn.pad_sequence(
        batch_y, batch_first=True, padding_value=char_to_id["<pad>"]
    )

    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens


"""
# Create DataLoader
if config.train_mode == "3digit_train_2digit_eval":
    df_train = df_train_3digit
    df_eval = df_eval_2digit
elif config.train_mode == "noisy_20_train":
    df_train = df_train_noise
"""
ds_train = Dataset(df_train[["char_id_list", "label_id_list"]])
# dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
dl_train = torch.utils.data.DataLoader(
    ds_train,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=os.cpu_count(),
    pin_memory=True,
    persistent_workers=True,
)


# Model Definition
class CharRNN(torch.nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super(CharRNN, self).__init__()

        self.embedding = torch.nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim,
            padding_idx=char_to_id["<pad>"],
        )

        self.rnn_layer1 = torch.nn.LSTM(
            input_size=embed_dim, hidden_size=hidden_dim, batch_first=True
        )
        self.rnn_layer2 = torch.nn.LSTM(
            input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True
        )

        self.linear = torch.nn.Sequential(
            torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size),
        )

    def forward(self, batch_x, batch_x_lens):
        return self.encoder(batch_x, batch_x_lens)

    def encoder(self, batch_x, batch_x_lens):
        # Embedding layer
        batch_x = self.embedding(batch_x)

        # Pack padded sequence for efficient RNN processing
        batch_x = torch.nn.utils.rnn.pack_padded_sequence(
            batch_x, batch_x_lens, batch_first=True, enforce_sorted=False
        )

        # RNN layers
        batch_x, _ = self.rnn_layer1(batch_x)
        batch_x, _ = self.rnn_layer2(batch_x)

        # Unpack sequence
        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x, batch_first=True)

        # Linear output layer
        batch_x = self.linear(batch_x)

        return batch_x

    def generator(self, start_char, max_len=200):
        """
        Autoregressive generation function
        """
        char_list = [char_to_id[c] for c in start_char]
        next_char = None

        device = next(self.parameters()).device
        while len(char_list) < max_len:
            # Prepare input tensor
            batch_x = torch.tensor(char_list).unsqueeze(0).to(device)
            batch_x_len = torch.tensor([len(char_list)])

            # Forward pass
            y = self.forward(batch_x, batch_x_len)

            # Get logits for the last position
            logits = y[0, -1]

            # Predict next character
            next_char = torch.argmax(logits, dim=-1).item()

            # Check for end of sequence
            if next_char == char_to_id["<eos>"]:
                break

            char_list.append(next_char)

        return [id_to_char[ch_id] for ch_id in char_list]


# Set random seed for reproducibility
#torch.manual_seed(SEED)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize model
model = CharRNN(vocab_size, embed_dim, hidden_dim)

# Loss function and optimizer
criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id["<pad>"])
optimizers = torch.optim.AdamW(model.parameters(), lr=config.adamw_lr, weight_decay=config.weight_decay)

# Training Loop
print(f"\n\nUsing device: {device}")
model = model.to(device)
i = 0
best_accuracy = 0.0
global_samples = 0
for epoch in range(1, epochs + 1):
    # Training phase
    model.train()
    bar = tqdm(dl_train, desc=f"Train epoch {epoch}")
    for batch_x, batch_y, batch_x_lens, batch_y_lens in bar:
        # Clear gradients
        global_samples += batch_x.size(0)
        batch_x = batch_x.to(device, non_blocking=True)
        batch_y = batch_y.to(device, non_blocking=True)

        optimizer.zero_grad()

        # Forward pass
        batch_pred_y = model(batch_x, batch_x_lens)

        # Compute loss (Teacher Forcing)
        loss = criterion(batch_pred_y.view(-1, vocab_size), batch_y.view(-1))

        # Backward pass
        loss.backward()

        # Gradient clipping

        raw_grad_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                raw_grad_norm += param_norm.item() ** 2
        raw_grad_norm = raw_grad_norm**0.5
        torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=grad_clip)

        # Update parameters
        optimizer.step()

        i += 1
        if i % 50 == 0:
            bar.set_postfix(loss=loss.item())
            wandb.log(
                {
                    "train_loss": loss.item(),
                    "raw_grad_norm": raw_grad_norm,
                    "batch_perplexity": torch.exp(loss).item(),
                },
                step=global_samples,
            )

    # Evaluation phase
    model.eval()
    matched = 0
    total = 0
    examples = []  # Store prediction examples
    bar_eval = tqdm(
        df_eval.sample(frac=0.1, random_state=SEED).iterrows(),
        desc=f"Validation epoch {epoch}",
    )
    print("\n")
    with torch.no_grad():
        for _, row in bar_eval:
            batch_x = row["src"]
            batch_y = row["tgt"]

            # Extract question part (before '=')
            batch_x = batch_x.split("=")[0] + "="

            # Generate prediction
            prediction = "".join(model.generator(batch_x, max_len=50))
            prediction = prediction.split("=")[-1].replace("<eos>", "")
            if total < 5:
                print(
                    f"[{total}] Input: {batch_x:<20} | Pred: {prediction:>8} | GT: {batch_y:>8} | {'✓' if prediction == batch_y else '✗'}"
                )

            # Check correctness
            is_correct = int(prediction == batch_y)
            matched += is_correct
            total += 1

            # Store first 10 examples for logging
            if total <= 10:
                examples.append(
                    {
                        "input": batch_x,
                        "prediction": prediction,
                        "ground_truth": batch_y,
                        "correct": is_correct,
                    }
                )

    # Calculate accuracy
    accuracy = matched / total
    print(f"Epoch {epoch} - Validation Accuracy: {accuracy:.4f} ({matched}/{total})")

    # Log to wandb
    wandb.log(
        {
            "val_accuracy": accuracy,
            "val_correct": matched,
            "examples": wandb.Table(dataframe=pd.DataFrame(examples)),
        }
    )

    # Track best accuracy
    if accuracy > best_accuracy:
        best_accuracy = accuracy

    # Set model back to training mode
    model.train()

# Log final summary
wandb.run.summary.update(
    {
        "best_val_accuracy": best_accuracy,
        "total_params": sum(p.numel() for p in model.parameters()),
        "trainable_params": sum(
            p.numel() for p in model.parameters() if p.requires_grad
        ),
    }
)

wandb.finish()

print(f"\nTraining completed. Best validation accuracy: {best_accuracy:.4f}")
