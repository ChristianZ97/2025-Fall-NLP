# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oNDVdlGNuFn9Xu114uK8Qi0Z8OClcdYz
"""

#! pip install evaluate
#! pip install datasets==2.21.0
#! pip install git+https://github.com/KellerJordan/Muon

# from google.colab import userdata
from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel
from datasets import load_dataset
from evaluate import load
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
#  You can install and import any other libraries if needed

from muon import SingleDeviceMuon
import wandb
import time
import numpy as np
import random
import os
from torch.cuda.amp import autocast

os.makedirs("./saved_models", exist_ok=True)


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"\n\nUsing random seed {seed}")


set_seed()

# from huggingface_hub import login
# login(token=userdata.get("HF_TOKEN"))

# Some Chinese punctuations will be tokenized as [UNK], so we replace them with English ones
token_replacement = [
    ["：", ":"],
    ["，", ","],
    ["“", '"'],
    ["”", '"'],
    ["？", "?"],
    ["……", "..."],
    ["！", "!"],
]

tokenizer = BertTokenizer.from_pretrained(
    "google-bert/bert-base-uncased", cache_dir="./cache/"
)
# tokenizer = RobertaTokenizer.from_pretrained("FacebookAI/roberta-base", cache_dir="./cache/")


class SemevalDataset(Dataset):
    def __init__(self, split="train") -> None:
        super().__init__()
        assert split in ["train", "validation", "test"]
        self.data = load_dataset(
            "sem_eval_2014_task_1",
            split=split,
            trust_remote_code=True,
            cache_dir="./cache/",
        ).to_list()

    def __getitem__(self, index):
        d = self.data[index]
        # Replace Chinese punctuations with English ones
        for k in ["premise", "hypothesis"]:
            for tok in token_replacement:
                d[k] = d[k].replace(tok[0], tok[1])
        return d

    def __len__(self):
        return len(self.data)


# data_sample = SemevalDataset(split="train").data[:3]
# print(f"Dataset example: \n{data_sample[0]} \n{data_sample[1]} \n{data_sample[2]}")

# Hyperparameter configuration
default_config = {
    "muon_lr": 0.000570946127776095,
    "adamw_lr": 0.000144505377143309,
    "alpha": 0.05,
    "dropout_rate": 0.05,
    "batch_size": 32,
    "muon_weight_decay": 0.0330037215159045,
    "adamw_weight_decay": 0.0352225102350684,
    "muon_momentum": 0.95,
}

wandb.init(
    project="nlp-hw3-multi-output",
    config=default_config,
)
config = wandb.config
save_dir = f"./saved_models/{wandb.run.id}"
os.makedirs(save_dir, exist_ok=True)

# Define the hyperparameters
# You can modify these values if needed
# lr = 3e-5
epochs = 3
train_batch_size = config.batch_size
validation_batch_size = 256

# TODO1: Create batched data for DataLoader
# `collate_fn` is a function that defines how the data batch should be packed.
# This function will be called in the DataLoader to pack the data batch.


def collate_fn(batch):
    # TODO1-1: Implement the collate_fn function
    # Write your code here
    # The input parameter is a data batch (tuple), and this function packs it into tensors.
    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.
    # Return the data batch and labels for each sub-task.

    pair_ids = [item["sentence_pair_id"] for item in batch]
    premises = [item["premise"] for item in batch]
    hypotheses = [item["hypothesis"] for item in batch]
    relatedness_labels = [item["relatedness_score"] for item in batch]
    entailment_labels = [item["entailment_judgment"] for item in batch]

    encoded = tokenizer(
        premises,
        hypotheses,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

    relatedness_tensor = torch.tensor(relatedness_labels, dtype=torch.float)
    entailment_tensor = torch.tensor(entailment_labels, dtype=torch.long)

    return {
        "sentence_pair_id": pair_ids,
        "input_ids": encoded["input_ids"],
        "attention_mask": encoded["attention_mask"],
        "token_type_ids": encoded["token_type_ids"],
        "relatedness_score": relatedness_tensor,
        "entailment_judgment": entailment_tensor,
    }


# TODO1-2: Define your DataLoader
# dl_train = # Write your code here
# dl_validation = # Write your code here
# dl_test = # Write your code here

dl_train = DataLoader(
    SemevalDataset(split="train"),
    batch_size=train_batch_size,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=min(4, os.cpu_count()),
    pin_memory=True,
    persistent_workers=True,
)
dl_validation = DataLoader(
    SemevalDataset(split="validation"),
    batch_size=validation_batch_size,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=min(4, os.cpu_count()),
    persistent_workers=True,
)
dl_test = DataLoader(
    SemevalDataset(split="test"),
    batch_size=validation_batch_size,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=min(4, os.cpu_count()),
    persistent_workers=True,
)


# TODO2: Construct your model
class MultiLabelModel(torch.nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Write your code here
        # Define what modules you will use in the model
        # Please use "google-bert/bert-base-uncased" model (https://huggingface.co/google-bert/bert-base-uncased)
        # Besides the base model, you may design additional architectures by incorporating linear layers, activation functions, or other neural components.
        # Remark: The use of any additional pretrained language models is not permitted.
        self.bert = BertModel.from_pretrained(
            "google-bert/bert-base-uncased", cache_dir="./cache/"
        )
        # self.roberta = RobertaModel.from_pretrained("FacebookAI/roberta-base", cache_dir="./cache/")
        hidden_size = self.bert.config.hidden_size
        # hidden_size = self.roberta.config.hidden_size

        self.shared_dense = torch.nn.Sequential(
            torch.nn.Linear(hidden_size, hidden_size),
            torch.nn.ReLU(),
            torch.nn.Dropout(config.dropout_rate),
        )

        self.regression_head = torch.nn.Sequential(
            torch.nn.Linear(hidden_size, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(256, 1),
        )

        self.classification_head = torch.nn.Sequential(
            torch.nn.Linear(hidden_size, 256),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(256, 3),  # 0, 1, 2
        )

    def forward(self, **kwargs):
        # Write your code here
        # Forward pass

        input_ids = kwargs["input_ids"]
        attention_mask = kwargs["attention_mask"]
        token_type_ids = kwargs["token_type_ids"]

        bert_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
        )
        # roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        cls_representation = bert_output.last_hidden_state[:, 0, :]
        # cls_representation = roberta_output.last_hidden_state[:, 0, :]

        shared_features = self.shared_dense(cls_representation)
        regression_output = torch.clamp(self.regression_head(shared_features), 1, 5)
        classification_output = self.classification_head(shared_features)

        return {
            "relatedness_score": regression_output,
            "entailment_judgment": classification_output,
        }


# TODO3: Define your optimizer and loss function

model = MultiLabelModel().to(device)
# TODO3-1: Define your Optimizer
# optimizer = # Write your code here

muon_params = [
    p
    for layer in [
        model.bert,
        # model.roberta,
        model.shared_dense,
        model.regression_head,
        model.classification_head,
    ]
    for p in layer.parameters()
    if p.ndim >= 2
]

adamw_params = [
    p
    for layer in [
        model.bert,
        # model.roberta,
        model.shared_dense,
        model.regression_head,
        model.classification_head,
    ]
    for p in layer.parameters()
    if p.ndim < 2
]

optimizer = [
    SingleDeviceMuon(
        muon_params,
        lr=config.muon_lr,
        weight_decay=config.muon_weight_decay,
        momentum=config.muon_momentum,
    ),
    torch.optim.AdamW(
        adamw_params, lr=config.adamw_lr, weight_decay=config.adamw_weight_decay
    ),
]

# TODO3-2: Define your loss functions (you should have two)
# Write your code here

criterion_regression = torch.nn.MSELoss()
criterion_classification = torch.nn.CrossEntropyLoss()


def consistency_loss(reg_scores, clf_logits):

    device = reg_scores.device
    batch_size = reg_scores.shape[0]

    E_neutral = (1.5 * 451 + 2.5 * 615 + 3.5 * 1398 + 4.5 * 326) / 2790
    E_entail = (1.5 * 1 + 2.5 * 0 + 3.5 * 65 + 4.5 * 1338) / 1404
    E_contra = (1.5 * 0 + 2.5 * 59 + 3.5 * 496 + 4.5 * 157) / 712

    clf_probs = torch.softmax(clf_logits, dim=1)
    expected_reg_from_clf = (
        clf_probs[:, 0] * E_neutral  # NEUTRAL
        + clf_probs[:, 1] * E_entail  # ENTAILMENT
        + clf_probs[:, 2] * E_contra  # CONTRADICTION
    )

    reg_consis_loss = torch.nn.functional.mse_loss(reg_scores, expected_reg_from_clf)

    expected_clf_from_reg = torch.zeros(batch_size, 3, device=device)

    # P(class | score_range)

    mask_1_2 = (reg_scores >= 1.0) & (reg_scores < 2.0)
    expected_clf_from_reg[mask_1_2] = torch.tensor(
        [451 / 452.0, 1 / 452.0, 0 / 452.0], device=device  # [NEUTRAL, ENTAIL, CONTRA]
    )

    mask_2_3 = (reg_scores >= 2.0) & (reg_scores < 3.0)
    expected_clf_from_reg[mask_2_3] = torch.tensor(
        [615 / 674.0, 0 / 674.0, 59 / 674.0], device=device
    )

    mask_3_4 = (reg_scores >= 3.0) & (reg_scores < 4.0)
    expected_clf_from_reg[mask_3_4] = torch.tensor(
        [1398 / 1959.0, 65 / 1959.0, 496 / 1959.0], device=device
    )

    mask_4_5 = reg_scores >= 4.0
    expected_clf_from_reg[mask_4_5] = torch.tensor(
        [326 / 1821.0, 1338 / 1821.0, 157 / 1821.0], device=device
    )

    clf_log_probs = torch.nn.functional.log_softmax(clf_logits, dim=1)
    clf_consis_loss = torch.nn.functional.kl_div(
        clf_log_probs, expected_clf_from_reg, reduction="batchmean"
    )

    return reg_consis_loss + clf_consis_loss


# scoring functions
psr = load("pearsonr")
acc = load("accuracy")

best_score = 0.0
sample_count = 0
for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model.train()
    # TODO4: Write the training loop
    # Write your code here
    # train your model
    # clear gradient
    # forward pass
    # compute loss
    # back-propagation
    # model optimization

    for batch in pbar:
        batch = {
            k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v
            for k, v in batch.items()
        }
        optimizer[0].zero_grad()
        optimizer[1].zero_grad()

        outputs = model(**batch)

        loss_reg = criterion_regression(
            outputs["relatedness_score"].squeeze(), batch["relatedness_score"]
        )
        loss_clf = criterion_classification(
            outputs["entailment_judgment"], batch["entailment_judgment"]
        )
        consis_loss = consistency_loss(
            outputs["relatedness_score"].squeeze(), outputs["entailment_judgment"]
        )
        # loss = config.alpha * loss_reg + (1 - config.alpha) * loss_clf
        loss = (1 - config.alpha) * (loss_reg + loss_clf) + config.alpha * consis_loss

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        raw_grad_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                raw_grad_norm += param_norm.item() ** 2
        raw_grad_norm = raw_grad_norm**0.5

        optimizer[0].step()
        optimizer[1].step()

        pbar.set_postfix(loss=loss.item())
        batch_size = batch["input_ids"].shape[0]
        sample_count += batch_size
        wandb.log(
            {
                "train_loss": loss.item(),
                "raw_grad_norm": raw_grad_norm,
                "batch_perplexity": torch.exp(loss).item(),
            },
            step=sample_count,
        )

    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model.eval()
    # TODO5: Write the evaluation loop
    # Write your code here
    # Evaluate your model
    # Output all the evaluation scores (PearsonCorr, Accuracy)
    # pearson_corr = # Write your code here
    # accuracy = # Write your code here
    # print(f"F1 Score: {f1.compute()}")
    with torch.no_grad():
        pearson_corr = 0
        accuracy = 0
        all_reg_preds = []
        all_reg_targets = []
        all_clf_preds = []
        all_clf_targets = []

        for batch in pbar:
            batch = {
                k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v
                for k, v in batch.items()
            }
            outputs = model(**batch)

            reg_pred = outputs["relatedness_score"].squeeze().cpu().numpy()
            reg_target = batch["relatedness_score"].cpu().numpy()
            all_reg_preds.extend(reg_pred)
            all_reg_targets.extend(reg_target)

            clf_pred = torch.argmax(outputs["entailment_judgment"], dim=1).cpu().numpy()
            clf_target = batch["entailment_judgment"].cpu().numpy()
            all_clf_preds.extend(clf_pred)
            all_clf_targets.extend(clf_target)

        pearson_result = psr.compute(
            predictions=all_reg_preds, references=all_reg_targets
        )
        pearson_corr = pearson_result["pearsonr"]

        accuracy_result = acc.compute(
            predictions=all_clf_preds, references=all_clf_targets
        )
        accuracy = accuracy_result["accuracy"]

        combined_score = 0.5 * pearson_corr + 0.5 * accuracy
        print(f"Epoch {ep+1}: Pearson={pearson_corr:.4f}, Accuracy={accuracy:.4f}")
        wandb.log(
            {
                "val_pearson": pearson_corr,
                "val_accuracy": accuracy,
                "val_combined_score": combined_score,
            },
            step=sample_count,
        )

        if combined_score > best_score:
            best_score = combined_score
            torch.save(model.state_dict(), f"{save_dir}/best_model.ckpt")

# Load the model
model = MultiLabelModel().to(device)
model.load_state_dict(torch.load(f"{save_dir}/best_model.ckpt", weights_only=True))

# Test Loop
pbar = tqdm(dl_test, desc="Test")
model.eval()

# TODO6: Write the test loop
# Write your code here
# We have loaded the best model with the highest evaluation score for you
# Please implement the test loop to evaluate the model on the test dataset
# We will have 10% of the total score for the test accuracy and pearson correlation

with torch.no_grad():
    pearson_corr = 0
    accuracy = 0
    all_reg_preds = []
    all_reg_targets = []
    all_clf_preds = []
    all_clf_targets = []

    for batch in pbar:
        batch = {
            k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v
            for k, v in batch.items()
        }
        outputs = model(**batch)

        reg_pred = outputs["relatedness_score"].squeeze().cpu().numpy()
        reg_target = batch["relatedness_score"].cpu().numpy()
        all_reg_preds.extend(reg_pred)
        all_reg_targets.extend(reg_target)

        clf_pred = torch.argmax(outputs["entailment_judgment"], dim=1).cpu().numpy()
        clf_target = batch["entailment_judgment"].cpu().numpy()
        all_clf_preds.extend(clf_pred)
        all_clf_targets.extend(clf_target)

    pearson_result = psr.compute(predictions=all_reg_preds, references=all_reg_targets)
    pearson_corr = pearson_result["pearsonr"]

    accuracy_result = acc.compute(predictions=all_clf_preds, references=all_clf_targets)
    accuracy = accuracy_result["accuracy"]

    combined_score = 0.5 * pearson_corr + 0.5 * accuracy
    print(f"Pearson={pearson_corr:.4f}, Accuracy={accuracy:.4f}")


wandb.log(
    {
        "test_pearson": pearson_corr,
        "test_accuracy": accuracy,
        "test_combined_score": combined_score,
    }
)

wandb.finish()

if os.path.exists(f"{save_dir}/best_model.ckpt"):
    os.remove(f"{save_dir}/best_model.ckpt")

import json

with open(f"./error_analysis.json", "w", encoding="utf-8") as f:
    json.dump(all_errors, f, indent=2, ensure_ascii=False)
