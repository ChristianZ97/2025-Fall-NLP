{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#  You can install and import any other libraries if needed\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # print(f\"\\n\\nUsing random seed {seed}\")\n",
    "\n",
    "\n",
    "set_seed(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Chinese punctuations will be tokenized as [UNK], so we replace them with English ones\n",
    "token_replacement = [\n",
    "    [\"：\", \":\"],\n",
    "    [\"，\", \",\"],\n",
    "    [\"“\", '\"'],\n",
    "    [\"”\", '\"'],\n",
    "    [\"？\", \"?\"],\n",
    "    [\"……\", \"...\"],\n",
    "    [\"！\", \"!\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\", cache_dir=\"./cache/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\",\n",
    "            split=split,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=\"./cache/\",\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # Replace Chinese punctuations with English ones\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# You can modify these values if needed\n",
    "adamw_lr = 0.00148975071320148\n",
    "adamw_weight_decay = 0.0661990066204037\n",
    "\n",
    "muon_lr = 0.000638688467548953\n",
    "muon_momentum = 0.932638804220204\n",
    "muon_weight_decay = 0.0841106463026747\n",
    "\n",
    "alpha = 0.205310994109178\n",
    "warmup_ratio = 0.192282667612615\n",
    "\n",
    "# lr = 3e-5\n",
    "epochs = 4\n",
    "train_batch_size = 16\n",
    "validation_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    # Write your code here\n",
    "    # The input parameter is a data batch (tuple), and this function packs it into tensors.\n",
    "    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.\n",
    "    # Return the data batch and labels for each sub-task.\n",
    "    pair_ids = [item[\"sentence_pair_id\"] for item in batch]\n",
    "    premises = [item[\"premise\"] for item in batch]\n",
    "    hypotheses = [item[\"hypothesis\"] for item in batch]\n",
    "    relatedness_labels = [item[\"relatedness_score\"] for item in batch]\n",
    "    entailment_labels = [item[\"entailment_judgment\"] for item in batch]\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        premises,\n",
    "        hypotheses,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    relatedness_tensor = torch.tensor(relatedness_labels, dtype=torch.float)\n",
    "    entailment_tensor = torch.tensor(entailment_labels, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"sentence_pair_id\": pair_ids,\n",
    "        \"premise\": premises,\n",
    "        \"hypothesis\": hypotheses,\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "        \"token_type_ids\": encoded[\"token_type_ids\"],\n",
    "        \"relatedness_score\": relatedness_tensor,\n",
    "        \"entailment_judgment\": entailment_tensor,\n",
    "    }\n",
    "\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "# dl_train = # Write your code here\n",
    "# dl_validation = # Write your code here\n",
    "# dl_test = # Write your code here\n",
    "dl_train = DataLoader(\n",
    "    SemevalDataset(split=\"train\"),\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=min(4, os.cpu_count()),\n",
    ")\n",
    "dl_validation = DataLoader(\n",
    "    SemevalDataset(split=\"validation\"),\n",
    "    batch_size=validation_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "dl_test = DataLoader(\n",
    "    SemevalDataset(split=\"test\"),\n",
    "    batch_size=validation_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2: Construct your model\n",
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        # Please use \"google-bert/bert-base-uncased\" model (https://huggingface.co/google-bert/bert-base-uncased)\n",
    "        # Besides the base model, you may design additional architectures by incorporating linear layers, activation functions, or other neural components.\n",
    "        # Remark: The use of any additional pretrained language models is not permitted.\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            \"google-bert/bert-base-uncased\", cache_dir=\"./cache/\"\n",
    "        )\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        self.gating = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.expert0 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.expert1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.regression_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, 1),  # [1, 5]\n",
    "        )\n",
    "\n",
    "        self.classification_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.RMSNorm(hidden_size),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, 3),  # 0, 1, 2\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "\n",
    "        input_ids = kwargs[\"input_ids\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        token_type_ids = kwargs[\"token_type_ids\"]\n",
    "\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        cls_representation = bert_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "        expert_weight = self.gating(cls_representation)\n",
    "        expert0_features = self.expert0(cls_representation)\n",
    "        expert1_features = self.expert1(cls_representation)\n",
    "\n",
    "        shared_features = (\n",
    "            expert_weight * expert0_features + (1 - expert_weight) * expert1_features\n",
    "        )\n",
    "        regression_output = self.regression_head(shared_features)\n",
    "        classification_output = self.classification_head(shared_features)\n",
    "\n",
    "        return {\n",
    "            \"relatedness_score\": regression_output,\n",
    "            \"entailment_judgment\": classification_output,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO3: Define your optimizer and loss function\n",
    "\n",
    "model = MultiLabelModel().to(device)\n",
    "# TODO3-1: Define your Optimizer\n",
    "# optimizer = # Write your code here\n",
    "muon_params = [\n",
    "    p\n",
    "    for layer in [\n",
    "        model.bert,\n",
    "        model.gating,\n",
    "        model.expert0,\n",
    "        model.expert1,\n",
    "        model.regression_head,\n",
    "        model.classification_head,\n",
    "    ]\n",
    "    for p in layer.parameters()\n",
    "    if p.ndim >= 2\n",
    "]\n",
    "\n",
    "adamw_params = [\n",
    "    p\n",
    "    for layer in [\n",
    "        model.bert,\n",
    "        model.gating,\n",
    "        model.expert0,\n",
    "        model.expert1,\n",
    "        model.regression_head,\n",
    "        model.classification_head,\n",
    "    ]\n",
    "    for p in layer.parameters()\n",
    "    if p.ndim < 2\n",
    "]\n",
    "\n",
    "optimizer = [\n",
    "    torch.optim.Muon(\n",
    "        muon_params,\n",
    "        lr=muon_lr,\n",
    "        weight_decay=muon_weight_decay,\n",
    "        momentum=muon_momentum,\n",
    "    ),\n",
    "    torch.optim.AdamW(adamw_params, lr=adamw_lr, weight_decay=adamw_weight_decay),\n",
    "]\n",
    "\n",
    "num_training_steps = len(dl_train) * epochs\n",
    "num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "scheduler = [\n",
    "    get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer[0],\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    ),\n",
    "    get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer[1],\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# TODO3-2: Define your loss functions (you should have two)\n",
    "# Write your code here\n",
    "criterion_regression = torch.nn.MSELoss()\n",
    "criterion_classification = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def consistency_loss(reg_scores, clf_logits):\n",
    "    # reg_scores shape: [B, 1], clf_logits shape: [B, 3]\n",
    "    device = reg_scores.device\n",
    "\n",
    "    # --- reg_scores -> expected_reg_from_clf ---\n",
    "    E_neutral = (1.5 * 451 + 2.5 * 615 + 3.5 * 1398 + 4.5 * 326) / 2790\n",
    "    E_entail = (1.5 * 1 + 2.5 * 0 + 3.5 * 65 + 4.5 * 1338) / 1404\n",
    "    E_contra = (1.5 * 0 + 2.5 * 59 + 3.5 * 496 + 4.5 * 157) / 712\n",
    "    E_vec = torch.tensor([E_neutral, E_entail, E_contra], device=device)\n",
    "\n",
    "    clf_probs = torch.softmax(clf_logits, dim=1)\n",
    "    expected_reg_from_clf = (clf_probs * E_vec).sum(dim=1)  # Shape: [B]\n",
    "    reg_consis_loss = torch.nn.functional.mse_loss(reg_scores, expected_reg_from_clf)\n",
    "\n",
    "    # --- clf_logits -> expected_clf_from_reg ---\n",
    "    p_1_2 = torch.tensor([451 / 452.0, 1 / 452.0, 0 / 452.0], device=device)\n",
    "    p_2_3 = torch.tensor([615 / 674.0, 0 / 674.0, 59 / 674.0], device=device)\n",
    "    p_3_4 = torch.tensor([1398 / 1959.0, 65 / 1959.0, 496 / 1959.0], device=device)\n",
    "    p_4_5 = torch.tensor([326 / 1821.0, 1338 / 1821.0, 157 / 1821.0], device=device)\n",
    "\n",
    "    mask_1_2 = ((reg_scores >= 1.0) & (reg_scores < 2.0)).unsqueeze(-1)\n",
    "    mask_2_3 = ((reg_scores >= 2.0) & (reg_scores < 3.0)).unsqueeze(-1)\n",
    "    mask_3_4 = ((reg_scores >= 3.0) & (reg_scores < 4.0)).unsqueeze(-1)\n",
    "    mask_4_5 = (reg_scores >= 4.0).unsqueeze(-1)\n",
    "\n",
    "    expected_clf_from_reg = (\n",
    "        mask_1_2 * p_1_2 + mask_2_3 * p_2_3 + mask_3_4 * p_3_4 + mask_4_5 * p_4_5\n",
    "    )  # Shape: [B, 3]\n",
    "\n",
    "    clf_log_probs = torch.nn.functional.log_softmax(clf_logits, dim=1)\n",
    "    clf_consis_loss = torch.nn.functional.kl_div(\n",
    "        clf_log_probs, expected_clf_from_reg, reduction=\"batchmean\"\n",
    "    )\n",
    "\n",
    "    return reg_consis_loss + clf_consis_loss\n",
    "\n",
    "\n",
    "# scoring functions\n",
    "psr = load(\"pearsonr\")\n",
    "acc = load(\"accuracy\")\n",
    "f1_metric = load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0.0\n",
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    # TODO4: Write the training loop\n",
    "    # Write your code here\n",
    "    # train your model\n",
    "    # clear gradient\n",
    "    # forward pass\n",
    "    # compute loss\n",
    "    # back-propagation\n",
    "    # model optimization\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {\n",
    "            k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        optimizer[0].zero_grad()\n",
    "        optimizer[1].zero_grad()\n",
    "\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss_reg = criterion_regression(\n",
    "            outputs[\"relatedness_score\"].squeeze(), batch[\"relatedness_score\"]\n",
    "        )\n",
    "        loss_clf = criterion_classification(\n",
    "            outputs[\"entailment_judgment\"], batch[\"entailment_judgment\"]\n",
    "        )\n",
    "\n",
    "        if ep > 3:\n",
    "            consis_loss = consistency_loss(\n",
    "                outputs[\"relatedness_score\"].squeeze(), outputs[\"entailment_judgment\"]\n",
    "            )\n",
    "            loss = (1 - alpha) * (loss_reg + loss_clf) + alpha * consis_loss\n",
    "        else:\n",
    "\n",
    "            loss = 0.5 * (loss_reg + loss_clf)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        raw_grad_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                raw_grad_norm += param_norm.item() ** 2\n",
    "        raw_grad_norm = raw_grad_norm**0.5\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer[0].step()\n",
    "        optimizer[1].step()\n",
    "        scheduler[0].step()\n",
    "        scheduler[1].step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    # TODO5: Write the evaluation loop\n",
    "    # Write your code here\n",
    "    # Evaluate your model\n",
    "    # Output all the evaluation scores (PearsonCorr, Accuracy)\n",
    "    # pearson_corr = # Write your code here\n",
    "    # accuracy = # Write your code here\n",
    "    # print(f\"F1 Score: {f1.compute()}\")\n",
    "    with torch.no_grad():\n",
    "        pearson_corr = 0\n",
    "        accuracy = 0\n",
    "        all_reg_preds = []\n",
    "        all_reg_targets = []\n",
    "        all_clf_preds = []\n",
    "        all_clf_targets = []\n",
    "\n",
    "        for batch in pbar:\n",
    "            batch = {\n",
    "                k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in batch.items()\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            reg_pred = outputs[\"relatedness_score\"].squeeze().cpu().numpy()\n",
    "            reg_target = batch[\"relatedness_score\"].cpu().numpy()\n",
    "            all_reg_preds.extend(reg_pred)\n",
    "            all_reg_targets.extend(reg_target)\n",
    "\n",
    "            clf_pred = torch.argmax(outputs[\"entailment_judgment\"], dim=1).cpu().numpy()\n",
    "            clf_target = batch[\"entailment_judgment\"].cpu().numpy()\n",
    "            all_clf_preds.extend(clf_pred)\n",
    "            all_clf_targets.extend(clf_target)\n",
    "\n",
    "        pearson_result = psr.compute(\n",
    "            predictions=all_reg_preds, references=all_reg_targets\n",
    "        )\n",
    "        pearson_corr = pearson_result[\"pearsonr\"]\n",
    "\n",
    "        accuracy_result = acc.compute(\n",
    "            predictions=all_clf_preds, references=all_clf_targets\n",
    "        )\n",
    "        accuracy = accuracy_result[\"accuracy\"]\n",
    "\n",
    "        f1_macro = f1_metric.compute(\n",
    "            predictions=all_clf_preds, references=all_clf_targets, average=\"macro\"\n",
    "        )[\"f1\"]\n",
    "        f1_weighted = f1_metric.compute(\n",
    "            predictions=all_clf_preds, references=all_clf_targets, average=\"weighted\"\n",
    "        )[\"f1\"]\n",
    "\n",
    "        combined_score = 0.5 * pearson_corr + 0.5 * accuracy\n",
    "        print(\n",
    "            f\"Epoch {ep+1}: Pearson={pearson_corr}, Accuracy={accuracy}, Macro-F1={f1_macro}, Weighted-F1={f1_weighted}, Combine={combined_score}\"\n",
    "        )\n",
    "\n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            torch.save(model.state_dict(), f\"./saved_models/best_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = MultiLabelModel().to(device)\n",
    "model.load_state_dict(torch.load(f\"./saved_models/best_model.ckpt\", weights_only=True))\n",
    "\n",
    "# Test Loop\n",
    "pbar = tqdm(dl_test, desc=\"Test\")\n",
    "model.eval()\n",
    "\n",
    "# TODO6: Write the test loop\n",
    "# Write your code here\n",
    "# We have loaded the best model with the highest evaluation score for you\n",
    "# Please implement the test loop to evaluate the model on the test dataset\n",
    "# We will have 10% of the total score for the test accuracy and pearson correlation\n",
    "with torch.no_grad():\n",
    "    pearson_corr = 0\n",
    "    accuracy = 0\n",
    "    all_reg_preds = []\n",
    "    all_reg_targets = []\n",
    "    all_clf_preds = []\n",
    "    all_clf_targets = []\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {\n",
    "            k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        reg_pred = outputs[\"relatedness_score\"].squeeze().cpu().numpy()\n",
    "        reg_target = batch[\"relatedness_score\"].cpu().numpy()\n",
    "        all_reg_preds.extend(reg_pred)\n",
    "        all_reg_targets.extend(reg_target)\n",
    "\n",
    "        clf_pred = torch.argmax(outputs[\"entailment_judgment\"], dim=1).cpu().numpy()\n",
    "        clf_target = batch[\"entailment_judgment\"].cpu().numpy()\n",
    "        all_clf_preds.extend(clf_pred)\n",
    "        all_clf_targets.extend(clf_target)\n",
    "\n",
    "        batch_size = batch[\"input_ids\"].shape[0]\n",
    "        pair_ids = batch[\"sentence_pair_id\"]\n",
    "\n",
    "    pearson_result = psr.compute(predictions=all_reg_preds, references=all_reg_targets)\n",
    "    pearson_corr = pearson_result[\"pearsonr\"]\n",
    "\n",
    "    accuracy_result = acc.compute(predictions=all_clf_preds, references=all_clf_targets)\n",
    "    accuracy = accuracy_result[\"accuracy\"]\n",
    "\n",
    "    f1_macro = f1_metric.compute(\n",
    "        predictions=all_clf_preds, references=all_clf_targets, average=\"macro\"\n",
    "    )[\"f1\"]\n",
    "    f1_weighted = f1_metric.compute(\n",
    "        predictions=all_clf_preds, references=all_clf_targets, average=\"weighted\"\n",
    "    )[\"f1\"]\n",
    "\n",
    "    combined_score = 0.5 * pearson_corr + 0.5 * accuracy\n",
    "    print(\n",
    "        f\"\\nTest: Pearson={pearson_corr}, Accuracy={accuracy}, Macro-F1={f1_macro}, Weighted-F1={f1_weighted}, Combine={combined_score}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31014",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
