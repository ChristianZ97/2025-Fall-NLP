id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention AWS using recycled water for cooling in 2023, so the question cannot be answered with confidence from the given documents."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the Amazon Solar Farm Maryland-CPV Backbone or its avoided CO2e, so the question cannot be answered with the supplied documents."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the amortized training cost for GPT‑4 (40 M) but do not give a numeric training budget for FLM‑101B, so the required factor cannot be determined from the supplied context."
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,percent,wu2021b,https://arxiv.org/pdf/2108.06738,"""The total energy consumption of the US data centers increased by about 4 from 2010-2014, compared with the estimated 24 increase from 2005-10 and nearly 90 increase from 2000-05.""","The passage from wu2021b explicitly states that U.S. data center electricity consumption rose by about 4% over the 2010‑2014 period, providing the average increase value."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not include a numeric estimate of GPU power usage in kWh for the specified SGLang benchmarking scenario. Therefore, I cannot provide a confident answer."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a specific numerical value for CO2 emissions per mile or for the round-trip distance between San Francisco and New York, so the question cannot be answered with confidence from the given documents."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied context passages contain any information about permitted annual emission limits for nitrogen oxides from data center backup generators in northern Virginia for the specified period.
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,morrison2025,https://arxiv.org/pdf/2503.05804,"NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as , or 3700 kg of COeq per 8x server node, equal 463 kg per GPU.","The context explicitly states that the estimated embodied carbon emissions for an NVIDIA GPU are 463 kg CO₂e per GPU, derived from 3700 kg per 8‑GPU server node."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,luccioni2023,https://arxiv.org/pdf/2302.08476,"""total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs)""","The Luccioni 2023 passage explicitly states that training GPT‑3 with 10,000 V100 GPUs took 14.8 days, which directly answers the question."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents report premature deaths for 2028 (≈1,300) but contain no data or projections for 2030, so the question cannot be answered with confidence from the given sources."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,fold,wu2021b,https://arxiv.org/pdf/2108.06738,"""This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.""","The cited passage from Doc [wu2021b] explicitly states that the improvement in processor clock speed from 1971 to 2021 is more than 6,750-fold, which directly answers the question."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context contains no explicit numeric value for the final average performance score of FLM-101B on the Open LLM Leaderboard, so a confident answer cannot be derived."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the launch year of the One Hundred Year Study on Artificial Intelligence, so it cannot be answered with confidence from these documents."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time‑saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""","The paper reports that using the growth strategy reduced training time from 76.74 days to 21.54 days, which corresponds to a 72% reduction in time."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit information about the number of experts in the JetMoE-8B architecture, so the answer cannot be determined with confidence."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,dodge2022,https://arxiv.org/pdf/2206.05229,"""This model was not trained to completion, but only until 13; a full training run would take 60 days.""","The quoted passage from Dodge et al. (2022) explicitly states that a full training run of the 6.1 billion‑parameter model would take 60 days, providing the numeric answer."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit information about the percentage of Switch Transformer parameters activated per token, so a confident answer cannot be derived."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""The UN’s Global E-Waste Monitor 2024 showed that about 22 of e-waste has been shown to be formally collected and recycled,""","The quoted sentence from Doc luccioni2025a states that approximately 22% of e‑waste is formally collected and recycled, which directly answers the question."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide a specific energy consumption figure for the pre‑training of the BLOOM model in MWh; available numbers refer to ranges for various models or to other BLOOM variants (e.g., BLOOMz), so the exact value cannot be inferred with confidence."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not contain a numeric value for the execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU, so the answer cannot be determined with confidence."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context passages contain placeholders where the numeric values for the English portion of FLM-101B’s training cost are missing, so the documents do not provide a clear, extractable answer."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command‑R Plus)""","The cited passage lists the highest GPU energy consumption for 1,000 queries as over 3,426 Wh for the model Command‑R Plus, indicating it has the highest usage among those mentioned in the appendix of the 2025 study."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""To be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero‑shot or multi‑task models, to allow comparisons both for differen"" and ""We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision.""",The quoted passages from the Power Hungry Processing study state that 88 distinct machine learning models were sampled and analyzed.
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time‑saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""","The context explicitly states that the growth strategy requires 21.54 days to train FLM‑101B, providing the exact wall‑clock time. This quote directly supports the numeric answer and unit."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x,2.2,is_blank,cottier2024,https://arxiv.org/pdf/2405.21015,"""ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.""","The cited passage from Cottier et al. gives a median ratio of total compute to final training run compute as 2.2×, indicating that overall development compute is about 2.2 times larger than the final training run alone."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,is_blank,is_blank,"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity","The context explicitly states that GPT‑3 training consumed 1,287 MWh, which directly answers the question."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement or data indicating that a majority of Facebook’s model experimentation workflows use GPUs at over 80% capacity, so the truthfulness of the claim cannot be determined with confidence."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,dodge2022,https://arxiv.org/pdf/2206.05229,"""We note our training run of the 6 billion parameter transformer only trained for approximately 13 of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.""","The document reports a full‑run energy estimate of 103,593 kWh for the 6.1 billion‑parameter transformer. Converting 103,593 kWh to megawatt hours (÷1000) gives roughly 103.5 MWh, which is the requested value."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""AI Energy Score , a project aiming to establish a unified approach for comparing the inference efficiency of AI models.""","The quoted sentence explicitly names the collaborative project as ""AI Energy Score"" and describes its goal of providing a standardized method for comparing inference efficiency across AI models."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any statement indicating that Red AI is on the decline, nor do they discuss a decline trend. Therefore, there is insufficient evidence to answer the question confidently."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the JetMoE-8B model or its top‑k expert selection strategy, so the answer cannot be determined from the available context."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied context passages contain any information about the number of AWS data center regions that were 100% matched with renewable energy in 2023.
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""the global carbon emissions for 2020 dropped by 6.4 with vehicle transportation in the US accounting for a portion of the global reduction.""","The context from document wu2021b explicitly states that global carbon emissions fell by 6.4 in 2020, implying a 6.4% reduction during the COVID‑19 pandemic. That figure is taken directly from the quoted sentence."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.","The provided documents state the compute increase was approximately 300,000×, not 200,000×, so the claim is not supported and is therefore false."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,About 50 years,50,years,luccioni2025c;wu2021b,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2108.06738,"""The last fifty years of digital products"" (luccioni2025c) and ""five decades ago"" (wu2021b).","Both passages indicate that the field of AI has been evolving for roughly five decades, so in 2025 its approximate age is about 50 years."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement or numeric value indicating Amazon’s energy storage capacity as of 2023, so the answer cannot be determined with confidence from the documents."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a specific numeric value for the execution time of the longest kernel in the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48 GB GPU.
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,20,20,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse.
-D  -S  -D  -S
-  2  8  6  20","The table in the provided context lists the maximum batch sizes for dense (D) and sparse (S) fine‑tuning. For the GSM8K dataset, the sparse configuration (S) shows a maximum batch size of 20 samples, which is the value required by the question."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Training a large language model (LLM),Training a large language model,is_blank,is_blank,is_blank,"""the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM)""","The five cars estimate originates from Strubell et al.’s 2019 study, which specifically quantified emissions from training a large language model (BERT), highlighting that the estimate is based on the infrequently performed AI process of training an LLM."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the percentage of AI inference workloads in Asia powered by coal in 2023, so a confident answer cannot be given."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement about the hardware processor used specifically for energy‑efficient local inference in financial sentiment classification, so the answer cannot be confidently extracted from the documents."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,approximately 272 transatlantic flights,272,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""the cumulative emissions from approximately 272 transatlantic flights between Boston and London.""","The quoted passage directly states that GPT-4o’s annual emissions are comparable to the cumulative emissions of about 272 transatlantic flights, giving the required number."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear numeric value or statement indicating the percentage decrease in energy use for Llama 3.1 8B when targeting a TPOT of 100 ms versus minimizing latency. Thus, the answer cannot be determined from the documents supplied."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",6.6,6.6,billion cubic meters,jegham2025,https://arxiv.org/pdf/2505.09598,"""Li et al.  analyzed GPT-3’s freshwater consumption, estimating over 5 million liters used during training and projecting that AI-related withdrawals could reach 6.6 trillion lit""","The cited projection is 6.6 trillion liters, which converts to 6.6 billion cubic meters (1 cubic meter = 1,000 liters)."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages contain information about the number of Amazon electric delivery vans added in 2022 or 2023, so the answer cannot be determined from the documents."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons Paradox,Jevons Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,Applying Jevons’ Paradox to technologies like AI has notable conceptual and empirical limitations due to the complexity inherent in technological development and diffusion.,The paper explicitly cites Jevons’ Paradox—where efficiency gains can lead to higher overall consumption—as the economic principle undermining the belief that technical efficiency alone will produce net environmental benefits.
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific value for the global average PUE of AI‑dedicated data centers in 2023. The only PUE figures mentioned refer to general data center populations, not AI‑dedicated facilities."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?","1,000,000,000",1000000000,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""We additionally evaluate a mixture-of-experts (MoE) model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens.""","The document specifies that the MoE model has 1 billion active parameters, which are the parameters activated for each input token during inference."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied contexts do not contain any specific numeric information about the greenhouse gas emissions (in tCO2e) associated with pre‑training the Llama 7B model, so a confident answer cannot be derived from the documents."
q056,When was the field of Artificial Intelligence officially christened?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied contexts mention the year AI was officially christened, so the answer cannot be confidently derived from the documents."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity.""","The cited passage from document wu2021b explicitly states that about 770 million people lack stable electricity, confirming the claim."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.""","The quoted passage from Morrison 2025 explicitly states that operational impacts include GHG emissions from servers and data center cooling, contradicting the claim that they are excluded. Therefore the statement is false."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement linking an 80% GPU utilization to a specific factor by which the overall carbon footprint decreases for LM training. Therefore, I cannot provide a confident answer based on the documents given."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules per token,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"with length 512, we see that it takes about 3-4 Joules for a output token","The context explicitly states that for a maximum generation length of 512 tokens, LLaMA‑65B consumes approximately 3–4 Joules per output token, giving the energy cost per token."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not contain any explicit mention of Facebook’s RM2 model or the quantization percentage from 32-bit to 16-bit. Therefore, the answer cannot be determined from the provided documents."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a numeric value or explicit statement of the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. Therefore, there is insufficient information to answer the question confidently."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally and 1.6 in the EU.""","The statement from ebert2024 explicitly provides the global average PUE for 2023 as 1.58, which is the value requested."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The reasoning behind the 5-10 reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their""; ""The second, Google-commissioned BCG study provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale.""","The cited documents state that the 5‑10 % reduction figure lacks clear, publicly available calculations and does not have sound scientific grounding, so the claim is not supported."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents provide quantitative evidence that sparsely activated DNNs consume less than one‑tenth the energy of large dense DNNs while maintaining accuracy, so the statement cannot be confirmed or denied with the available information."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","25,000 USD",25000,USD,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Grover was trained on 256 TPU chips for two weeks, at an estimated cost of 25,000.""","The quoted passage from Doc [schwartz2019] explicitly states that Grover’s training on 256 TPU chips over two weeks cost approximately 25,000 USD, which directly answers the question."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention wind turbines contracted by Microsoft for Azure AI clusters in 2023, so the answer cannot be determined from the given documents."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention the number of members in the inaugural 2015 Study Panel of the One Hundred Year Study on AI, so the answer cannot be inferred with confidence from the documents."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,The optimizer stage in fine-tuning takes a considerable portion of the running time (up to 53 when conducting sparse fine-tuning with batch size = 1).,The quoted sentence directly states that the optimizer stage accounts for up to 53% of the running time when using a batch size of 1 on a NVIDIA A40-48GB GPU in sparse fine‑tuning. This matches the question’s request for the percentage of running time.
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any information about the distance between Earth and the Sun, so the answer cannot be determined from these documents."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,Gemini Ultra has the highest fraction of RD staff cost at 49,"The context explicitly states that Gemini Ultra’s R&D staff cost—including equity—constitutes 49% of the total amortized development cost, which directly answers the question."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention of the 100 Year Study on AI or its Study Panel, so the documents do not provide sufficient information to determine whether the panel is concerned that AI is an imminent threat to humankind."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The available context does not provide a numeric energy consumption for the o3 model for a long prompt. The only related numeric value is for o3-mini (3.098 Wh), which does not directly answer the question about o3. Therefore the answer cannot be determined with confidence from the supplied documents."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Meta’s Llama 3 family of models emitted about 11,390 tCO₂e during pre‑training, roughly 40‑plus times the ‘five cars’ estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Meta reports that their Llama 3 family of models emitted 11,390 tons COe or over 40x the five cars estimate.","The quote from Doc [luccioni2025c] gives the pre‑training emissions as 11,390 tCO₂e and states that this is more than 40 times the five‑cars baseline, directly answering both parts of the question."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a specific numeric factor describing how much Facebook’s AI training infrastructure capacity grew between Yr1-Q1 and Yr2-Q2 (2019–2021).
q080,True or False: The AlphaGo program defeated the human Go champion.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context contains no explicit statement that AlphaGo defeated a human Go champion, so the answer cannot be determined with confidence from these documents."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""different models with a similar number of parameters often perform different amounts of work.""","The quotation indicates that parameter count alone does not determine the amount of work—and thus energy—required during inference, showing that a larger model does not always consume more energy."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,is_blank,is_blank,"""0.083 kWh/1k queries"" (from Table in Doc [luccioni2024#0063]), ""1,000,000,000 queries per day"" (question statement).","The table gives Flan‑T5‑xxl energy of 0.083 kWh per 1 000 queries. For 1 000 000 000 queries/day, that is 1 000 000 × 0.083 kWh = 83 000 kWh, which equals 83 MWh."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of the number of H100 GPU hours required for the JetMoE-8B alignment process, including dSFT and dDPO fine‑tuning. Therefore, a confident answer cannot be derived from the documents supplied."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)""","The appendix of the 2025 study lists GPU energy usage for 1,000 inference queries for various models, citing a minimum of 0.06 Wh and a maximum of 3,426 Wh, which defines the requested range."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",1594 gCO2eq,1594,gCO2eq,is_blank,is_blank,"""the most carbon-intensive image generation model () generates 1,594 grams of  for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle""","The quoted passage from Doc luccioni2024#0048 states that the most carbon‑intensive image generation model emits 1,594 grams of CO₂eq per 1,000 inferences. This matches the value for stable‑diffusion‑xl‑base‑1.0, the model identified as the most intensive in the 2024 study."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric percentage indicating how much of a client device’s total carbon footprint is due to manufacturing. Therefore, the answer cannot be determined with confidence from the documents alone."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific figure for the CO2 emissions of OpenAI's API requests in January 2024, nor does it allow a reliable inference of that value."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,dynamic batching,dynamic batching,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization while maintaining low latency.,"The context explicitly names the strategy as ""dynamic batching,"" which is described as a method that optimizes GPU utilization, implying it reduces idle time by replacing completed requests with new ones."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.""","The excerpts from the provided document explicitly state that researchers do not believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed, supporting the false answer."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems.""","The context explicitly states that expanding AI transparency to include socio-technical aspects and societal/environmental footprints is referred to as ""social transparency."""
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages mention German public administration texts, sentence embeddings, or the accuracy of classification experiments on that specific dataset, so the answer cannot be determined from the provided documents."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the gross carbon intensity of energy according to the U.S. average mix in 2021, so the answer cannot be determined with confidence from these documents."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"""Given a SLO requirement of 100 TPS, selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of 2.13. On the other hand, both Max-Performance and  without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, ..."" and ""... with a total cost of 2.699, an increase of about 26.7.""","The InferSave policy chose g4dn.xlarge costing 2.13, while Max‑Performance chose g6e.xlarge costing 2.699. The cost difference is 0.569, which is (0.569/2.13)*100 ≈ 26.7%, matching the quoted increase."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google observed a 20 uptick in the same period.""","The context states Google’s data center water consumption rose by 20% from 2021 to 2022, directly answering the question."
q094,What is the total number of parameters in the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of a JetMoE-8B model or its total parameter count. Therefore, the answer cannot be determined from the documents alone."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied context passages do not contain any information about Amazon’s replenishment projects or the amount of water returned to communities in 2023. Therefore the answer cannot be determined from the provided documents.
q093,How many parameters does the largest T5 model have?,11 billion,11000000000,parameters,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Flan‑T5‑XXL, which is the biggest model in terms of parameter count in the multi‑purpose models that we tested (11 billion)""","The cited passage states that Flan‑T5‑XXL, the largest T5‑style model examined, has 11 billion parameters."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon intensity,Carbon intensity,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"The unit of measurement typically used for quantifying and comparing carbon emissions is . This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of  emitted per kilowatt hour of electricity generated ()","The passage defines the metric as grams of CO₂ emitted per kilowatt‑hour of electricity, which is the standard definition of carbon intensity, the emissions metric used in the LLM evaluation framework."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit statement or data about the fraction of local throughput achieved for NLP when training was distributed across four continents versus remaining local, so the answer cannot be determined from these documents."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,40 million USD,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""OpenAI's GPT-4 at 40M""","The context explicitly states that the amortized hardware CapEx plus energy cost for GPT-4 is 40 million, indicating the estimated amortized training cost."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",51.3%,51.3,%,li2025a,https://arxiv.org/pdf/2309.03852,"""The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3.""","The context explicitly states that each training stage, including the final 101B stage, achieved a FLOPs utilization rate of at least 51.3%; thus the reported utilization percentage is 51.3%."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement of the name of the LLM inference system described in the 2025 Chen et al. paper that uses model-attention disaggregation. Therefore, I cannot confidently provide an answer based solely on the documents given."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,cottier2024,https://arxiv.org/pdf/2405.21015,"""components of amortized hardware CapEx + energy in , we find that on average, 44 goes toward AI accelerator chips. The rest of the server (including markup) makes up 29 of the cost, while cluster level interconnect makes up 17.""","The quoted passage states that, on average, 44% of the amortized hardware CapEx plus energy cost is attributed to AI accelerator chips, which directly answers the question."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts.,"The cited sentence from Doc rubei2025 indicates that employing custom tags in prompt engineering—applied to source code completion tasks—can lower LLM energy consumption. Therefore, the statement is supported and the answer is True."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million,3700000,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand""","The 2025 paper states NVIDIA shipped 3.7 million GPUs in 2024, which is interpreted as the number of GPUs delivered to data centers. The quoted sentence directly provides the figure, supporting the answer."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,the PUE of Facebook datacenters is 1.10 (2020),"The provided passage from document wu2021b explicitly states that Facebook’s data centers have a PUE of 1.10 in 2020, which directly answers the question."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",Up to a 45% reduction in the operational carbon footprint.,45,%,is_blank,is_blank,"""reduce energy consumption and carbon emissions by up to 45 post quantization""","The cited study reports that applying full‑stack optimizations, including quantization, can cut the carbon emissions of a universal translation Transformer by as much as 45 %, indicating a roughly 45‑percent reduction compared to a CPU‑baseline."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems""","The cited sentence explicitly names the Finnish project as ETAIROS, making that the required acronym."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of a decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents, so the answer cannot be determined with confidence from the documents."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any information about the EPA’s recently tightened primary standard for the annual average limit of PM2.5, so a confident answer cannot be derived from them."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""the ratio of the highest to lowest county-level per-household health cost reaches approximately 200.""","The cited study reports that the most affected, economically‑disadvantaged communities can experience about a 200‑fold higher per‑household health burden from air pollutants compared to less impacted communities, making 200 the factor by which the burden could exceed."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks.  

These provisions lack sufficient emphasis on environmental factors. Although environmental protection is included in the Act's objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required.","The excerpts show that while the AI Act requires risk assessment for GPAI models with systemic risk, it does not explicitly mandate that environmental risks be included. Therefore the statement is false."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""115 books would produce the same amount of CO as a single Amazon Kindle device.""","The life cycle assessment cited in Doc [luccioni2025a] directly states that 115 physical print books emit the same amount of CO₂ as one Amazon Kindle e‑reader, providing the numerical answer."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 lbs",36156,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"""American life, avg, 1 year  36,156""","The provided context lists the estimated CO2e for an average American life in one year as 36,156 pounds, which directly answers the question."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the energy consumption of Meena training runs or the total energy of a full GPT‑3 training run, so the comparison cannot be made with confidence."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not specify the name of any West Virginia county that is projected to have the highest per-household health cost in the 2030 projections of the paper, so a confident answer cannot be given."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,jegham2025;luccioni2025a,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2501.16548,"""as per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox"" (Doc [jegham2025])
""these gains can result in higher overall consumption due to effects such as Jevons Paradox"" (Doc [luccioni2025a])","The excerpts explicitly state that improved efficiency leads to increased usage and resource consumption, and identify this effect as the Jevons Paradox, confirming that this is the phenomenon described."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about a 2022 paper by Dodge et al. or the parameter count of the model they analyzed, so the answer cannot be determined from the documents supplied."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30 million,30,million USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at 40M and Google's Gemini Ultra at 30M.""","The context explicitly states that Gemini Ultra’s amortized training cost is 30M, i.e., $30 million. The value is taken directly from the quoted sentence. "
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any reference to the energy consumption of a DS Llama 70B model on the FKTG dataset, so the answer cannot be determined from the documents."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy""",The quote from the 2024 study ‘Power Hungry Processing’ explicitly states that the total energy consumed for all model experimentation and evaluation was 754.66 kWh.
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,ebert2024,https://arxiv.org/pdf/2410.06681,"""energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process""","The study reports 51,686 kWh for training and 7,571 kWh for fine‑tuning the BLOOMz‑7B model. Adding these gives a combined energy cost of 59,257 kWh."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals, so the question cannot be answered with confidence from the supplied context."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"""51,686  25,634  17,052  10,505  7,571  3,242  1,081  543  1.0 ×  7.3 ×   6.2 ×   5.4 ×  592,570,000  395,602,740  292,467,741  204,592,592""","The table in Luccioni 2024 lists the number of inferences required for the cumulative inference energy to match the training+finetuning energy for each BLOOMz model. For BLOOMz‑7B the figure is 592,570,000, indicating that 592,570,000 inferences equal the initial training and fine‑tuning energy cost."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""84 of LLM usage is through models with no disclosure, 14 for indirectly disclosed models, and only 2 for models with direct disclosure.""","The quoted passage from the May 2025 OpenRouter data states that 84 % of token usage occurred through models that did not disclose their environmental impact, supporting the answer."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"Specifically, we train three models,  with 16B, 51B, and 101B parameters, respectively","The quoted passage from the FLM‑101B documentation explicitly states that the final model has 101 billion parameters, which is the total number of parameters for the final FLM‑101B model."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.75,0.75,is_blank,is_blank,is_blank,Mistral-small  0.70  0.67  0.65  0.67  0.020 ... Mistral-small  0.73  0.70  0.69  0.70  0.015,"The emissions for Mistral-small before optimization were 0.020, and after optimization were 0.015. Dividing the post‑optimization value by the pre‑optimization value gives 0.015/0.020 = 0.75, indicating emissions were multiplied by 0.75."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any statement that directly relates the 3.2 tCO2e emissions of the Evolved Transformer NAS to the number of passengers on a round‑trip flight between San Francisco and New York. Therefore the answer cannot be determined from the available documents.
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","Approximately 104,000 inferences",104000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"From Dodge et al.: ""we estimate a full training run would consume approximately 103,593 kWh."" From Luccioni et al.: ""1.0 × kWh for the biggest one, BLOOMz-7B.""","The full training of a 6.1B‑parameter model consumes about 103,593 kWh. Each inference of BLOOMz‑7B costs roughly 1 kWh, so 103,593 kWh / 1 kWh ≈ 103,593 ≈ 104,000 inferences are needed to match the training energy."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any explicit numeric information or statement about the total carbon emissions avoided by pruning and quantizing large language models in 2023, so the answer cannot be determined from the documents."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a total freshwater consumption figure for Meta's Llama 3 inference serving clusters in 2024.
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",3.31 kWh,3.31,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,image generation 3.31,"The table of mean energy per 1,000 queries lists image generation at 3.31 kWh, which is the average energy consumption for 1,000 image generation inferences as reported in Table 2 of the 2024 study."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""using 2 A100s and 1 A10G results in a 24 cost saving over A100-only and 31 over A10G.""",The quoted passage explicitly states that the hybrid mix of 2 A100s and 1 A10G yields a 24% cost saving relative to an A100‑only strategy.
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,dodge2022,https://arxiv.org/pdf/2206.05229,"""we estimate it would have emitted 21 to 78 metric tons of (depending on the region it was run in).""","The quoted sentence from the Dodge 2022 document directly states that a complete training run of the 6.1 billion‑parameter transformer would emit between 21 and 78 metric tons of CO₂, providing the requested range."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any specific information about the JetMoE project’s training budget, total GPU hours, or cost per H100 GPU-hour. Therefore, a reliable estimate cannot be derived from the documents given."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""In fact, most carbon footprint analyses gather the information manually by writing to authors.""","The cited passage from luccioni2025b explicitly states that most analyses require manual contact with authors, contradicting the claim that they gather information automatically. Hence the statement is false."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,TRUE,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45 post quantization.,"The cited passage from the Khan 2025 document explicitly states a 45% reduction in carbon emissions after quantization, confirming the truth of the statement."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include a statement from Chen et al. (2025) giving the price per hour for an NVIDIA H20, so the answer cannot be determined with confidence."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,"""This is equivalent to approximately 44 of the data centers' total electricity cost.""","The document states that in 2023 the public health cost of data centers was about 44% of their total electricity cost when using the average attribution method, providing the percentage directly."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain any numeric information about the proportion of Apple’s total water footprint that originates from its supply chain, so the answer cannot be determined from the documents."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not contain any information about the number of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024, so the answer cannot be derived with confidence from the documents provided."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a clear statement or data indicating the percentage of Amazon’s U.S. workforce that identified as men in 2023.
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,answers,luccioni2025b,https://arxiv.org/pdf/2504.00797,"reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers","The document states that after contacting more than 500 authors, the researchers received 95 responses, directly supporting the answer."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,is_blank,is_blank,"Model Size    Count  Max. Batch size  Count  Max. Batch size  
 7B   1  64  1  64",The table of bare‑minimum hardware shows that the LLaMA‑7B model can run with a single GPU (both for V100 and A100) at a batch size of 64. Thus one NVIDIA A100 80GB GPU is sufficient.
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 NVIDIA A100 80GB GPU,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,13B 2 64 1 64,"The table in the document lists the bare‑minimum GPU counts for each model. For LLaMA‑13B, the entry shows ""13B 2 64 1 64"", meaning two GPUs are required on V100 and only one GPU on A100. Therefore the bare minimum number of A100 80GB GPUs needed is one."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention the Standing Committee of the One Hundred Year Study or describe how often it forms a Study Panel, so there is no evidence to support an answer."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not provide a specific numeric value for the total execution time of a sparse BlackMamba model fine-tuned on an NVIDIA A40-48GB GPU with a batch size of 84. No table, figure, or sentence cites this exact configuration or its timing, so the answer cannot be determined with confidence from the documents."
q149,How many tokens were used to pre-train the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not contain any mention of the JetMoE-8B model or the number of tokens used to pre-train it, so the answer cannot be determined from the documents given."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,times,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year""","The document states the deal could add 640 percent more emissions, which is equivalent to 6.4 times the yearly carbon removal target."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"WUE can be computed based on either water withdrawal (the total volume drawn from natural or municipal sources) or water consumption (the portion of withdrawn water permanently lost, primarily through evaporation).","The context defines ‘water withdrawal’ as the total volume of freshwater drawn from natural or municipal sources, matching the question’s description of freshwater taken from ground or surface sources for various uses."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 connected devices,25,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.""","The context from doc wu2021b explicitly states that the average U.S. household had 25 connected devices in 2021, providing the numeric answer directly."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention IBM's Watson or its performance on Jeopardy, so the question cannot be answered from the supplied documents."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific percentage figure for cost reduction by Mélange in conversational chat settings. Therefore, the answer cannot be determined with confidence from the given documents."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8 to 3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)""","The quoted passage from Doc luccioni2025c explicitly gives the minimum and maximum publicly reported pre‑training energy consumption for LLMs, yielding the range 0.8–3,500 MWh."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""10–50 queries on GPT-3 consumes around half a liter of water.""","The passage from Doc [luccioni2025a] states that between 10 and 50 GPT‑3 queries use about half a liter of water, so the answer is a range of 10–50 queries."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the JetMoE-8B-Chat model, its MT-Bench score, or its comparison to Llama-2-13b-Chat. Therefore, the answer cannot be determined from these documents."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement of the dataset name used for German nuclear waste site objection texts classified in the experiments, so a confident answer cannot be derived."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",120%,120,%,han2024,https://arxiv.org/pdf/2412.06288,"""the health cost that even exceeds 120 of the training electricity cost.""","The context states that the health cost exceeded 120% of the electricity cost for training a Llama‑3.1‑scale model in Iowa, implying the health cost was 120% of the electricity cost."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000",10000,round trips,luccioni2025c;han2024,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2412.06288,"""training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.""","Both documents state that training a LLaMa‑3.1‑scale model emits enough air pollutants to equal more than 10,000 LA‑NYC round trips. The quoted sentence provides the exact figure, justifying the answer of ""more than 10,000"" round trips."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg,178.97,kg,luccioni2024,https://arxiv.org/pdf/2311.16863,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of .""","The provided passage from the Power Hungry Processing study explicitly states that the total emissions for all experiments and evaluations were 178.97 kg of CO₂ equivalent, which directly answers the question."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).""","The document states that the LLaMA‑65B model requires four 80 GB A100 GPUs for bare‑minimum inference without compression or quantization. Therefore, the minimum number of A100 GPUs needed is four."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,FALSE,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4o consumes around 2.875Wh while GPT-4o mini's consumption is slightly higher at 3.098Wh due to deployment conditions.""","The context explicitly states that GPT-4o mini consumes more energy per query (3.098 Wh) than GPT-4o (2.875 Wh), proving the claim is false."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific percentage of ML workload estimated to be inference processing by NVIDIA in 2019, so the answer cannot be determined with confidence from the documents."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Typically higher than the actual consumption, as GPU utilization rarely reaches its theoretical peak due to other resource constraints.""","The cited passage states that TDP usually overestimates real GPU power draw because GPUs are not fully utilized, indicating that TDP-based estimation is neither reliable nor accurate."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain a specific numeric value for the throughput of a dense Mixtral-CS-A100-40GB at batch size 1, so the answer cannot be determined from the given documents."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,"more than 700,000 liters",700000,liters,is_blank,is_blank,"""more than 700 kiloliters (kL) of water for cooling alone , enough to fill a quarter of an Olympic-sized swimming pool.""","The context states that GPT‑4 training used over 700 kL of water for cooling, which converts to more than 700,000 liters. The quote directly supports the numerical value."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific monthly on‑demand rental cost figure for serving Llama‑2‑70B at BF16 precision using 2 NVIDIA A100 GPUs, so an hourly estimate cannot be derived with confidence."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied passages give a quantitative relationship between model size and BLEU score that would allow calculation of how much larger a GPT‑3‑based model must be to raise BLEU from 5 to 40.
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""peaked in 2022, with 10 of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.""","The quoted passage from the 2025 paper indicates that after the 2022 peak, the trend of direct environmental disclosures reversed and declined, not increased. Therefore, the statement is false."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8,8,is_blank,is_blank,is_blank,"While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.  The 65B model was run on 8 V100 GPUs.","The documents state that the bare‑minimum sharding for LLaMA‑65B on 32‑GB V100 GPUs requires 8 GPUs. The table lists 65B with 8 GPUs, and the narrative confirms the use of 8 V100s for inference without compression or quantization."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","$1,000,000,000",1000000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""the amortized cost of frontier training runs will exceed one billion dollars by 2027.""","The cited passage explicitly states that, following the current growth trend, the largest training runs are projected to cost more than one billion dollars by 2027, which is the threshold mentioned in the question."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,56.4%,56.4,%,schwartz2019,https://arxiv.org/pdf/1907.10597,"""AlexNet 61.1 0.7 56.4 2012""",The table in Doc schwartz2019 lists AlexNet’s top‑1 accuracy on ImageNet as 56.4 %. This value is directly taken from the acc. column for the 2012 entry.
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context provides throughput per GPU and token counts, but lacks the necessary information on GPU count for the final stage, training duration of that stage, or FLOPs per token to compute total zettaFLOPs. Thus the documents do not contain enough data to calculate the requested value."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8×80G) servers.""",The context states that each of the 24 servers contains 8 A800 GPUs. Multiplying 24 servers by 8 GPUs per server yields a total of 192 GPUs used for training the FLM‑101B model.
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement of the total number of floating point operations used to train GPT-3, so it cannot be answered with confidence from the available information."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The cited document explicitly states that training FAIR’s RoBERTa on the 160 GB dataset required approximately 25,000 GPU hours."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emission for training a Transformer with NAS (626,155 lbs) but do not contain a specific emissions‑to‑driving‑distance ratio or sufficient data to calculate an equivalent driving distance in miles. Therefore, a confident answer cannot be derived from the supplied information."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context from the Griggs et al. (2024) documents does not contain a clear numeric value for the normalized on-demand hourly price of an H100 GPU, so the answer cannot be confidently extracted."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,16 completions,16,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""the inference server energy consumption for a much smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request"" (Doc li2025b) and the U.S. water‑usage efficiency factor is 3.14 L/kWh (Doc li2025b). 0.010 kWh × 3.14 L/kWh ≈ 0.0314 L per medium‑length GPT‑3 completion; a 500 mL bottle contains 0.5 L, so 0.5 L ÷ 0.0314 L ≈ 16 completions.","The per‑completion water use is obtained by multiplying the reported 0.010 kWh per request by the U.S. WUE of 3.14 L/kWh, yielding ~0.0314 L. Dividing the bottle’s 0.5 L by this gives roughly 16 completions."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The NAS training of a Transformer-based machine‑translation model emits about 626,155 lbs of CO₂, roughly equivalent to five average American lifetimes (i.e., five times the lifetime emissions of a U.S. car).",626155,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"""Transformer (big)   w/ neural architecture search  626,155""
""Car, avg incl. fuel, 1 lifetime  126,000""","The Strubell et al. table lists 626,155 lbs of CO₂ for a big Transformer trained with NAS. Dividing by the 126,000 lbs per-car lifetime gives ≈5 American lifetimes, matching the stated equivalence."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the amount of CO2e avoided by Amazon's on-site solar energy systems compared to nonrenewable sources, so a confident answer cannot be derived."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,CTCF (Communication‑to‑Compute Factor),CTCF,is_blank,is_blank,is_blank,"""CTCF-adjusted values closely matched the actual measurements.""","The excerpt from Kim 2025 references CTCF as a metric that adjusts computation time based on communication overhead, indicating it is used to assess the ratio of computation to communication time in distributed training scenarios."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give annual water consumption figures and general usage statistics, but they do not contain a specific calculation or statement of gallons of water consumed per ChatGPT user session in 2023."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide a clear numeric value for the energy consumption per inference of the BLOOMz-7B model, which is necessary to calculate the total energy for 1,000,000 inferences per download. Without that figure, a confident estimate cannot be made."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a specific numeric value for the H100 GPU hours consumed during the pre‑training of the JetMoE‑8B model. Therefore the answer cannot be determined with confidence from the supplied documents.
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Microsoft reporting a 34 increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons""","The context from the 2025 paper (Doc luccioni2025a) explicitly states that Microsoft reported a 34 % increase in global water consumption between 2021 and 2022, which directly answers the question."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,yielding a total of approximately 772 billion GPT-4o queries in 2025,"The document states that the projected number of GPT‑4o queries for 2025 is about 772 billion, so the total estimated count is 772 billion queries."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,homes,jegham2025,https://arxiv.org/pdf/2505.09598,"Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes.","The Jegham 2025 analysis explicitly states that 700 million daily GPT‑4o queries would use enough electricity annually to match the consumption of 35,000 U.S. residential households, confirming the numeric answer."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents mention Yelp sentiment analysis benchmarks or provide accuracy comparisons between traditional models and large language models for such benchmarks.
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any information about the number of AI training runs that used renewable-only power in 2022, so the answer cannot be determined with confidence."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29%–49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""RD staff costs including equity are between 29 and 49 of the total amortized cost.""","The cited passage from Cottier et al. states that for the four models (GPT‑3, OPT‑175B, GPT‑4, Gemini Ultra) RD staff costs with equity comprise a range of 29% to 49% of the total amortized cost, which directly answers the question."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""53 of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search""","The passage states that out of 100 analyzed news articles, 53 cited the contested 3 Wh/10‑times‑Google‑search estimate, yielding a 53% share."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context contains explicit energy consumption figures for Llama 3.1 70B on one node versus two nodes, nor a comparison that would allow calculation of a factor increase."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about the JetMoE-8B model or its OpenLLM Leaderboard score.
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided passages explicitly state the U.S. national average PUE for 2020, so the value cannot be confidently extracted from the documents."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain a specific numeric value for the KV Cache size for OPT-2.7B at batch size 32, so the answer cannot be determined with confidence from the given documents."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"""proposes the Compute Time Calibration Function (CTCF) to accurately determine the optimal instance based on user requirements.""","The context explicitly states that the proposed function is the Compute Time Calibration Function (CTCF), which adjusts predictions to align theoretical GPU performance with actual measurements, thereby improving instance selection accuracy."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,luccioni2024;morrison2025,https://arxiv.org/pdf/2311.16863;https://arxiv.org/pdf/2503.05804,"""used the Code Carbon package to measure both the energy consumed and the carbon emitted during inference."" and ""measure cumulative energy consumption using CodeCarbon tracking,""","Both documents state that the energy consumption during inference runs was measured with the CodeCarbon software package, directly supporting the answer."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,HGX,HGX,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high speed interconnect between each node, and between 2 and 128 nodes concurrently per training run.""","The quoted sentence from the Morrison 2025 document shows that large language models were deployed across multiple GPUs and nodes using the NVIDIA HGX server framework, indicating HGX as the deployment framework."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,True,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Open‑source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).","The quoted passage states that open‑source general‑purpose AI models are exempt from transparency (including energy consumption reporting) unless they pose a systemic risk, which directly supports the claim that the exemption applies with that condition."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,TRUE,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""increasing the number of shards still tends to increase the energy costs of inference per response""","The quoted sentence explicitly states that adding more GPU shards raises the energy cost per response for LLaMA‑65B, confirming the statement as true."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include any specific mention of the PUE value for Google's Iowa data center during the run of the Evolved Transformer, so the answer cannot be determined from the documents."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk... However, it does not mandate the disclosure of energy consumption.""","The quoted passage explicitly states that open‑source general‑purpose AI models are excluded from the requirement to disclose energy consumption under current EU rules, making the statement false."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a;wu2021b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2108.06738,"In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30 of all PPAs purchased by corporations worldwide.  In 2020, Amazon, Microsoft, Meta, and Google ... accounting for 30 of the cumulative total from corporations globally.","Both cited documents report that the four companies represented 30% of all corporate PPAs in 2020, making 30% the correct percentage."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts mention that the FLM-101B model has a pre-training carbon footprint that is one‑tenth of a typical LLM, but they do not give an explicit numeric value in metric tons of CO2 equivalent. Therefore the documents do not contain enough information to determine the exact total emissions."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,approximately 14.8 times,14.8,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094 … o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082""","The long‑prompt energy for GPT‑4.1 nano is 0.827 Wh, while for o3 it is 12.222 Wh. Dividing 12.222 by 0.827 gives roughly 14.8, so o3 consumes about 14.8 times more energy than GPT‑4.1 nano for a long prompt."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Importantly, the health costs are unevenly distributed across counties and communities, particularly affecting low-income counties that could experience approximately 200x per-household health costs than others.","The documents explicitly state that public health costs are unevenly distributed, with a ratio of about 200 between the highest and lowest county-level per-household health costs, indicating that the costs are not evenly spread across communities."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",6.7 billion US dollars,6.7,billion US$,han2024,https://arxiv.org/pdf/2412.06288,"""total public health cost of about 6.7 billion, or 47.5 per household, in 2023""","The context explicitly states that the total public health cost of U.S. data centers in 2023 is about 6.7 billion dollars, which matches the average attribution method figure."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",TRUE,1,is_blank,is_blank,is_blank,"""As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years.""","The quoted sentence directly states that GPU theoretical performance per watt doubles approximately every 3‑4 years, matching the claim in the question. No contradictory information is present in the provided context."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided excerpts do not contain a specific numeric value for the total execution time in seconds of a sparse Mixtral model fine‑tuned with a batch size of 1 on an NVIDIA A40‑48 GB GPU. Therefore the answer cannot be determined from the documents alone.
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"We use Ollama for local AI model deployment, which ensures data privacy by processing entirely on-device.","The cited passage explicitly states that Ollama was employed for local deployment, and earlier context notes that 4‑bit quantization was applied through the same open‑source platform, identifying Ollama as the tool used for both quantization and local inference in the financial sentiment case study."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied context does not contain any information about a storage service used for sharding and streaming datasets for spot VMs that can terminate at any time.
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages contain the specific cost‑reduction percentage range for Mélange on short‑context Arena workloads with a 120 ms SLO, so the answer cannot be determined from the given documents."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a statement from Chen et al. (2025) specifying the price per hour for an NVIDIA H100, so the answer cannot be determined with confidence from the given documents."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,4 years,4,years,morrison2025,https://arxiv.org/pdf/2503.05804,"Internally, we assume a 4 year lifespan for our GPUs","The document states an internal assumption of a 4‑year lifespan for GPUs, which serves as the estimated average lifetime before retirement in AI data centers for 2024."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,2,is_blank,is_blank,is_blank,"""13B  2  64""",The table in Doc [samsi2024#0016] lists the bare minimum number of V100 GPUs required for each LLaMA model; for the 13B model it shows a count of 2 GPUs.
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain any statement about a nearly linear relationship between runtime and energy consumption for inference experiments with large language models, so the question cannot be answered with confidence from the given documents."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,strubell2019,https://arxiv.org/pdf/1906.02243,"""report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).""","The cited Strubell 2019 paper explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for a period of 2 weeks, which equals 336 hours. This directly answers the training duration question."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, which is over four times the five cars estimate.",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons COe, over 4x the estimate that forms the basis for the five cars number.""","The context states the Gemma pre‑training emissions as 1247.61 tons COe and explicitly says this is over four times the five cars estimate, allowing us to report the figure and its comparison."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,wu2021b;dodge2022,https://arxiv.org/pdf/2108.06738;https://arxiv.org/pdf/2206.05229,"""PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021)"" and ""Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021""","Both cited documents report Google’s hyperscale data centers having a PUE of 1.10 in 2021, so the answer is 1.10."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.011 kL,0.011,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"""Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg COeq , and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.""","The passage states that mining 1 kg of rare earth uses 11 kL of water. An H100 GPU contains 0.1 % rare earth by mass, i.e., 0.001 kg of rare earth per GPU. Multiplying 0.001 kg by 11 kL/kg gives 0.011 kL of water consumed in mining per GPU."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",75%,75,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""GPU accounts for almost 3/4 of electricity consumption.""","The Dodge 2022 study reports that GPUs consume roughly three‑quarters of the total power in a typical data‑center server, which corresponds to about 75 % of the provisioned power."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific percentage for the reduction in carbon footprint when customers move workloads from on-premises data centers to AWS in North America. Therefore, I cannot provide a confident answer based on the documents given."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Senator Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""",The context explicitly names Senator Edward J. Markey as the introducer of the bill on 1 Feb 2024.
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific net cost figure or the necessary throughput and pricing details needed to calculate the cost of fine‑tuning a sparse Mixtral model with 2 million queries on an NVIDIA H100 GPU. Therefore, the answer cannot be determined from the given documents."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""A single short GPT-4o query consumes 0.42 Wh ( Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40.""","The context from Doc jegham2025 explicitly states that a short GPT‑4o query consumes 0.42 Wh, which directly answers the question."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"""When actively training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.""",The context explicitly states that during active training the average GPU power exceeds 600 W. This directly answers the question about the average GPU power for a single node during the first 300 logging steps while actively training.
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a numeric value or comparison for the average system power per processor for TPU v2 versus the V100 GPU, so a confident answer cannot be derived from the available information."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 GPUs,96,GPUs,morrison2025,https://arxiv.org/pdf/2503.05804,"""Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high speed interconnect between each node, and between 2 and 128 nodes concurrently per training run.""","The context states that each server (node) contains 8 H100 GPUs. With 12 nodes in the training cluster, the total number of GPUs is 12 × 8 = 96."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""reporting the financial cost or price tag of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.""","The context from the Schwartz 2019 paper explicitly states that Green AI includes reporting the financial cost of developing, training, and running models, supporting the claim that the statement is true."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages contain a figure for global average CO2e emissions per human per year, so the answer cannot be derived from the provided documents."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""current averages of less than 3 years for cell phones""","The provided passage from Doc [wu2021b] explicitly states that cell phones currently average lifetimes of less than 3 years, which supports the claim that smartphones have short lifespans contributing to e‑waste concerns."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25 times,1.25,times,samsi2024,https://arxiv.org/pdf/2310.03003,"particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100","The quoted passage states that for the 13B model the inference latency (and thus throughput) increased by about 1.25‑fold when using A100 versus V100, indicating a ~1.25× speedup in throughput."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any statement about intra‑zone scaling with T4 GPUs or the per‑GPU speedup for computer vision models, so the truth value cannot be determined from the documents."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""reaching 62 million tonnes in 2022.""",The 2025 paper cites that electronic waste worldwide reached 62 million tonnes in 2022.
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"The FLM-101B model is structured with a hidden state dimension of , a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of .","The document explicitly states that the FLM‑101B model’s context window is 2,048 tokens, which directly answers the question."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents compare LLM inference power draw to diffusion models or discuss decoding bottlenecks, so the statement cannot be verified."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,A10G,A10G,is_blank,is_blank,is_blank,"""smaller sizes are best served on A10G, and larger sizes are best served on A100.""",The quoted passage indicates that for very small request sizes—such as those generating a single classification token—the A10G GPU provides the most energy‑efficient performance compared to higher‑end GPUs.
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not contain explicit information about the size increase of Facebook's recommendation and ranking models between 2019 and 2021.
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the percentage of Amazon's People Managers globally who identified as women in 2023.
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context documents do not contain any information about the number of packages Amazon delivered via electric vehicles in Europe in 2023, so the answer cannot be determined with confidence from these sources."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The context excerpts do not contain a clear statement indicating which specific model achieved the highest rank in the eco‑efficiency DEA analysis.
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,700 kiloliters,700,kL,jegham2025,https://arxiv.org/pdf/2505.09598,"""Training GPT-3 is estimated to consume 1,287 megawatt‑hours (MWh) of electricity and emit over 550 metric tons of CO equivalent (), while requiring more than 700 kiloliters (kL) of water for cooling alone ,""","The cited passage states that training GPT‑3 in Microsoft’s U.S. data centers uses more than 700 kL of water for cooling, which is directly evaporated freshwater. This figure is taken as the answer to the question."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context mentions that the EPA provides an average CO2 value per kilowatt-hour for U.S. power consumption, but the actual numeric value is not included in the excerpts. Therefore, the documents do not contain enough information to answer the question confidently."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""metrics such as accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between en..."" and ""metrics like F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications...""","The quoted passages from the Khan 2025 documents state that accuracy and F1 scores can decline after optimization, contradicting the claim that they always improved. Therefore the statement is false."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about Yelp sentiment analysis benchmarks or a comparison between traditional models and large language models for that task. Therefore the truth value cannot be determined from the documents.
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",about 15%,15,%,kim2025,https://arxiv.org/pdf/2504.11816,"""total cost increased to 2.699, resulting in about a 15 higher cost.""","The Max-Performance instance cost 2.699 versus InferSave’s top choice cost 2.344, a difference of 0.355 which is roughly 15% higher."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""for very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30 in multiple regions, and up to 80 in West US;""","The context explicitly states that for the short DenseNet 201 job the maximum potential reduction in the West US region is 80%, which is the highest value mentioned for that region."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided context documents contain a statement specifying the number of Amazon Renewable Energy Projects announced in the United States as of January 2024. Therefore the answer cannot be determined from the supplied material.
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Amazon's AWS (the largest cloud computing platform) only covered fifty percent of its power usage with renewable energy""","The context explicitly states that Amazon’s AWS covered fifty percent of its power usage with renewable energy, which corresponds to 50%."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,The GPU alone accounts for 74 of the total energy consumption due to these components.,"The context explicitly states that in the BERT-base training experiment, the GPU accounts for 74% of the total electricity consumption."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,836,000 tokens",1836000,tokens,kim2025,https://arxiv.org/pdf/2504.11816,"""ut tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.""","The online inference workload uses 100 input tokens + 512 output tokens per request, totaling 612 tokens per request. With 3000 requests, the total processed tokens are 612 × 3000 = 1,836,000 tokens."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",47% to 64%,"[47, 64]",%,cottier2024,https://arxiv.org/pdf/2405.21015,Computing hardware makes up 4764 of the full model development cost,"The cited passage states that computing hardware accounts for 47–64% of the total amortized cost for the four key models, and this figure applies when equity is excluded from RD staff costs."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",over 1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""from the least energy‑intensive task, text classification, with mean consumption of 0.002 kWh per 1,000 inferences, to the most energy‑intensive one, image generation, whose mean consumption is 2.9 kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.""","The quoted passage directly states that image generation consumes 2.9 kWh versus 0.002 kWh for text classification, yielding a factor greater than 1450. Thus the energy required for image generation exceeds that of text classification by about 1450 times."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention the quantity of fiber optic cable installed for AI workloads, so the answer cannot be derived from the documents."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear statement indicating the exact number of NVIDIA A100‑80GB GPUs required to serve a Llama2‑70b model at BF16 precision.
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any information about the energy consumption of the two preemption mechanisms—recomputation or swapping—when an LLM inference server is overloaded, so the answer cannot be determined from the documents."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Require energy consumption to be measured and reported at the cumulative server level.
Energy consumption should be reported at the cumulative server level (see also ).","The cited passages explicitly state that the authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility, and advise against more granular or higher‑level measurements."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",FALSE,0,is_blank,ebert2024;luccioni2025a,https://arxiv.org/pdf/2410.06681;https://arxiv.org/pdf/2501.16548,"""The AI Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration )"" (ebert2024).","The documents state that the AI Act does not mandate disclosure of greenhouse gas emissions for AI applications such as oil and gas exploration, indicating that the statement is false."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,800 million dollars,800000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""we estimate that it cost 800M to acquire the hardware used to train GPT‑4, compared to 40M for the amortized hardware CapEx + energy cost.""",The cited sentence from the Cottier 2024 document gives the estimated upfront hardware acquisition cost for GPT‑4 as 800 million dollars.
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""The umbrella term `Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.""","The passage states that Sustainable AI was proposed to encompass both climate-positive AI applications and improving the environmental sustainability of AI itself, not only climate-positive uses. Thus the statement is false."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention of McKinsey projections or the percentage of U.S. national electricity consumption that data centers are expected to account for in 2030. Therefore, the answer cannot be determined from the documents given."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google reports a 48 increase in GHG emissions since 2019""","The passage from the 2024 environmental sustainability report states that Google reported a 48% rise in GHG emissions since 2019, which directly answers the question."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents mention JetMoE-8B, Llama2-7B, or any comparison of their inference computation, so the requested percentage cannot be determined from the provided information."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages contains a projected range of electricity consumption by global AI for the year 2027. The closest figures refer to different years (e.g., 2026 or 2028) or to U.S. data center consumption rather than global AI consumption."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the maximum batch size for fine‑tuning a Mixtral model on an NVIDIA A100‑40GB GPU, nor do they mention a Mixtral model explicitly. Therefore the answer cannot be determined with confidence from the given context."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the number of hectares occupied by new AI data centers in 2022, so a confident answer cannot be derived."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement regarding the performance impact of intercontinental training for CV models with high granularity, nor does it mention a 7% slowdown. Therefore, there is insufficient information to answer the question."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b;strubell2019,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/1906.02243,"""the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of emissions.""","The quoted passage from the 2019 study by Strubell et al. explicitly states that training BERT produced 626,155 pounds of CO₂e emissions, which directly answers the question."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear numeric statement of the total operational energy footprint reduction achieved by Facebook between 2019 and 2021. Therefore, a confident answer cannot be derived from the documents."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so a confident answer cannot be derived from the documents."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""peaked in 2022, with 10 of notable models that year releasing some degree of information.""","The context explicitly states that the practice of directly releasing environmental information peaked in 2022, indicating that year as the peak before a subsequent decline."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided context passages contain a numeric estimate of the full GPT‑3 training energy in MWh.
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g CO2eq,0.32,g CO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"""emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for GPT‑3 and 4.67g for PaLM""","The context states that the BERT‑based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 g CO2eq per 1,000 text classification queries, which directly answers the question."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain projections for 2028 but do not mention any projected total public health burden for 2030, so the answer cannot be determined from the supplied information."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",19 thousand grams,19,thousand grams,dodge2022,https://arxiv.org/pdf/2206.05229,"""7k grams vs.26k grams, for the most effic…""",The context gives a direct comparison of emissions for the most efficient region (7 k g) and the least efficient region (26 k g). Subtracting gives a difference of approximately 19 k g.
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2,2,samples,xia2024,https://arxiv.org/pdf/2408.04693,Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse. -D -S -D -S 2 8 6 20,"The table in the provided context lists the maximum batch sizes for dense (D) and sparse (S) fine-tuning. The first pair of numbers (2 and 8) corresponds to the dense setup for Mixtral on the Hellaswag dataset, giving a maximum batch size of 2 samples."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",True,1,is_blank,is_blank,is_blank,"""MoE layer is the most time‑consuming, accounting for 85% of the overall execution time on average.""  ""The execution time of LLM fine‑tuning is dominated by the MoE layer.""","The cited passages identify the MoE layer as the largest cost in fine‑tuning, implying it is a primary target for performance improvement."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not contain a specific execution time for a sparse Mixtral model fine-tuned on an NVIDIA A40-48GB with batch size 10, so the answer cannot be determined with confidence."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""finding that training accounted for only half of the model's overall emissions""","The 2023 article notes that training represented half of the overall emissions, i.e., 50% of the BLOOM model’s total carbon footprint."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",9.6%,9.6,%,dodge2022,https://arxiv.org/pdf/2206.05229,PR   7.6  9.2  9.6  1.6  2.0  2.4  9.2  9.3  9.2  9.6  9.6,"The table for the Pause & Resume (PR) optimization lists a 9.6 % emissions reduction for the 6B parameter transformer, which is the highest value reported for that model in the study."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit estimate of the total cost (in dollars, GPU hours, or any other metric) for fine‑tuning a Mixtral model on GSM8K with sparse MoE on an NVIDIA A40‑48GB GPU, so a confident answer cannot be derived from the documents alone."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",8,8,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Maximum batch size supported by LLM fine‑tuning; D: dense and S:sparse.
-D  -S  -D  -S
  2  8  6  20","The table lists the maximum batch sizes for dense (D) and sparse (S) fine‑tuning. For the Mixtral‑8x7B (first pair), the sparse configuration supports a batch size of 8 samples, which is the longest running MoE layer for that model on an NVIDIA A40‑48 GB."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the total energy consumption of training the FLM-101B model, so an answer cannot be given with confidence."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention the JetMoE-8B model or its GSM8k benchmark score, so the information required to answer the question is missing."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",2738 days,2738,days,is_blank,is_blank,"""the amount of water consumed by one average person in the United States in about 7.5 years.""","The context states that training a sub‑billion‑parameter OLMo model (such as the 60 M model trained on 1.7 trillion tokens) consumes water equivalent to 7.5 years of water use for an average U.S. person. 7.5 years ≈ 7.5 × 365 ≈ 2738 days, which is the required answer."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,is_blank,is_blank,totaling 8.3 metric tons per year,"The context from Doc [dodge2022#0065] states that the average U.S. home’s yearly energy use results in 8.3 metric tons of CO₂ emissions, directly providing the required estimate."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"7B   1  64  1  64 
The 7B model was run on a single GPU","The table for bare‑minimum hardware lists 1 V100 GPU for the 7B model, and the text confirms that the 7B inference was executed on a single GPU, so the minimum required number is one."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10–50 user requests,"[10, 50]",is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water.""","The context from Doc luccioni2025a states that 10–50 GPT‑3 queries use roughly 0.5 L of water, which matches a 500 ml bottle. Thus, between 10 and 50 user requests would consume that amount of water during training."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.""","The cited passage from ebert2024 explicitly states that GPU-level power monitoring is not recommended for overall AI energy reporting, contradicting the claim that it is the preferred method."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages discuss the computational intensity of MoE layers and cost-related terminology, but none explicitly state that adding compute resources to accelerate MoE layers during fine-tuning would increase overall costs. Therefore the answer cannot be determined with confidence from the given documents."
