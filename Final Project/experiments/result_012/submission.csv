id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The question asks for the percentage time saved using the growth strategy for the 101B model. Doc [li2025a] explicitly states a 72% time-saving, so the answer is 72%."
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,Doc [wu2021b]: 'the total energy consumption of the US data centers increased by about 4% from 2010-2014',"The question asks for the average increase in U.S. data center electricity consumption between 2010 and 2014. Doc [wu2021b] explicitly states that the total energy consumption of U.S. data centers increased by about 4% during that period, providing the required figure."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021;luccioni2023,https://arxiv.org/pdf/2104.10350;https://arxiv.org/pdf/2302.08476,"Doc [patterson2021]: 'It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.'","The question asks for the training duration using 10,000 V100 GPUs at 24.6‚ÄØTeraFLOPS/sec. Doc [patterson2021] explicitly states that this setup takes ~14.8 days, and Doc [luccioni2023] corroborates the 14.8‚Äëday figure, so the answer is 14.8 days."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24 data centers,24,data centers,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.'","The question asks how many data centers used recycled water for cooling in 2023. The cited passage from Doc [amazon2023] states that the number increased to 24, so 24 is the correct answer."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Amazon Solar Farm Maryland‚ÄìCPV Backbone will avoid more than 64,000 metric tons of CO2e each year‚Äîthe equivalent of taking more than 13,900 cars off the road.'","The question asks for the number of cars equivalent to the avoided CO2e. Doc [amazon2023] explicitly states that the solar farm‚Äôs avoided emissions are equivalent to taking more than 13,900 cars off the road, so 13,900 is the answer."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",3700 kg CO2eq,3700,kg CO2eq,is_blank,is_blank,"Doc [morrison2025#0048]: 'Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq'","The question asks for the estimated embodied carbon emissions per GPU. Doc [morrison2025#0048] states that lacking NVIDIA data, they assume the Luccioni et al. estimate of 3700‚ÄØkg CO2eq per GPU, which directly answers the query."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,Doc [morrison2025#0138]: 8 0.036 12.0 0.054 12.64 21.5 bil.,"The question asks for the GPU power usage at an 8‚ÄØreq/s request rate for the Llama‚ÄØ3.2‚ÄØ1B model during the SGLang benchmarking on 2400 ShareGPT prompts.  Doc [morrison2025#0138] lists the GPU Power Usage for 8‚ÄØreq/s as 0.036‚ÄØkWh, so that is the answer."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold improvement",6750,fold,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.'","The question asks for the fold increase in clock speed from 1971 to 2021. Doc [wu2021b] states the improvement is more than 6,750 fold, so the answer is 6,750 fold."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 tCO2e,1.2,tCO2e,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: 'single passenger round trip SF-NY is ~1.2t CO 2 e (Table 2).',"The question asks for the approximate CO2e of a single passenger round trip between SF and NY. Doc [patterson2021] explicitly states that this trip emits about 1.2‚ÄØtCO2e, so the answer is 1.2‚ÄØtCO2e."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide premature death estimates for 2028 but do not contain any information about scope‚Äë2 premature deaths projected for 2030, so a confident answer cannot be derived from the given context."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: The UN‚Äôs Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.,"The question asks for the percentage of e-waste formally collected and recycled according to the UN‚Äôs Global E-Waste Monitor 2024. Doc [luccioni2025a] explicitly states that about 22% of e-waste has been formally collected and recycled, so 22% is the answer."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a monetary total training budget for FLM-101B, so the factor cannot be determined from the given context."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"Doc [dodge2022#0056]: 'This model was not trained to completion, but only until 13%; a full training run would take 60 days.'","The question asks for the estimated duration of a full training run for the 6.1‚ÄØbillion‚Äëparameter model. Doc [dodge2022#0056] explicitly states that a full training run would take 60 days, which directly answers the query."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'The total permitted site-level annual emission limits are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.' Doc [han2024#0046]: 'The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.'","The question asks for the total permitted annual NOx limits for data center backup generators in Northern Virginia during 2023‚Äë2024. Both Doc [han2024] and Doc [han2024#0046] state that the site‚Äëlevel annual limits are approximately 13,000 tons of NOx, which directly answers the question."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of  2014, is a long-term investigation of the field of Artificial Intelligence...'","The question asks for the launch year of the study. Doc [stone2022] states that the study was launched in the fall of 2014, so 2014 is the correct year."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.25%,0.25,%,is_blank,is_blank,Doc [patterson2021#0065]: 'Percent of model activated on every token 100% 100% 100% 0.25% 0.10% 100%',"The question asks for the activation percentage of the Switch Transformer with 1500‚ÄØB parameters. In Doc [patterson2021#0065] the table lists 0.25% activation for the 1500‚ÄØB model, so that is the answer."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'h GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).'","The passage lists the GPU energy consumption for 1,000 queries and identifies Command‚ÄëR‚ÄØPlus as the highest at 3,426‚ÄØWh, so it is the model with the greatest GPU energy use among those in the appendix."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts per MoE layer,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'set the same number of experts to 8 and top-k to 2 for every layer.',"The question asks for the number of experts in each Mixture‚Äëof‚ÄëExperts layer of JetMoE‚Äë8B. Doc [shen2024] explicitly states that each layer is configured with 8 experts, so the answer is 8 experts."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention a hardware processor used for energy-efficient local inference in financial sentiment classification.
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).',"The question requests the English portion cost in zettaFLOPs. Doc [li2025a] directly states 28.22 zettaFLOPs for English, so that is the answer."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.94 score,43.94,score,is_blank,is_blank,"Doc [li2025a#0053]: 'Results. On average, FLM-101B achieves a score of 43.94,'","The question asks for the final average performance score of FLM-101B on the Open LLM Leaderboard. The context explicitly states that FLM-101B achieves an average score of 43.94, which directly answers the question."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,1.2x to 4x,"[1.2, 4]",x,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024#0030]: 'a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.'","Cottier et al. (2025) report that the ratio of total compute for model development to the compute of the final training run falls in the range 1.2x to 4x, as stated in the cited passages."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons‚Äô Paradox,Jevons‚Äô Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a#0002]: 'ines how the problem of Jevons‚Äô Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.'","The question asks which economic principle explains that technical efficiency gains may not lead to net environmental benefits. Doc [luccioni2025a#0002] explicitly states that Jevons‚Äô Paradox applies to AI, and Doc [luccioni2025a] also references Jevons‚Äô Paradox, supporting the answer."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The question asks for the total wall‚Äëclock time to train FLM‚Äë101B using the growth strategy. Doc [li2025a] explicitly states that the total time cost is 21.54 days, which directly answers the query."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement of the total execution time for a dense BlackMamba model with batch size 30 fine-tuned on an NVIDIA A40 GPU, so I cannot answer the question with confidence."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3√ó,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3√ó.'","The question asks for the factor by which the overall carbon footprint decreases when GPU utilization is raised to 80% for LM training. Doc [wu2021a] explicitly states that the footprint decreases by 3√ó, so the answer is a factor of 3."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,Doc [schwartz2019]: 'Red AI is on the rise despite the well-known diminishing returns of increased cost',"The statement claims Red AI is declining, but Doc [schwartz2019] explicitly states that Red AI is on the rise. Therefore the statement is false."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,1287 MWh,1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity'","The question asks for the training electricity consumption of GPT-3. All cited documents report that the training consumed 1,287‚ÄØMWh, so the answer is 1287‚ÄØMWh."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 ‚Äì 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'More critically, the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 ‚Äì 6 Denmark or half of the United Kingdom.'","The question asks for the projected water withdrawal in 2027. Doc [li2025b] explicitly states that global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027, so that range is the answer."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,is_blank,is_blank,"Doc [luccioni2024#0029]: 'we sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or multi-task models, to allow comparisons both for different architectures on a given task and between tasks for the same architecture.'","The passage from Doc [luccioni2024#0029] explicitly states that 88 models were sampled and analyzed in the Power Hungry Processing study, so the answer is 88 models."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.'","The question asks for the name of a collaborative project that aims to create a standardized method for comparing inference efficiency. Doc [luccioni2025c] explicitly names the AI Energy Score project as that collaborative effort, so the answer is AI Energy Score."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific numeric value for the energy consumption of pre-training the BLOOM model in MWh, so I cannot answer the question with confidence."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22 regions,22,is_blank,is_blank,is_blank,Doc [amazon2023#0277]: '100% of the electricity consumed by¬†22¬†AWS data center regions is matched with renewable energy',"The question asks how many AWS data center regions had 100% renewable-matched electricity in 2023. Doc [amazon2023#0277] explicitly states that 22 regions achieved this, so the answer is 22 regions."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts per layer,2,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: the same number of experts to 8 and top-k to 2 for every layer.,"The question asks how many experts are activated for each token in each layer of JetMoE-8B. Doc [shen2024] states that the model uses top‚Äëk‚ÄØ=‚ÄØ2 for every layer, so two experts are selected per token per layer."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,is_blank,is_blank,"Doc [wu2021b#0060]: 'the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].'","The question asks for the reported drop in global carbon emissions in 2020, and Doc [wu2021b#0060] explicitly states that emissions dropped by 6.4%."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019#0008]: 'the amount of compute used to train deep learning models has increased 300,000x in 6 years.'","The documents report a 300,000√ó increase in compute over six years, not the 200,000√ó stated in the question, so the statement is false."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",False,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Fig. 10. A vast majority of model experimentation (over tens of thousands of training workÔ¨Çows) utilizes GPUs at only 30-50%, leaving room for utilization and efÔ¨Åciency improvements.' Doc [wu2021a#0096]: 'Figure¬†10 illustrates the utilization of GPU accelerators in Facebook‚Äôs research training infrastructure. A signiÔ¨Åcant portion of machine learning model experimentation utilizes GPUs at only 30‚Äë50%, leaving signiÔ¨Åcant room for improvements to efÔ¨Åciency.'","The documents state that the majority of experimentation workflows use GPUs at only 30‚Äë50% (or 30‚Äë60% average), not over 80% capacity, so the statement is false."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,69 years,69,years,stone2022,https://arxiv.org/pdf/2211.06318,Doc [stone2022]: 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.',"The question asks for the field‚Äôs age in 2025. Doc [stone2022] states that AI was born in 1956; subtracting 1956 from 2025 yields 69 years, which is the approximate age."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.593 MWh,103.593,MWh,is_blank,is_blank,"Doc [dodge2022#0054]: 'we estimate a full training run would consume approximately 103,593 kWh.'","The question asks for the total energy consumption of a full training run of a 6.1‚ÄØbillion parameter transformer model. Doc [dodge2022#0054] states the full run would consume approximately 103,593‚ÄØkWh. Converting kWh to MWh gives 103,593‚ÄØkWh √∑ 1,000 = 103.593‚ÄØMWh, which is the answer."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not give a specific energy savings percentage for a 100‚ÄØms TPOT target, only for a 77‚ÄØms target."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the documents mention the percentage of AI inference workloads powered by coal in Asia for 2023.
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.'","The question asks for Amazon's energy storage capacity in 2023. Doc [amazon2023] explicitly states that they now hold 1.3 GW of storage capacity, which provides the required answer."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B parameters,2,B,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token.'","The question asks how many parameters are active per token during inference. Doc [shen2024] explicitly states that JetMoE-8B activates 2B parameters for each input token, so the answer is 2B parameters."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give a specific numeric value for the execution time of the longest MoE kernel for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48 GB GPU.
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,Doc [stone2022]: 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.',"The question asks for the year the field was officially christened. Doc [stone2022] explicitly states that this occurred in 1956, so that is the answer."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",training and fine-tuning a large Transformer model with Neural Architecture Search (NAS),training and fine-tuning a large Transformer model with Neural Architecture Search (NAS),is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"Doc [luccioni2023]: 'The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars'","The five cars estimate originates from the 2019 Strubell et al. study and is specifically based on the emissions from training and fine‚Äëtuning a large Transformer model using Neural Architecture Search (NAS), an infrequently performed AI process."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,approximately 272 transatlantic flights,272,transatlantic flights,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'cumulative emissions from approximately 272 transatlantic flights between Boston and London.',"The question asks how many transatlantic flights have emissions comparable to GPT‚Äë4o‚Äôs annual inference emissions. Doc [jegham2025] explicitly states that the emissions are equivalent to the cumulative emissions of approximately 272 transatlantic flights, so the answer is 272 flights."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not specify a global average PUE for AI-dedicated data centers in 2023.
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.'","The question asks for the percentage reduction in RM2 model size when quantized from 32‚Äëbit to 16‚Äëbit. Doc [wu2021a] explicitly states a 15% reduction, so 15% is the answer."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency]'.","The question asks whether the claim that roughly 770‚ÄØmillion people lack stable electricity supply is correct. Doc [wu2021b] states exactly that, so the statement is true."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention an ""o3 model"" or give its energy consumption for a long prompt, so I cannot answer this question based on the available context."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules per token,"[3, 4]",Joule,is_blank,is_blank,"Doc [samsi2024#0058]: 'for length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.'","The question asks for the energy per token at a max generation length of 512 tokens. Doc [samsi2024#0058] explicitly states that the energy cost per output token is about 3‚Äì4 Joules for length 512, so the answer is 3‚Äë4 Joules per token."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not contain a specific numeric value for the GHG emissions of pre-training the Llama 7B model, so the answer cannot be determined from the given context."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",True,1,is_blank,is_blank,is_blank,"Doc [patterson2021#0002]: 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy'","The question asserts that sparsely activated DNNs use less than one‚Äëtenth the energy of dense DNNs while maintaining accuracy. Doc [patterson2021#0002] states exactly that, confirming the claim as true."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific numeric value for the maximum batch size of BlackMamba in sparse fine‚Äëtuning on the GSM8K dataset with an NVIDIA A40 GPU.
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","25,000 USD",25000,USD,is_blank,is_blank,"Doc [schwartz2019#0023]: 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.'","The question asks for the estimated training cost of Grover on 256 TPU chips for two weeks. Doc [schwartz2019#0023] explicitly states that this cost is $25,000, so the answer is 25,000 USD."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: ‚ÄòThe reasoning behind the 5‚Äë10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG‚Äôs exp.‚Äô,"The question asks whether the 5‚Äë10% AI emissions‚Äëreduction claim is backed by clear, publicly available calculations. Doc [luccioni2025c] states that the reasoning is unclear and the calculations are not detailed, indicating a lack of transparent, sound scientific grounding. Therefore the claim is not supported, so the answer is False."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The context does not contain the energy figure for Flan‚ÄëT5‚Äëxxl or any related data needed to compute the daily consumption.
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: The average data center PUE in 2023 was 1.58 globally[74],"The question asks for the average global data center PUE in 2023, and Doc [ebert2024] explicitly states that the average PUE was 1.58, which is the value used in the answer."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1).,"The question asks for the optimizer stage share of running time for BlackMamba sparse fine‚Äëtuning on an NVIDIA A40‚Äë48GB GPU with batch size 1. The quoted passage from Doc [xia2024] states that this share is up to 53%, which directly answers the question."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,is_blank,is_blank,Doc [cottier2024#0054]: Gemini Ultra has the highest fraction of R&D staff cost at 49%,"The question asks for the R&D staff share of Gemini Ultra‚Äôs development cost. Doc [cottier2024#0054] explicitly states that Gemini Ultra‚Äôs R&D staff cost accounts for 49% of its total amortized development cost, so 49% is the correct answer."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry , and AI-s'","The question asks for the number of members in the inaugural 2015 Study Panel. Doc [stone2022] explicitly states that the panel was a seventeen‚Äëmember panel, so 17 is the correct answer."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training a'
Doc [morrison2025]: 'sions arising from energy sources used to power model training and deployment, including servers and data center cooling.'","The question asks whether operational impacts exclude GHG emissions from servers and cooling. Doc [morrison2025] states that operational impacts include GHG emissions arising from the energy sources that power model training and deployment, explicitly mentioning servers and data center cooling. Therefore the statement is false."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.'","The question asks for the proportion of a client device‚Äôs total carbon footprint that comes from manufacturing. Doc [wu2021a] explicitly states that manufacturing carbon cost accounts for 74% of the total footprint, so the answer is 74%."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Doc [chung2025#0062]: 'Generally, models with more parameters consume more energy, but this is not always the case.'","The statement claims that a larger‚Äëparameter model always uses more inference energy. Doc [chung2025#0062] explicitly says this is not always the case, so the claim is false."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the distance from Earth to the Sun, so I cannot answer this question based on the available context."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"14,380 electric delivery vans added",14380,vans,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Last Mile Electric Delivery Vehicles  By Region 2022 2023 U.S. 2,600 11,800 Europe 1,220 3,000+ India 3,800 7,200+'","The table in Doc [amazon2023] lists 2,600 US vans in 2022 and 11,800 in 2023, 1,220 European vans in 2022 and 3,000+ in 2023, and 3,800 Indian vans in 2022 and 7,200+ in 2023. The additions are 9,200 (US), 1,780 (Europe), and 3,400 (India), totaling 14,380 vans added across 2022 and 2023."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.'","The question asks whether the Study Panel is concerned about AI being an imminent threat. The cited passage from Doc [stone2022] states that the panel found no cause for concern, so the statement is false."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the CO2 emissions from OpenAI API requests in January 2024, so the question cannot be answered with confidence."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e (‚âà 40√ó the five cars estimate)",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the ‚Äúfive cars‚Äù estimate.'","The question asks for the pre‚Äëtraining emissions of Meta's Llama¬†3 family and a comparison to the five‚Äëcars estimate. Doc [luccioni2025c] states that the Llama¬†3 models emitted 11,390‚ÄØtCO2e, which is over 40 times the five‚Äëcars benchmark, directly answering both parts of the question."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9√ó,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9√ó increase in AI training infrastructure capacity over the 1.5 years.',"The question asks for the factor of increase in AI training infrastructure capacity over the 1.5‚Äëyear period. Doc [wu2021a] explicitly states that this increase was 2.9√ó, so 2.9 is the correct factor."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,is_blank,is_blank,"Doc [fernandez2025#0045]: 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, imp'",The question asks for the batching strategy that reduces idle GPU time by replacing completed requests with new ones; Doc [fernandez2025#0045] and Doc [fernandez2025#0046] explicitly name this strategy as Continuous batching.
q080,True or False: The AlphaGo program defeated the human Go champion.,True,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match'","The question asks whether AlphaGo defeated the human Go champion. Doc [stone2022] explicitly states that AlphaGo beat the human Go champion in a five‚Äëgame match, confirming the statement as true."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: The entire alignment process takes 60 H100 GPU hours.,"The question asks for the total GPU hours spent on the alignment phase, which includes both dSFT and dDPO fine‚Äëtuning. Doc [shen2024] explicitly states that the entire alignment process requires 60 H100 GPU hours, providing the required value."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to over 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)'","The question asks for the range of GPU energy usage for 1,000 inference queries according to a 2025 study‚Äôs appendix. Doc [luccioni2025c] explicitly reports this range, giving a minimum of 0.06 Wh and a maximum of over 3,426 Wh, so this is the answer."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: 'The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].',"The question asks for the gross carbon intensity of energy from the U.S. average mix in 2021. Doc [patterson2021] states it as 0.429 kg CO2e/KWh, so that is the answer."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 gCO2eq",1594,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams ofùê∂ùëÇ2ùëíùëûfor 1,000 inferences'","The question asks for the CO2eq emissions per 1,000 inferences of the stable-diffusion-xl-base-1.0 model. Doc [luccioni2024] explicitly states that this model generates 1,594 grams of CO2eq for 1,000 inferences, which is the required value."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b#0134]: 'We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist) ‚Äì ...',"The documents state that researchers do not believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed, so the correct answer is False."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,is_blank,is_blank,"Doc [chen2024#0003]: 'develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation...'","The question asks for the name of the inference system using model-attention disaggregation. All cited passages from the Chen 2024 papers state that the system is named ""Lamina"" and explicitly mention model-attention disaggregation, so the answer is Lamina."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,is_blank,is_blank,"Doc [erben2023#0021]: 'Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.' 
Doc [erben2023#0165]: 'We used the Hivemind framework for all of our experiments, as it provided the base for training on spot instances in high latency, low bandwidth networks.'","The question seeks the name of a decentralized PyTorch‚Äëbased framework used for distributed spot instance training across clouds and continents. Both Doc [erben2023#0021] and Doc [erben2023#0165] explicitly identify Hivemind as a PyTorch‚Äëbased decentralized framework employed for this purpose, so ""Hivemind"" is the correct answer."
q093,How many parameters does the largest T5 model have?,11B parameters,11.0,B,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'The largest size has 11B parameters,'","The question asks for the parameter count of the largest T5 model. Doc [patterson2021] explicitly states that the largest T5 has 11B parameters, so the answer is 11B parameters."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,1 L/kWh,1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'On average, depending on the weather conditions and operational settings, data centers can evap-orate approximately 1 ‚Äì 9 liters per kWh of server energy: 1 L/kWh for Google‚Äôs annualized global on-site water efficiency [4] and 9 L/kWh for a large commercial data center during the summer in Arizona [16].'","The question asks for the average WUE of Google‚Äôs AI‚Äëdedicated data centers. The context provides a figure of 1 L/kWh for Google‚Äôs annualized global on‚Äësite water efficiency, which is the average WUE value used for Google data centers, thus answering the question."
q096,What is the name of the emissions metric defined as 'CO‚ÇÇ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency,"The question asks for the metric name defined as CO‚ÇÇ emissions per unit of electricity consumed. In Doc [khan2025] the table lists 'Carbon Intensity' with that definition, so the answer is 'Carbon Intensity'."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [ 56].'","The question seeks the term used to broaden transparency to include socio-technical and environmental aspects. Doc [luccioni2025b] explicitly proposes ""social transparency"" for this purpose, so that is the correct answer."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810√ó,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer‚Äëbased universal translation model by 810√ó.'","The question asks for the reduction factor relative to a CPU baseline. Doc [wu2021a] states that full‚Äëstack optimization‚Äîincluding platform‚Äëlevel caching, GPU acceleration, and algorithmic changes‚Äîreduces the operational carbon footprint by 810√ó, so that is the factor to report."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20% increase,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"Doc [li2025b]: Importantly, the company‚Äôs data center water consumption increased by ‚àº20% from 2021 to 2022. Doc [luccioni2025a]: Google observed a 20% uptick in the same period.","The question asks for the percentage increase in Google's data center water consumption from 2021 to 2022. Both Doc [li2025b] and Doc [luccioni2025a] state a 20% increase, confirming the answer."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about classification experiments on German public administration texts or the accuracy of models using sentence embeddings.
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5B liters,3500000000.0,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: '3.5B Liters of water returned to communities from replenishment projects in 2023, with additional volume contracted and r'","The question asks for the amount of water returned to communities in 2023. Doc [amazon2023] states that 3.5B liters were returned, providing the exact figure needed."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems'","The question asks for the acronym of the Finnish project described in the context. Doc [luccioni2025b] explicitly names the project as ETAIROS, so that is the answer."
q094,What is the total number of parameters in the JetMoE-8B model?,8B parameters,8,B,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.'","The question asks for the total number of parameters in JetMoE-8B. Doc [shen2024] explicitly states that the model has 8B parameters, so the answer is 8B."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: line Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 16 2 1 96 192 2304 162 51.90% 51 4 2 24 192 2304 160 51.30% 101 4 4 12 192 2160 165 52.88%,"The question asks for the FLOPs utilization percentage in the final growth stage (101B). The table in Doc [li2025a] lists the utilization for the 101 stage as 52.88%, which directly answers the question."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.64,0.64,is_blank,is_blank,is_blank,Doc [erben2023#0088]: 'results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs' . Doc [erben2023#0091]: 'C-4 is slower at NLP (36%) compared to its local run (A-4)',"The documents state that training NLP across four continents (C-4) results in a 36% throughput slowdown relative to local training (A-4). Therefore, the achieved throughput is 1 - 0.36 = 0.64 of the local throughput."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,is_blank,is_blank,"Doc [cottier2024#0049]: Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The question asks for the average share of amortized hardware and energy cost attributed to AI accelerator chips. Both Doc [cottier2024#0049] and Doc [cottier2024#0050] state that on average 44% of these costs go to AI accelerator chips, so the answer is 44%."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3.7,million GPUs,is_blank,is_blank,"Doc [luccioni2025a#0081]: 'NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency.'","The 2025 paper cited in the context states that NVIDIA shipped 3.7 million data‚Äëcenter GPUs in 2024, directly answering the question."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 ¬µg/m¬≥,9,¬µg/m¬≥,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'EPA‚Äôs recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m 3, considerably higher than t'","The question asks for the EPA‚Äôs tightened primary standard for the annual average limit of PM2.5. Doc [han2024] explicitly states the limit is 9‚ÄØ¬µg/m¬≥, so that is the answer."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].'","The question asks how many print books equal the CO2 of one Kindle. Doc [luccioni2025a] explicitly states that 115 books match a Kindle‚Äôs CO2, so 115 is the answer."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Doc [rubei2025]: 'Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.',"The question asks whether custom tags can reduce energy consumption when using zero‚Äëshot, one‚Äëshot, and few‚Äëshot techniques. The cited passage from Doc [rubei2025] explicitly states that custom tags reduce energy consumption for all three techniques, confirming the statement as true."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,"Doc [wu2021a]: 'Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook‚Äôs data centers...' ; Doc [wu2021b]: 'the PUE of Facebook datacenters is 1.10 (2020)'.","Both documents state that Facebook‚Äôs data centers have a PUE of about 1.10, so the answer is 1.10."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,40M USD,40,$M,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'training costs. We find that the most expensive publicly-announced training runs to date are OpenAI‚Äôs GPT-4 at $40M...' / Doc [cottier2024#0047]: '... compared to $40M for the amortized hardware CapEx + energy cost.',"The question asks for the estimated amortized training cost of GPT-4. The cited passages from Doc [cottier2024] and its section explicitly state $40M as the amortized cost, so the answer is 40 million dollars."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024#0003]: 'per-household health burden could be 200x more than that in less-impacted communities.',"The question asks for the factor by which the per-household health burden could exceed that in less-impacted communities. Doc [han2024#0003] states that it could be 200x higher, so the answer is 200x, a dimensionless factor. The supporting quote directly provides the numeric factor."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9).' ... 'We argue that these measures should also consider environmental risks, in keeping with...' ","The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, but the context indicates that inclusion of environmental risks is a suggested improvement rather than a mandatory requirement. Therefore the statement is false."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: SLO 100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13 InferSave-2nd g6.xlarge 60 415.04 2.344 Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699","The table in Doc [kim2025] shows that for the 100‚ÄØTPS SLO, Max‚ÄëPerformance selected g6e.xlarge at $2.699 while InferSave selected g4dn.xlarge at $2.13. The difference of $0.569 is approximately 26.7% of $2.13, so Max‚ÄëPerformance was about 26.7% more expensive."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not mention the energy consumption of the DS Llama 70B model on the FKTG dataset, so I cannot answer this question with confidence."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'image generation 2.907 3.31' (Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis).","The question asks for the average energy consumption for 1,000 image generation inferences. Table 2 in Doc [luccioni2024] lists the mean energy for image generation as 2.907 kWh, so that is the required value."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about Mistral-small's emissions change after optimization in the financial sentiment classification task.
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",rebound effect,rebound effect,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b#0037]: '-the relationship between eÔ¨Éciency and sustainability is far from straightforward, given phenomena such as rebound eÔ¨Äe cts, in which improved eÔ¨Éciency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resour ces'","The question asks for the phenomenon where improved efficiency leads to higher usage and resource consumption. Doc [luccioni2025b#0037] defines this as ""rebound effects"", matching the description."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 pounds of CO2e",36156,pounds,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'American life, avg, 1 year 36,156'","The question asks for the CO2e estimate for an average American life in one year. Doc [strubell2019] lists 'American life, avg, 1 year 36,156' as the corresponding value in pounds of CO2e, so this is the answer."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a statement giving the number of parameters in the model analyzed by Dodge et al. 2022, so I cannot answer the question based on the supplied context."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'most expensive publicly-announced training runs to date are OpenAI‚Äôs GPT-4 at $40M and Google‚Äôs Gemini Ultra at $30M.',"The passage directly states that Google‚Äôs Gemini Ultra had an estimated amortized training cost of $30M, matching the question‚Äôs focus."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not mention a dataset of German nuclear waste site objection texts used in the experiments.
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific West Virginia county name with the highest projected per-household health cost for 2030, so the question cannot be answered with confidence."
q125,What is the total number of parameters in the final FLM-101B model?,101B parameters,101,B,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'Params 175B 280B 540B 130B 70B 101B',"The question asks for the total number of parameters in the final FLM-101B model. Doc [li2025a] lists the parameter counts for each model and shows that FLM-101B has 101B parameters, so the answer is 101B."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,is_blank,is_blank,"Doc [luccioni2024#0109]: 'f model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû.'","The question asks for the total energy consumed for all model experimentation and evaluation in the 2024 study Power Hungry Processing. The passage from Doc [luccioni2024#0109] explicitly states that 754.66 kWh of energy were used, which directly answers the question."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention freshwater consumption for Meta's Llama 3 inference serving clusters in 2024, so I cannot answer this question with confidence."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 A100 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'Model Size V100 32GB A100 80GB Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128',"The question asks for the bare minimum number of NVIDIA A100 80GB GPUs needed for LLaMA‚Äë13B inference. The table in Doc [samsi2024] shows that for the 13B model, the bare‚Äëminimum configuration requires 1 A100 GPU. Thus the answer is 1."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The question asks for the number of inferences needed for BLOOMz‚Äë7B to match its training and fine‚Äëtuning energy cost. In Doc [luccioni2024] the table lists the cost parity for each BLOOMz model; for BLOOMz‚Äë7B the value is 592,570,000. Thus that is the required number of inferences."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,‚âà3 passengers,3,passengers,is_blank,is_blank,"Doc [patterson2021#0244]: 'yielding 1.2t of CO2 e per passenger round trip. Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.'","The question asks how many passenger round trips between San‚ÄØFrancisco and New‚ÄØYork correspond to 3.2‚ÄØtCO2e. Doc [patterson2021#0244] explicitly states that 3.2‚ÄØtCO2e is equivalent to about 3 passengers on that route, so the answer is 3 passengers."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,is_blank,is_blank,"Doc [luccioni2024#0084]: 'Training energy (kWh) 51,686 ... Finetuning energy (kWh) 7,571 ...'","The Power Hungry Processing study lists training energy for BLOOMz-7B as 51,686 kWh and fine‚Äëtuning energy as 7,571 kWh. Summing these gives a combined energy cost of 59,257 kWh."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the total carbon emissions avoided by pruning and quantizing large language models in 2023, so I cannot answer this question based on the available context."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed'","The question asks for the percentage of token usage through models without environmental impact disclosure. Doc [luccioni2025c] states that 84% of LLM usage is through models with no disclosure, so the answer is 84%."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons of CO2,"[21, 78]",tCO2,is_blank,is_blank,Doc [dodge2022#0069]: ... we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it w,"The question asks for the estimated CO2 emission range of a complete training run of a 6.1‚ÄØbillion‚Äëparameter transformer model. Doc [dodge2022#0069] explicitly states that a full run would emit 21 to 78 metric tons of CO‚ÇÇ, providing the required range."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 A100 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: ""Model Size V100 32GB A100 80GB Count Max. Batch size 7B 1 64 1 64""","The table in Doc [samsi2024] lists the bare minimum hardware for LLaMA-7B. It shows that 1 A100 80GB GPU is sufficient, so the bare minimum number of such GPUs required is 1."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24% cost saving,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.,The question asks for the cost savings achieved when blending A100 and A10G GPUs compared to an A100-only strategy. Doc [griggs2024] explicitly states a 24% cost saving in that scenario.
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,answers,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers,'","The question asks how many answers were collected after contacting over 500 authors. Doc [luccioni2025b] explicitly states that 95 answers were obtained, so 95 is the correct numeric answer."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024]: 'This is equivalent to approximately 44% of the data centers‚Äô total electricity cost.',"The passage explicitly states that in 2023 the public health cost was about 44% of the data centers‚Äô total electricity cost, which directly answers the question."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",‚âà 996 million inferences,996000000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [dodge2022#0054]: 'we estimate a full training run would consume approximately 103,593 kWh.'; Doc [luccioni2024]: 'Table 3‚Ä¶ BLOOMz-7B 7B 14.46 0.104' (energy in kWh for 1,000 inferences).","The 6.1‚ÄØB‚Äëparameter model consumes about 103,593‚ÄØkWh for a full training run (Doc [dodge2022#0054]). BLOOMz‚Äë7B uses 0.104‚ÄØkWh to perform 1,000 inferences, i.e., 0.000104‚ÄØkWh per inference (Doc [luccioni2024]). Dividing the training energy by the per‚Äëinference energy gives 103,593‚ÄØkWh / 0.000104‚ÄØkWh ‚âà 9.96‚ÄØ√ó‚ÄØ10^8 inferences, or roughly 996‚ÄØmillion inferences."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",4.63 $/hr,4.63,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,Doc [chen2024]: 'Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr',"The table in Doc [chen2024] lists the price per chip per hour for the NVIDIA H20 as $4.63/hr, matching the requested value."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'In fact, most carbon foot print analyses gather the information manually by writing to authors.'","The context states that most carbon footprint analyses gather information manually by writing to authors, which contradicts the claim that they gather automatically without contacting authors. Hence the statement is false."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",$3.33 per H100 GPU-hour,3.33,$/hour,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'less than$0.1 million, using 30,000 H100 GPU hours.'","The JetMoE project cost was reported as less than $0.1‚ÄØmillion for 30,000 H100 GPU hours. Dividing 100,000 by 30,000 gives an approximate cost of $3.33 per H100 GPU‚Äëhour."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36 projects,36,projects,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: United Kingdom 36 901,The table in Doc [amazon2023] lists the number of renewable energy projects by country; the United Kingdom entry shows 36 projects announced as of January 2024.
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,is_blank,is_blank,Doc [li2025b#0034]: 'Apple reports that its supply chain accounts for 99% of its total water footprint [23]'.,"The question asks for the share of Apple's total water footprint attributable to its supply chain. Doc [li2025b#0034] explicitly states that the supply chain accounts for 99% of the total water footprint, so 99% is the correct answer."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide GPT-3 training energy (1,287 MWh) but do not provide a numeric energy consumption for a Meena training run, so the number of Meena runs that would match GPT-3‚Äôs energy cannot be determined from the available context."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]'.","The question asks for the average number of connected devices per U.S. household in 2021. Doc [wu2021b] explicitly states that the average household has 25 connected devices, so 25 devices is the answer."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",water withdrawal,water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses'","The question asks for the term describing freshwater taken from ground or surface sources. Doc [li2025b] defines this as ""water withdrawal"", so that is the correct answer."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25T tokens,1250000000000.0,tokens,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024#0038]: 'JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.'","The question asks for the number of tokens used in pre‚Äëtraining JetMoE-8B. Doc [shen2024#0038] explicitly states that the model is trained on 1.25 trillion tokens, which is expressed as 1.25T tokens. Therefore the correct answer is 1.25T tokens."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,is_blank,is_blank,"Doc [erben2023#0015]: 'we introduce the granularity metric, the ratio of calculation to communication time'","The question asks for the metric used to assess the ratio of computation to communication time. Doc [erben2023#0015] explicitly states that this metric is called the granularity metric, making it the correct answer."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: led computing systems. As its core activity , the Standing Committee that oversees the One Hundred Y ear Study forms a Study Panel every five years to assess the current state of AI.","The question asks how often the Standing Committee forms a Study Panel. Doc [stone2022] explicitly states that it does so every five years, so the answer is five years."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: . IBM‚Äôs Watson program, which beat human contenders to win the Jeopardy challenge in 2011","The context states that Watson beat human contenders in Jeopardy, so the claim that it did NOT beat them is false."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents mention that health cost can exceed 120% of electricity cost for Iowa, but they do not provide a specific percentage for Altoona, Iowa. Therefore, the exact percentage cannot be confidently determined from the provided context."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Doc [khan2025]: 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.'; Doc [khan2025#0044]: 'Table III shows significant reductions in carbon emissions across all models, with some achieving up to 45% after optimization.'","Both Doc [khan2025] and Doc [khan2025#0044] report that quantization and related techniques can achieve up to 45% reductions in carbon emissions, so the statement is true."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8‚Äì3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'els. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly.'","The question asks for the energy consumption range for pre-training an LLM. Doc [luccioni2025c] explicitly states that the range spans from 0.8 MWh to 3,500 MWh, so that is the answer."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10‚Äì50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,Doc [luccioni2025a]: 'one paper suggesting that 10‚Äì50 queries on GPT-3 consumes around half a liter of water',"The question asks for the number of GPT-3 queries that use about half a liter of water. Doc [luccioni2025a] explicitly states that 10‚Äì50 queries consume roughly that amount, and Doc [li2025b] corroborates with a similar estimate, so the answer is the range 10‚Äì50 queries."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not give a numeric value for the total execution time of a sparse BlackMamba fine‚Äëtuning on an NVIDIA A40-48GB with batch size 84, so it cannot be answered confidently from the context."
q168,The 2024 Griggs et al. paper reports that M√©lange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,is_blank,is_blank,"Doc [griggs2024#0004]: 'M√©lange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.'","The question asks for the maximum cost reduction in conversational chat settings, and Doc [griggs2024#0004] explicitly states that the reduction can be up to 77%."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: JetMoE-8B-chat 6.681 Llama-2-13b-chat 6.650,"The table in Doc [shen2024] lists the MT-Bench score for JetMoE-8B-chat as 6.681, which is higher than Llama-2-13b-chat‚Äôs 6.650, directly answering the question."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,times,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '... could add up to 640 percent more carbon emissions compared to the company‚Äôs carbon removal targets for the year...',"The passage states emissions could be 640‚ÄØpercent higher than the annual removal targets, which translates to 6.4‚ÄØtimes more emissions."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",49.8%,49.8,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: 49.8%49.9%,"The table in Doc [amazon2023] lists the U.S. workforce composition by gender. The row for men shows 49.8% for the U.S. workforce, indicating that 49.8% of Amazon‚Äôs U.S. employees identified as men in 2023."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80‚Äì90%,"[80, 90]",%,is_blank,is_blank,Doc [patterson2021#0020]: 'NVIDIA estimated that 80‚Äì90% of the ML workload is inference processing [Leo19].',"The question asks for the percentage of ML workload estimated to be inference processing by NVIDIA in 2019. Doc [patterson2021#0020] explicitly states that NVIDIA estimated this to be 80‚Äì90%, which is quoted as the answer."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [han2024#0002]: 'training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.'","Both Doc [han2024#0002] and Doc [luccioni2025c] explicitly state that training a Llama-3.1 scale model produces air pollutants equivalent to more than 10,000 LA‚ÄëNYC round trips, so the answer is >10,000 round trips."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10 ‚Äì 50 completions,"[10, 50]",is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'Additionally, GPT-3 needs to ‚Äúdrink‚Äù (i.e., consume) a500ml bottle of waterfor roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed.'","The passage in Doc [li2025b] indicates that one 500‚ÄØmL bottle of water can support roughly 10 to 50 medium‚Äëlength GPT‚Äë3 completions, so the answer is a range of 10‚Äì50 completions."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about water used for cooling during GPT-4 training.
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$7.22 per hour,7.22,$/h,is_blank,is_blank,"Doc [griggs2024#0005]: 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.'","The context states that serving Llama-2-70B with 2 A100 GPUs costs over $5,200 per month. Dividing $5,200 by (30 days √ó 24 hours = 720 hours) gives approximately $7.22 per hour."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",7.5164 $/h,7.5164,$/h,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: To ensure fair cost comparisons, we normalize RunPod‚Äôs H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod‚Äôs H100 cost ($4.69) to RunPod‚Äôs A100-80G cost ($2.29), then adjusting relative to the A100‚Äôs price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) √ó 3.67 = $7.516 for H100. ... On-demand Price ($/h) 0.7 1.01 3.67 7.5164","The question asks for the normalized hourly on‚Äëdemand price of an H100 GPU in the Griggs et al. (2024) study. The document explicitly states that the normalized price is $7.516 (shown as 7.5164 in the table), so that value is reported as the answer."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"Doc [chung2025#0055]: 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU ‚Äì or any computing device ‚Äì to draw its maximum power at every moment in time.'
Doc [luccioni2023]: 'While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding real-time power consumption is only possible by using a tool like Code Carbon... Nonetheless, the TDP-based approach is often used in practice when e'","The question asks whether TDP-based estimation is reliable and accurate. The cited passages show that TDP assumes 100% GPU utilization, which is rarely achieved, leading to overestimation and unreliability. Therefore the statement is false."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.'","The 2025 paper notes that after the 2022 peak, the trend reversed and direct disclosures actually decreased, so the claim that it continued to increase is false."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",1000√ó larger,1000,times,is_blank,is_blank,"Doc [wu2021a#0013]: 'with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000√ó larger in size.'","The question asks how much larger the GPT‚Äë3 model must be to raise BLEU from 5 to 40. Doc [wu2021a#0013] explicitly states that a 1,000√ó larger model is required, so the answer is 1,000√ó (i.e., 1000 times larger)."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents provide a specific emissions-to-driving-distance ratio or the necessary conversion factor to estimate miles of driving equivalent to the CO2 emissions from training a Transformer model with neural architecture search.
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the documents provide a clear, explicit total CO2 equivalent emission value for the entire Power Hungry Processing study, so I cannot answer this question with confidence."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4 A100 GPUs,4,A100 GPU,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: '... at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' Doc [rubei2025#0055]: '... 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.'",The question asks for the bare minimum number of 80GB A100 GPUs needed for LLaMA‚Äë65B inference. Both documents state that 4 A100 GPUs are required. Thus the answer is 4.
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 A800 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 √ó80G) servers.',"The document states that 24 servers, each with 8 A800 GPUs, were used. Multiplying 24 by 8 gives 192 GPUs, so the total number of A800 GPUs for training is 192."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a numeric top-1 accuracy value for AlexNet 2012, so I cannot answer the question with confidence."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1000000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.' Doc [cottier2024#0065]: 'the amortized cost of frontier training runs will exceed one billion dollars by 2027.'","The question asks for the cost threshold that largest training runs will surpass by 2027. Both Doc [cottier2024] and Doc [cottier2024#0065] explicitly state that the cost will exceed one billion dollars, so the answer is 1‚ÄØbillion dollars."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 GPUs,8,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'at a minimum, 8 V100 GPUs each with 32 GB of RAM ...'  Doc [rubei2025#0055]: '8 V100 GPUs each with 32 GB of RAM ...'","Both documents state that the bare‚Äëminimum setup for LLaMA‚Äë65B inference without compression or quantization requires 8 NVIDIA V100 32GB GPUs, so the answer is 8 GPUs."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: 'OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].',"The question asks for the total floating point operations reported by OpenAI for training GPT‚Äë3. Doc [patterson2021] explicitly states that OpenAI published the total as 3.14E+23 FLOPs, so that value is reported as the answer."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'FAIR‚Äôs RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.'","The question asks for the number of GPU hours used to train FAIR‚Äôs RoBERTa on 160GB of text. Doc [schwartz2019] explicitly states that this training required about 25,000 GPU hours, so that is the answer."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: ... 30,000 H100 GPU hours.","The question asks for the number of H100 GPU hours used for JetMoE-8B pre‚Äëtraining. Doc [shen2024] explicitly states that JetMoE-8B was trained with 30,000 H100 GPU hours, so that is the answer."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini‚Äôs consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s. GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The question asks whether GPT-4o mini consumes less energy per query than GPT-4o. The cited passages from Doc [jegham2025] state that GPT-4o mini consumes more energy (3.098‚ÄØWh vs‚ÄØ2.875‚ÄØWh) and 20% more energy on long queries, so the statement is false."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide total water consumption figures but do not give a per‚Äësession consumption value for ChatGPT in 2023, so the question cannot be answered from the available context."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the energy consumption factor for deploying the Llama 3.1 70B model on two nodes versus one node, so the answer cannot be determined from the given context."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Yelp sentiment analysis benchmarks or compare traditional models to large language models in that context, so I cannot answer the question with confidence."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34% increase,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the sam'","The 2025 paper (Doc [luccioni2025a]) explicitly states that Microsoft reported a 34% increase in global water consumption from 2021 to 2022, so 34% is the correct answer."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,Megatron-LM,Megatron-LM,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: [7] M. Shoeybi, M. Patwary, R. Puri et al., ‚ÄúMegatron-lm: Training multi-billion parameter language models using model parallelism,‚Äù 2020.","The question asks for the framework used to deploy large language models across multiple GPUs and nodes. Doc [samsi2024] lists Megatron-LM as a framework for training multi‚Äëbillion parameter language models using model parallelism, indicating it is the framework employed for multi‚ÄëGPU, multi‚Äënode deployment."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,homes,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset.'","The question asks for the number of U.S. homes whose annual electricity use matches that of 700‚ÄØmillion daily GPT‚Äë4o queries. The cited passage explicitly states that this scenario is equivalent to 35,000 U.S. homes."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'Inference energy (kWh) 1.0 √ó 10‚àí4 ...' and Doc [luccioni2024#0084]: 'mallest model, BLOOMz-560M to 1.0 √ó 10‚àí4 kWh for the biggest one, BLOOMz-7B.'","The inference energy per inference for BLOOMz-7B is 1.0√ó10‚Åª‚Å¥‚ÄØkWh (‚âà1.0√ó10‚Åª‚Å∑‚ÄØMWh). With 606,096 downloads each generating 1‚ÄØmillion inferences, the total number of inferences is 606,096‚ÄØ√ó‚ÄØ1,000,000 = 6.06096√ó10¬π¬π. Multiplying by 1.0√ó10‚Åª‚Å∑‚ÄØMWh gives ‚âà60,609.6‚ÄØMWh."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons of CO2e",47400,tCO2e,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: '... avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.'","The question asks for the amount of CO2e avoided by Amazon's on-site solar energy systems. Doc [amazon2023] explicitly states that these systems avoid roughly 47,400 metric tons of CO2e annually, so that is the answer."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: n from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The question asks for the estimated total number of GPT‚Äë4o queries in 2025. Doc [jegham2025] explicitly states that the total is approximately 772‚ÄØbillion, so that value is used as the answer."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,PUE,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: datacenter   PUE   online   every   quarter .   The   PUE   for   the   Iowa   datacenter   where   we   ran   Evolved   Transformer   is   1.11,   a   factor   of   1.4X   better.","The passage from Doc [patterson2021] explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11, so that is the answer."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific count of AI training runs conducted globally on renewable-only power in 2022.
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'The US national datacenter average in 2018 was 1.58, which is the value [Str19] used ; In 2020, it was 1.59.'","The question asks for the US national datacenter average PUE in 2020. Doc [patterson2021] explicitly states that in 2020 the average was 1.59, which is the value we report as the answer."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0',"The table in Doc [shen2024] lists the OpenLLM Leaderboard average scores for four models. The fourth value, 53.0, corresponds to the JetMoE-8B model, which is the required final average score."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,is_blank,is_blank,"Doc [kim2025#0019]: 'For example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.'","The question asks for the KV Cache size at batch size 32 for the OPT‚Äë2.7B model. Doc [kim2025#0019] explicitly states that at batch size 32 the cache expands to 5.312GB, providing the required value."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the estimated CO2 emissions for NAS (284 metric tons) but do not give a clear equivalence to average American lifetimes, so I cannot answer that part with confidence."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,True,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: '3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].'","The question asks whether open‚Äësource general‚Äëpurpose AI models are fully exempt from reporting their energy consumption unless they pose systemic risk. Doc [ebert2024] explicitly states that open‚Äësource GPAI models are largely excluded from transparency requirements unless they present a systemic risk, which directly supports the statement. Hence the answer is True."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.'","The question asks for the percentage range of R&D staff costs (including equity) for the four notable models. Both Doc [cottier2024] and Doc [cottier2024#0052] state that this range is 29% to 49% of the total amortized cost, so that is the answer."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.'","The question asks for the name of the function that improves instance selection accuracy by correcting theoretical and actual GPU performance differences. The cited passage explicitly names this function as the Compute Time Calibration Function (CTCF), providing the required answer."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].'","The question asks whether open‚Äësource general‚Äëpurpose AI models must report energy consumption to authorities. The cited passage from Doc [ebert2024] states that OS models are largely excluded from transparency requirements, indicating they are not required to report energy consumption under current EU rules."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give the energy consumption for GPT-4.1 nano (0.827 Wh for long prompts) but do not provide a corresponding energy figure for the o3 model, so the factor cannot be determined from the available information."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.'","The passage from Doc [samsi2024] explicitly states that increasing the number of shards tends to increase the energy costs of inference per response, which directly supports the statement that more shards raise energy cost per response for LLaMA‚Äë65B."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,fernandez2025,https://arxiv.org/pdf/2504.17674,Doc [luccioni2024#0034]: 'used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference',"The question asks which software package was used to measure energy consumption during inference runs. The cited documents explicitly state that the CodeCarbon package was employed for this purpose, providing the needed answer."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: . For each article, we noted whether it mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by acknowledging uncertainty or suggesting that such statistics should be viewed critically. Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it con","The question asks for the percentage of 100 analyzed news articles that cited the contested 3‚ÄØWh estimate. The passage from Doc [luccioni2025c] states that 53% of the articles cited that figure, providing the required answer. "
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",almost 30%,30,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide'","The question asks for the percentage of PPAs in 2020 held by Amazon, Microsoft, Meta, and Google. Doc [luccioni2025a] explicitly states that they accounted for almost 30% of all PPAs worldwide, so that is the answer."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.'","The question requests the 2023 public health cost based on the average attribution method. Doc [han2024] explicitly states that the total public health cost of U.S. data centers in 2023 was about $6.7‚ÄØbillion, which matches the requested figure."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",0.061 zettaFLOPs,0.061,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: '101 4 4 12 192 2160 165 52.88%' ; Doc [li2025a]: '101 2e‚àí4 230,400 4.31 6.54 26.54'","The throughput for the final 101B stage is 165 teraFLOPs/s (Doc [li2025a]). The stage duration is 4.31 days (‚âà372,384‚ÄØs, Doc [li2025a]). Multiplying throughput by duration gives 165‚ÄØ√ó‚ÄØ372,384‚ÄØ=‚ÄØ61,443,360 teraFLOPs, which equals 0.061‚ÄØzettaFLOPs. The answer is expressed in zettaFLOPs as requested."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,is_blank,is_blank,"Doc [wu2021b#0024]: 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].'","The question asks whether GPU theoretical performance per watt was observed to double every 3‚Äë4 years based on 2019 product data. Doc [wu2021b#0024] states exactly that GPU theoretical performance per watt doubles every 3‚Äë4 years, citing Sun et al., 2019. Therefore the statement is true."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",14.9 qps,14.9,qps,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 14.9 Dense(bsz=1) Dense(bsz=6) Sparse(bsz=1) Sparse(bsz=6) Sparse(bsz=20) Blackmamba-MATH0 5 10 15 20 2.2 5.3 2.2 6.5 11.6 Dense(bsz=1) Dense(bsz=2) Sparse(bsz=1) Sparse(bsz=2) Sparse(bsz=8) Throughput (quries/second) Fig. 8. Query throughput of Mixtral and BlackMamba.,"The figure caption shows that the ground‚Äëtruth throughput for Mixtral-CS dense at batch size‚ÄØ1 is 14.9‚ÄØqueries per second, which matches the numeric value in the cited passage."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"Doc [khan2025#0033]: 'We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.'","The question asks for the open-source tool used for 4‚Äëbit quantization and local deployment. Doc [khan2025#0033] explicitly states that quantization was applied through Ollama, identifying it as the tool in the financial sentiment case study."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024',"The question asks which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024. Doc [ebert2024] explicitly states that Senator Edward J. Markey introduced the bill, so the answer is Senator Edward J. Markey."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 tCO2e,26,tCO2e,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: n et al. 2023b)FLM-101B Params 175B 280B 540B 130B 70B 101B GPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5 Chip Power/TDP 330 283 378.5 400 400 400 Energy (MkWh) 1171 1066 3179 444 688 40 net tCO2e 552 380 271 257 291 26,"The table in Doc [li2025a] lists the net carbon emissions for FLM-101B as 26 tCO2e, which directly answers the question with the requested unit."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,SpotLake,SpotLake,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,Doc [erben2023]: 'SpotLake: Diverse Spot Instance Dataset Archive Service.',"The question asks for the storage service used to shard and stream datasets for spot VMs that can terminate at any time. Doc [erben2023] explicitly names SpotLake as a diverse spot‚Äëinstance dataset archive service, indicating it is the storage service in use."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.000022 kL,2.2e-05,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12‚Äëinch silicon wafer weighs 125 grams 12 and produces about 63 H100s.'","The passage gives 11 kL of water per 1‚ÄØkg of rare earths and that a wafer weighing 125‚ÄØg yields 63 H100s. Each H100 thus weighs 125‚ÄØg/63 ‚âà 2‚ÄØg, so 0.1% of that is ~0.002‚ÄØg of rare earths. Converting 0.002‚ÄØg to kilograms (2√ó10‚Åª‚Å∂‚ÄØkg) and multiplying by 11‚ÄØkL/kg gives ‚âà2.2√ó10‚Åª‚Åµ‚ÄØkL of water per GPU."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,False,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].'","The documents explicitly state that AI‚Äôs public health costs are highly unevenly distributed, with low‚Äëincome counties experiencing up to 200√ó higher per‚Äëhousehold burdens, which contradicts the claim of even distribution."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a clear statement that the relationship between runtime and energy consumption during inference was found to be nearly linear.
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",$11.06/hr,11.06,USD/hr,chen2024,https://arxiv.org/pdf/2405.01814,Doc [chen2024]: 'Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr',"The question asks for the price per hour of an NVIDIA H100. Doc [chen2024] provides the price per chip for H100 as $11.06 per hour, which directly answers the question."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,3.7 years,3.7,years,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.'","The question asks for the average GPU lifetime before retirement in AI data centers in 2024. Doc [cottier2024] directly states that the expected hardware lifetime would be 3.7 years, providing the required estimate."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,percent,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Research shows that in North America, AWS can lower its customers‚Äô workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy‚Ä¶'","The question asks for the typical reduction in carbon footprint when moving to AWS. The cited Amazon 2023 document states a reduction of up to 96%, which directly answers the question."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, over 4x the five cars estimate",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the ‚Äúfive cars‚Äù number.'","The question asks for the pre‚Äëtraining emissions of Gemma and how they compare to the five cars estimate. Doc [luccioni2025c] states the emissions are 1247.61 tCO‚ÇÇe and explicitly notes this is over 4 times the five‚Äëcars baseline, so the answer reflects that value and comparison."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,"Doc [dodge2022]: 'Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021', Doc [wu2021b]: 'PUE of hyperscalar datacenters, such as Google‚Äôs, has improved from 1.21 (2008) to 1.10 (2021)'","Both Doc [dodge2022] and Doc [wu2021b] explicitly state that Google's hyperscale data centers had a reported PUE of 1.10 in 2021, which directly answers the question."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: water evaporation rates from hydropower generation is included [1]. For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The question asks for the U.S. national average water consumption for electricity generation. Doc [li2025b] explicitly states that the consumption factor is 3.1 L/kWh, so that is the answer."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 V100 GPUs,2,V100 GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128,The table in Doc [samsi2024] lists the bare minimum hardware; for the 13B model it requires 2 V100 GPUs.
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,USD,is_blank,is_blank,"Doc [xia2024#0099]: 'Using this model, we showed the dollar amount that needs to be budgeted for fine‚Äëtuning LLMs, which is much lower than pre‚Äëtraining. For example, our model predicted that fine‚Äëtuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.'; Doc [xia2024#0093]: 'For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost‚Äëeffective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.'","The question asks for the net cost of fine‚Äëtuning a sparse Mixtral model on an NVIDIA H100 GPU with 2‚ÄØmillion queries. Both documents explicitly state that this cost is $3460, so the answer is $3460 USD."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50‚Äì70%,"[50, 70]",%,is_blank,is_blank,"Doc [chung2025#0014]: GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50‚Äì70% of the total provisioned power in the datacenter","The question asks for the typical share of total provisioned power that GPUs consume. Doc [chung2025#0014] explicitly states that GPUs account for 50‚Äì70% of this power, so this range is the answer."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,is_blank,is_blank,Doc [strubell2019#0022]: 'Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).',"The question asks for the training duration of ELMo on 3 NVIDIA GPUs. Doc [strubell2019#0022] explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, i.e., 336 hours, which is the value used in the answer."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'g 12 nodes and 96 H100s.',"The document states that the training cluster had 12 nodes and housed 96 H100 GPUs, so the total number of H100 GPUs used was 96."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,is_blank,is_blank,Doc [jegham2025#0079]: 'A single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh) ...',"The question asks for the energy use of a single short GPT‚Äë4o query. Doc [jegham2025#0079] explicitly states that such a query consumes 0.42 Wh, and Doc [jegham2025#0077] corroborates this figure, so the answer is 0.42 Wh."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 lbs",11023,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'Consumption CO 2e (lbs) Air travel, 1 passenger, NY ‚Üî SF 1984 Human life, avg, 1 year 11,023'","The question asks for the estimated CO2e for an average human life in one year globally. Doc [strubell2019] provides the figure of 11,023 lbs, so that is the answer."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, M√©lange achieved cost reductions in what percentage range compared to single-GPU baselines?",9-77%,"[9, 77]",%,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: 'M√©lange reduces costs by 9-77% for short-context tasks (interactive chats)',"The question asks for the percentage range of cost reductions for short‚Äëcontext (Arena) workloads at a 120‚ÄØms SLO. Doc [griggs2024] states that M√©lange reduces costs by 9‚Äì77% for short‚Äëcontext tasks, which applies to the Arena dataset. Therefore the answer is 9‚Äì77‚ÄØ%."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'can directly evaporate 700,000 liters of clean freshwater'","The question asks for the amount of clean freshwater directly evaporated during GPT‚Äë3 training in Microsoft‚Äôs U.S. data centers. Doc [li2025b] states that this amount is 700,000 liters, which provides the numeric answer."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280% more expensive,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'but at a cost of $2.699, which is about 280% more expensive than InferSave‚Äôs top choice.'","The question asks for the percentage by which Max-Performance‚Äôs g6e.xlarge cost exceeds InferSave‚Äôs top choice for a 400‚ÄØTPS SLO.  The context states that the Max‚ÄëPerformance instance costs $2.699 versus $0.71 for InferSave‚Äôs top choice, explicitly noting this is about 280% more expensive, which directly answers the question."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'Processor Average (Watts)  StDev   %  DNNs   used to   calculate   average   power   TPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   Search   [So19]   V100   GPU   325   2%   Transforme'","The context provides per‚Äëprocessor average system power: 221‚ÄØW for TPU‚ÄØv2 and 325‚ÄØW for V100 GPU. Subtracting gives a difference of 104‚ÄØW, which is the answer."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",True,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'Reporting the computational price tag of Ô¨Ånding, training, and running models is a key Green AI practice (see Equation 1).'","The question asks whether Green AI includes providing the financial cost of finding, training, and running models. Doc [schwartz2019] explicitly states that reporting that price tag is a key Green AI practice, so the statement is true."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25√ó,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.'","The passage explicitly states that for the LLaMA‚Äë13B model, inference on A100 GPUs achieves roughly a 1.25√ó increase in throughput compared to V100 GPUs, indicating the approximate speedup."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'AI contributes to the ballooning issue of electronic waste. AI‚Äôs expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'","The question seeks the worldwide electronic waste amount in 2022. The 2025 paper (Doc [luccioni2025a]) explicitly states that e‚Äëwaste reached 62‚ÄØmillion tonnes in 2022, so that is the answer."
q264,"What is the context window size, in tokens, for the FLM-101B model?","2,048 tokens",2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.'","The question asks for the context window size of FLM-101B in tokens. Doc [li2025a] explicitly states that the context window is 2,048 tokens, so that is the answer."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"As shown in Figure 8, OpenAI‚Äôs reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic‚Äôs Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The question asks which model ranked highest in the eco-efficiency analysis using DEA. Doc [jegham2025] states that o3-mini has the highest cross‚Äëefficiency score (0.884), so o3‚Äëmini is the correct answer."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'When actively training, the average GPU power is over 600W, over 85% of an H100‚Äôs maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.'","The context from Doc [morrison2025] directly states that during the first 300 logging steps of OLMo 2‚ÄØ7B training, the average GPU power while actively training is over 600‚ÄØW, so the answer is 600‚ÄØW (over)."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20√ó,20,√ó,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'Facebook‚Äôs recommendation model sizes have increased by 20√ó between 2019 and 2021.',"The question asks for the increase in model sizes. Doc [wu2021a] states that Facebook‚Äôs recommendation model sizes grew by 20√ó from 2019 to 2021, so the answer is 20√ó."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million,150,million,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: 'We delivered 150 million packages via EVs.',"The question asks for the number of packages delivered via EVs in Europe in 2023. The Amazon 2023 Sustainability Report states that Europe delivered 150 million packages via EVs, so the answer is 150 million."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61‚Äì76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Computing hardware makes up 47‚Äì64%, while energy comprises only 2‚Äì6%. However, if we exclude equity the fraction for R&D staff drops to 19‚Äì33%, and the fractions of computing hardware costs and energy rise to 61‚Äì76% and 2‚Äì7% respectively.'","The question asks for the computing hardware share when equity is excluded. The passage in Doc [cottier2024] states that the fraction of computing hardware costs rises to 61‚Äì76% under this condition, so that is the answer."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,wu2021b;luccioni2025a,https://arxiv.org/pdf/2108.06738;https://arxiv.org/pdf/2501.16548,"Doc [wu2021b]: 'current averages of less than 3 years for cell phones [Cordella et al., 2020]'; Doc [luccioni2025a]: 'AI contributes to the ballooning issue of electronic waste. AI‚Äôs expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'","The statement that smartphones average less than 3 years is explicitly stated in Doc [wu2021b], and the broader e‚Äëwaste concern is highlighted in Doc [luccioni2025a]. Together they confirm the claim is true."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO 2 emissions: CO2e = 0.954pt'","The question asks for the EPA‚Äôs average CO2 per kWh in pounds. Doc [strubell2019] directly states this value as 0.954 lbs/kWh, so that is the answer."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 ‚Äì 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,Doc [li2025b]: 'a recent study suggests that the global AI could consume 85 ‚Äì 134 TWh of electricity in 2027 [7]',"The question asks for the projected electricity consumption range for global AI in 2027. Doc [li2025b] explicitly states that the range is 85‚Äì134 TWh, so that is the answer."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents include a numeric value for the total execution time (in seconds) of a sparse Mixtral model with batch size 1 on an NVIDIA A40-48 GB GPU.
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,True,1,is_blank,is_blank,is_blank,"Doc [chung2025#0064]: 'LLM‚Äôs power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model‚Äôs power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low ... This leads to the GPU‚Äôs computation throughput being bottlenecked by VRAM bandwidth.'  Doc [chung2025#0192]: 'Diffusion models, on the other hand, consume nearly the maximum power of the GPU ... This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.'","The documents state that LLM inference draws much less power than diffusion models, citing low compute intensity and VRAM bandwidth bottleneck for LLM decoding, while diffusion models are more compute-intensive and draw near‚Äëmaximum GPU power. Thus the statement is supported and is True."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,is_blank,is_blank,"Doc [kim2025#0089]: 'time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens... The workload evaluates a total of 3000 requests.'","The online inference workload uses 128 input tokens plus 512 output tokens per request, totaling 640 tokens per request. With 3000 requests, 640 √ó 3000 = 1,920,000 tokens, which is the total number of tokens processed."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,is_blank,is_blank,"Doc [khan2025#0046]: 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade‚Äëoff between energy efficiency and overall predictive perfor‚Äëmance.'; Doc [khan2025#0053]: 'Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications‚Ä¶'","Both documents state that accuracy and F1 can decline after optimization, so the assertion that they always improved is incorrect."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention Yelp sentiment analysis benchmarks or compare traditional models to large language models.
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450x,1450,is_blank,is_blank,is_blank,Doc [luccioni2024#0045]: 'the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.',The 2024 study reports that the energy consumption for image generation is over 1450 times that of text classification. Thus the factor is 1450.
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244 projects,244,projects,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'United States 244 17,706'",The table of Amazon Renewable Energy Projects announced as of January 2024 lists 244 projects for the United States. This directly answers the question about how many U.S. projects were announced.
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Doc [schwartz2019]: 'Amazon‚Äôs AWS only covered Ô¨Åfty percent of its power usage with renewable energy.',"The question asks for the renewable‚Äëenergy coverage of AWS power usage in 2018. Doc [schwartz2019] states that AWS covered fifty percent of its power usage with renewable energy, which directly answers the question."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'Energy Reporting at the Cumulative Server Level : Energy consumption should be reported at the cumulative server level (see also [4]).',"The question asks which measurement level the authors recommend for reporting AI energy consumption. The cited passage from Doc [ebert2024] explicitly states that energy should be reported at the cumulative server level, which balances accuracy (captures total computation power) and feasibility (uses data center‚Äëreported PUE)."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"Doc [dodge2022#0081]: 'For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West‚ÄØUS;'","The question asks for the maximum potential reduction in CO2 emissions for DenseNet 201 in West US under Flexible Start. Doc [dodge2022#0081] explicitly states that the reduction can be up to 80% in that region, which is the answer."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5%,28.5,%,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).,"The question asks for the total operational energy footprint reduction over 2019‚Äë2021. Doc [wu2021a] explicitly states a 28.5% reduction, which is the figure reported for Facebook‚Äôs iterative hardware‚Äësoftware optimization during that period."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,GPUs,is_blank,is_blank,Doc [griggs2024#0005]: 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs',"The question asks how many NVIDIA A100-80GB GPUs are needed to serve Llama2-70b at BF16 precision, and Doc [griggs2024#0005] explicitly states that 2 GPUs are required."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the amount of fiber optic cable installed globally to support AI workloads in 2023, so I cannot answer this question based on the available context."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement that intra‚Äëzone scaling with T4 GPUs achieved nearly linear per‚ÄëGPU speedup for CV models, so the answer cannot be determined with confidence."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration. Doc [ebert2024]: The AI Act fails to address indirect greenhouse gas emissions from AI applications.","The question asks whether the AI Act requires providers to disclose GHG emissions of AI applications such as oil and gas exploration. Doc [ebert2024] explicitly states that the AI Act fails to address these emissions, indicating no such mandate."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents provided do not contain a clear statement identifying which GPU architecture is most energy‚Äëefficient for generating only a single classification token.
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800M,800000000,USD,is_blank,is_blank,"Doc [cottier2024#0047]: 'we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.'","The question asks for the upfront hardware acquisition cost to train GPT‚Äë4. Doc [cottier2024#0047] explicitly states that the acquisition cost was $800‚ÄØmillion, so that is the answer."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: ... Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to ‚Äúincreases in data center energy consumption‚Äù.,"The question asks for the percentage increase in GHG emissions reported by Google. Doc [luccioni2025a] explicitly states a 48% increase, so this is the answer."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74 percent,74,percent,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022#0045]: 'GPU alone accounts for 74% of the total energy consumption due to these components.',"The question asks for the GPU share of electricity consumption when training a BERT‚Äëbase model. Doc [dodge2022#0045] states that the GPU alone accounts for 74% of the total energy consumption, directly answering the question."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping,Swapping,is_blank,is_blank,is_blank,Doc [chung2025#0177]: 'Swapping consistently consumes less energy.',"The question asks which preemption mechanism uses less energy when a server is overloaded. Doc [chung2025#0177] explicitly states that Swapping consistently consumes less energy than Recomputation, so the answer is Swapping."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention McKinsey projections for 2030 data center electricity consumption, so the requested information cannot be determined from the context."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'seminal 2019 article by Strubell et al. which quantiÔ¨Åed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].'","The question asks for the training emissions of BERT in pounds CO2e. Doc [luccioni2025b] explicitly states that the 2019 Strubell et al. study quantified BERT‚Äôs training emissions as 626,155 pounds of CO2e, which is the value used for the answer."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,is_blank,is_blank,"Doc [luccioni2025b#0014]: 'The umbrella term ‚ÄòSustainable AI‚Äô was initially proposed by van Wynsberghe as a Ô¨Åeld of practice that both aims to use AI in climate-positive applications, as well as i mproving upon the (environmental) sustainability of AI approaches themselves [203].'","The question asserts that Sustainable AI was proposed to only encompass climate-positive applications. The cited passage shows it also aims to improve the environmental sustainability of AI approaches, so the statement is false."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.'","The question asks for the percentage reduction in inference computation, and Doc [shen2024] explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, so 70% is the answer."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [li2025b#0046]: ... an estimated training energy of 1287 MWh [29].  Doc [jegham2025]: Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity ...","The question asks for the training energy of GPT-3 in MWh. Both Doc [li2025b#0046] and Doc [jegham2025] state that the estimated training energy is 1,287 MWh, so the answer is 1287 MWh."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 gCO2eq,0.32,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries'","The question asks for the CO2eq emissions per 1,000 text classification queries for bert-base-multilingual-uncased-sentiment. Doc [luccioni2024] explicitly states that this model emits 0.32‚ÄØg CO2eq for 1,000 queries, so that value is the answer."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28 samples,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'bsz=28',"The figure caption in Doc [xia2024] indicates the ground truth maximum batch size for fine-tuning Mixtral on an NVIDIA A100-40GB GPU is 28 samples, which is the value requested."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give a number of hectares of land occupied by new AI data centers in 2022.
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so the question cannot be answered with confidence based on the available context."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,is_blank,is_blank,Doc [xia2024#0047]: 'MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.',"The question asks whether the MoE layer is often targeted for performance enhancement during LLM fine‚Äëtuning. Doc [xia2024#0047] states that the MoE layer is a prime target for optimization, directly supporting a true answer."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention a projected public health burden for 2030.
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.'","The passage explicitly states the peak year as 2022, matching the question."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,False,0,is_blank,is_blank,is_blank,Doc [xia2024#0100]: 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.',"The passage states that adding compute resources to accelerate the MoE layers is a way to further reduce cost, so it does not increase costs. Therefore the statement is false."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a specific equivalent water usage for an OLMo 60M model trained on 1.7 to 5.6 trillion tokens, so the question cannot be answered confidently with the available context."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MWh,40,MWh,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: Energy (MkWh) 1171 1066 3179 444 688 40,"The question asks for the total energy consumption for training FLM‚Äë101B. Doc [li2025a] lists the energy (in megawatt‚Äëhours) for each model, and the value for FLM‚Äë101B is 40, so the total energy consumption is 40 MWh."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",7 to 26 thousand grams,"[7, 26]",thousand grams,is_blank,is_blank,"Doc [dodge2022#0062]: '7k grams vs. 26k grams, for the most efficient vs. least efficient regions.'","The question asks for the approximate range of CO2 emissions in thousands of grams between the most and least efficient regions. Doc [dodge2022#0062] directly states that the emissions are 7k grams for the most efficient region and 26k grams for the least efficient region, giving a range of 7 to 26 thousand grams."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,32.7 $,32.7,$,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: A40 48GB 4 1.01 0.79 32.7,"The question asks for the total fine‚Äëtuning cost of a Mixtral model on GSM8K with sparse MoE using an NVIDIA A40‚Äë48GB GPU. In Doc [xia2024] Table IV the row for A40 lists the total cost as 32.7, which directly answers the question."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.',"The question asserts that GPU-level monitoring is recommended as the preferred method for overall AI energy reporting. Doc [ebert2024] explicitly states that GPU-level tracking is discouraged for overall energy measurements, contradicting the assertion and proving the answer is False."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear numeric value or statement indicating the maximum potential emissions saving for the 6B parameter transformer when using the Pause and Resume optimization.
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 V100 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: '7B 1 64 1 64',The table in Doc [samsi2024] lists the bare minimum hardware for LLaMA-7B: one NVIDIA V100 32GB GPU is sufficient for inference without compression or quantization.
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.30 metric tons,8.3,metric tons,is_blank,is_blank,"Doc [dodge2022#0065]: 'one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)'","The question asks for the estimated CO2 emission for a typical U.S. home over one year. Doc [dodge2022#0065] explicitly states that this value is 8.30 metric tons, which directly answers the question."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",True,1,is_blank,is_blank,is_blank,"Doc [erben2023#0093]: 'rity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.' Doc [erben2023#0092]: 'on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.'","Both documents state that CV models, which have high granularity, experience only a 7% slowdown when training intercontinentally compared to local training, confirming the statement."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10‚Äì50 responses,"[10, 50]",responses,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'Additionally, GPT-3 needs to ‚Äúdrink‚Äù a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed.'","The question asks how many user requests consume a 500ml bottle of water. Doc [li2025b] explicitly states that GPT‚Äë3 requires roughly 10‚Äì50 medium‚Äëlength responses, so the answer is 10‚Äì50 responses."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: training accounted for only half of the model‚Äôs overall emissions,"The 2023 article by Luccioni et al. states that training accounted for only half of the model‚Äôs overall emissions, which is 50%."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide a clear, unequivocal statement of the 2023 percentage of Amazon's People Managers who identified as women."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2 samples,2,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8 ...',"The question asks for the maximum batch size for dense fine‚Äëtuning of Mixtral on the Hellaswag (CS) dataset using an A40 GPU. Table III in Doc [xia2024] lists Mixtral-D (dense) for CS as 2, so the supported batch size is 2 samples."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8 percent,27.8,percent,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: GSM8k 14.5 17.3 16.9 27.8,"The GSM8k row in Doc [shen2024] lists four scores corresponding to the four models in order LLaMA2, DeepseekMoE, Gemma, and JetMoE. The final value, 27.8, is therefore the JetMoE-8B score on GSM8k."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide the total execution time for a sparse Mixtral fine-tuning run on an NVIDIA A40 with batch size 10.
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement of the batch size for the longest-running MoE layer of a sparse Mixtral model fine-tuned on an NVIDIA A40-48‚ÄØGB GPU, so I cannot answer confidently."
