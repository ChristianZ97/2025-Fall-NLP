id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""total energy consumption of the US data centers increased by about 4% from 2010-2014""","The context states that U.S. data center energy consumption rose approximately 4% between 2010 and 2014, which is the average increase requested."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2t CO2e,1.2,t,patterson2021,https://arxiv.org/pdf/2104.10350,"""single passenger round trip SF-NY is ~1.2t CO 2 e (Table 2).""","The passage explicitly states that a single passenger round trip from San Francisco to New York emits approximately 1.2 tons of CO₂e, providing the required value."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72% time-saving,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""",The context explicitly states that the growth strategy reduced training time by 72% for the 101B model compared to a from-scratch approach.
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.94,43.94,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""On average, FLM-101B achieves a score of 43.94,""","The document states that the FLM-101B model’s average score on the Open LLM Leaderboard is 43.94, which is the final average performance score requested."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the amortized training cost of GPT‑4 ($40 M) but do not give a monetary total training budget for FLM‑101B, only FLOP and energy figures. Without a cost figure for FLM‑101B, the factor cannot be determined from the provided context."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the solar farm’s annual CO₂e avoidance is equivalent to taking more than 13,900 cars off the road, providing the requested estimate."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",3700 kg,3700,kg,is_blank,is_blank,"Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq","The context specifies that NVIDIA’s embodied carbon emissions are estimated at 3700 kg CO2eq per GPU, based on Luccioni et al. (2023)."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,"""Llama 3.2 1B 8 0.036 12.0 0.054 12.64 21.5 bil.""","The table in the cited documents lists the GPU Power Usage for the Llama 3.2 1B model at a request frequency of 8 requests per second as 0.036 kWh, which directly answers the question."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021;luccioni2023,https://arxiv.org/pdf/2104.10350;https://arxiv.org/pdf/2302.08476,"""It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS."" (Doc [patterson2021])
""...the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs)"" (Doc [luccioni2023])","Both documents explicitly state that training GPT‑3 on 10,000 V100 GPUs at 24.6 TFLOPS/sec requires approximately 14.8 days, providing the same numeric value with the same units."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""more than 6,750 fold improvement in processor clock speed"" and ""Intel 4004 … ran at 740kHz … typical microprocessor … capable of running at 5,000,000kHz""","The cited passage explicitly states a 6,750‑fold increase in clock speed from 740 kHz (Intel 4004) to 5 000 000 kHz (typical 2021 processors)."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages give premature death estimates for 2028 (≈1,300) but do not include any figure or projection for 2030, so the requested information cannot be determined with confidence from these documents."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24 data centers,24,data centers,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.""",The context states that in 2023 AWS increased the number of data centers using recycled water for cooling to 24. This directly answers the question of how many data centers began using recycled water for cooling in that year.
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"""The total permitted site‑level annual emission limits are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons."", ""The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons."", ""The total permitted annual emission limits for data center backup generators in Virginia are approximately 13,000 tons of NOx.""","The passages from document han2024 explicitly state that the total permitted annual emission limits for nitrogen oxides (NOx) from data center backup generators in Virginia (including northern Virginia) are about 13,000 short tons, which directly answers the question."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled,"The cited passage from Doc luccioni2025a states that roughly 22% of e‑waste is formally collected and recycled, directly answering the question."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of  2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.""","The preface of the document explicitly states the study was launched in the fall of 2014, making 2014 the launch year."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Table 1 lists the hyperparameter ""Nexperts"" as 8, and the text states ""we set the same number of experts to 8 and top‑k to 2 for every layer.""",The hyperparameter table and accompanying description explicitly state that each MoE layer in JetMoE‑8B contains 8 experts.
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"""a full training run would take 60 days.""","The document explicitly states that the 6.1 billion parameter model, trained for only 13 % of its full run, would take roughly 60 days to complete, providing the required estimate."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,"""The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).""",The quoted sentence directly states that the English portion of the FLM-101B model required 28.22 zettaFLOPs according to the cost estimation.
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3×,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.""","The quoted sentence from Wu et al. (2021) directly states that reaching 80% GPU utilization reduces the overall carbon footprint by a factor of three, which is the requested factor."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command‑R Plus,Command-R Plus,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus).","The appendix of the 2025 study lists GPU energy consumption per 1,000 queries, showing Command‑R Plus at over 3,426 Wh, the highest value reported."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided passages contain a clear numeric value for the pre‑training energy consumption of the BLOOM model in MWh. The excerpts mention energy ranges for other models and give cost estimates, but no explicit MWh figure for BLOOM is given, so an answer cannot be given with confidence."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context contains conflicting statements: one source lists 0.25% activated per token, another lists 0.1%. The documents do not provide a clear, consistent figure, so a confident answer cannot be given."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,is_blank,is_blank,"""we sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected""","The context explicitly states that the study sampled 88 machine learning models, which is the number requested."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2,2.2,is_blank,is_blank,is_blank,"""a log‑normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run. … we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.""","The cited passage states explicitly that the median ratio of total compute to the final training run compute is 2.2×, which answers the question. No other value is given in the provided documents."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the total execution time of a dense BlackMamba model with a batch size of 30 fine‑tuned on an NVIDIA A40‑48 GB GPU, so a confident answer cannot be derived."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons’ Paradox,Jevons’ Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.""",The quoted passage from the 2025 paper explicitly cites Jevons’ Paradox as the economic principle explaining why AI efficiency gains might lead to rebound effects rather than net environmental benefits.
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Figure 10. A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30‑50%, leaving room for utilization and efficiency improvements.""","The context explicitly states that most experimentation uses GPUs at 30‑50% capacity, which is far below the 80% threshold. Therefore, the claim that a majority use GPUs over 80% capacity is incorrect."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any mention of financial sentiment classification or the hardware processor used for energy‑efficient local inference in that setting.
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 – 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"""global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027""","The cited passage from the document states the projected range of global AI water withdrawal for 2027 as 4.2 to 6.6 billion cubic meters, which directly answers the question."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time‑saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""","The context explicitly states that using the growth strategy, the FLM‑101B model required 21.54 days of wall‑clock time to train, which is the answer to the question."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).""","The cited document explicitly states that Red AI is on the rise, contradicting the claim that it is on the decline, so the statement is false."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"""set the same number of experts to 8 and top-k to 2 for every layer."" (Doc shen2024); Table 1: JetMoE-8B hyperparameters – Top-k = 2.","The JetMoE‑8B architecture selects the top‑2 experts per token in each layer, as explicitly stated in the text and confirmed by the hyperparameter table."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,is_blank,is_blank,"""the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].""","The quoted passage from doc wu2021b#0060 explicitly states that global carbon emissions fell by 6.4% in 2020, attributing part of the decrease to vehicle transport during the COVID‑19 pandemic. This directly answers the question with the reported percentage drop."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,1287 MWh,1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity"" (jegham2025)","The quoted passage explicitly states that GPT‑3’s training consumed 1,287 MWh of electricity, which is the value requested."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets. These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.""","The quoted passage from Doc [luccioni2025c] explicitly names the collaborative project as the AI Energy Score, noting its goal of creating a unified, standardized method for comparing inference efficiency across AI models."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The provided documents state that compute increased 300,000× over six years, not 200,000×, so the numeric claim is incorrect; thus the statement is false."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22,22,is_blank,is_blank,is_blank,"""100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy"" (Doc amazon2023#0277) and ""22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022"" (Doc amazon2023#0278).","Both documents state that in 2023 22 AWS data center regions had all of their electricity matched with renewable sources, so the count is 22."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,about 69 years,69,years,stone2022,https://arxiv.org/pdf/2211.06318,"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.""",The documents state that AI was officially founded in 1956; subtracting that from the year 2025 gives an approximate age of 69 years.
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.""","The document states that Amazon held 1.3 GW of storage capacity in 2023, which directly answers the question."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context includes a figure caption and a textual description of the MoE kernel execution time breakdown, but it does not provide explicit numeric values for the longest kernel time at batch size 30 on a dense BlackMamba model. Therefore the documents do not contain enough information to give a confident answer."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific percentage for the energy reduction when targeting a 100 ms TPOT for the Llama 3.1 8B model. The only quantitative figure given is a 44% reduction at 77 ms, which does not directly answer the question about 100 ms."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,flights,jegham2025,https://arxiv.org/pdf/2505.09598,"""cumulative emissions from approximately 272 transatlantic flights between Boston and London.""","The context states that GPT-4o’s annual emissions are comparable to the cumulative emissions of about 272 transatlantic flights, providing a direct numeric comparison."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages include any data on the share of AI inference workloads in Asia that were powered by coal in 2023, so the answer cannot be derived from these documents."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",training and fine-tuning a large Transformer model with Neural Architecture Search (NAS),training and fine-tuning a large Transformer model with Neural Architecture Search (NAS),is_blank,luccioni2023;luccioni2025b,https://arxiv.org/pdf/2302.08476;https://arxiv.org/pdf/2504.00797,"""the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars""","The five‑cars estimate is derived from the 2019 Strubell et al. study, which quantified emissions from training a large Transformer model using Neural Architecture Search (NAS). The quoted passage directly states that the estimate is based on that specific, infrequently performed AI process."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.,"The context explicitly states that the AI field was officially christened in 1956 at the Dartmouth workshop, providing a clear date for the answer."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].""","The quoted passage from document wu2021b explicitly states that about 770 million people lack stable electricity access, matching the claim, so the statement is true."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B,2.0,B,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.",The context explicitly states that during inference JetMoE-8B activates 2B out of its 8B total parameters for each input token.
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The cited passage explicitly states that operational impacts include GHG emissions from servers and data center cooling, contradicting the claim that they are excluded."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The cited sentence from the Samsi 2024 context explicitly states that at a maximum generation length of 512 tokens, LLaMA‑65B consumes approximately 3–4 Joules per output token, providing the required energy-per-token value."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.""",The cited sentence in the Wu 2021 paper explicitly states that quantizing RM2 from 32‑bit to 16‑bit reduces its model size by 15 %. This directly answers the question.
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numerical value for the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model, so the answer cannot be derived with confidence."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",25000,USD,is_blank,is_blank,"Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context explicitly states that Grover’s training on 256 TPU chips over two weeks cost $25,000, which directly answers the question."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"at least 14,380 electric delivery vans",14380,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Region 2022 2023
U.S. 2,600 11,800
Europe 1,220 3,000+
India 3,800 7,200+","The table shows the number of electric delivery vans in 2022 and 2023 for each region. The increases are 11,800‑2,600 = 9,200 for the U.S., at least 3,000‑1,220 = 1,780 for Europe, and at least 7,200‑3,800 = 3,400 for India. Adding these gives at least 14,380 vans added across the two years."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.1,1.1,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"To account for the data center non-IT energy overheads, we conservatively assume a power usage effectiveness (PUE) of 1.1, which is a fairly low value even for state‑of‑the‑art data center facilities [4].","The provided context explicitly states that AI‑dedicated data centers are assumed to have a PUE of 1.1, representing the global average PUE for AI in 2023."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].""","The passage from ebert2024 explicitly states the global average PUE for 2023 as 1.58, which is the required answer."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,2.195 L/kWh,2.195,L/kWh,morrison2025,https://arxiv.org/pdf/2503.05804,"""WUE of 1.29 liters per kWh"" and ""WUE of 3.1 liters per kWh"" for Google’s AI‑dedicated clusters","The two AI‑dedicated clusters (Jupiter and Augusta) have reported WUE of 1.29 L/kWh and 3.1 L/kWh respectively. Averaging these two values yields (1.29+3.1)/2 = 2.195 L/kWh, which is the average WUE for Google’s AI‑dedicated data centers in 2024."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,"""optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)""","The quoted passage directly states that the optimizer stage accounts for up to 53% of the running time for BlackMamba sparse fine-tuning on an NVIDIA A40-48GB GPU with batch size 1, which is the requested percentage."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",TRUE,1,is_blank,is_blank,is_blank,"Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy","The quoted passage from Patterson et al. explicitly states that sparsely activated DNNs use less than one-tenth the energy of dense models while maintaining accuracy, directly supporting the true claim."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,33.8 Wh,33.8,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""a long, high-reasoning query reaches an average of 33.8 Wh""","The context from doc jegham2025 states that a long, high-reasoning query (the o3 model’s long prompt) consumes on average 33.8 Wh, directly answering the question."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,is_blank,is_blank,"""Gemini Ultra has the highest fraction of R&D staff cost at 49%""","The context explicitly states that Gemini Ultra’s R&D staff cost—including equity—constitutes 49% of its total development cost, which is the requested percentage."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The seventeen-member Study Panel""","The context explicitly states that the inaugural 2015 Study Panel comprised seventeen members, providing a clear numerical answer."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain the specific energy figure of 0.083 kWh per 1,000 queries for Flan‑T5‑xxl, so the daily consumption cannot be calculated from the given documents."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c;luccioni2025a,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2501.16548,"The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s exp.
This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that “Research shows that by scaling currently proven applications an…”, but the detailed methodology is not provided.","Both documents indicate that the 5‑10% reduction figure lacks clear, publicly available calculations and detailed scientific grounding, so the claim is not supported."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the distance from Earth to the Sun.
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""manufacturing carbon cost accounts for 74% of the total footprint""","The quoted passage from Wu et al. (2021) directly states that 74% of a client device’s total carbon footprint is due to manufacturing, providing the required percentage."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages provide a specific number of wind turbines directly contracted by Microsoft for Azure AI clusters in 2023, so the answer cannot be derived with confidence from these documents."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.","The cited passage explicitly states that higher parameter counts do not guarantee higher inference energy consumption, contradicting the ""always"" claim. Thus, the statement is false."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any figure or statement indicating the amount of CO2 emitted by OpenAI's API requests in January 2024, so the answer cannot be determined from the documents."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.""","The quoted passage explicitly states that the Study Panel found no cause for concern that AI poses an imminent threat to humankind, indicating that the panel is not concerned. Therefore the statement is false."
q080,True or False: The AlphaGo program defeated the human Go champion.,True,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match""","The quoted passage from Doc stone2022 explicitly states that AlphaGo defeated the human Go champion, confirming the statement as true."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e, over 40× the five cars estimate",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the ‘five cars’ estimate.""","The quoted sentence from the Luccioni 2025c document explicitly states the pre‑training emissions for Meta’s Llama 3 family as 11,390 tCO₂e and indicates that this figure is more than forty times the reference ‘five cars’ estimate."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9×,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.,"The cited sentence directly states that the explosive growth in AI use cases led to a 2.9‑fold increase in AI training infrastructure capacity over the 1.5‑year period, answering the question."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"""The entire alignment process takes 60 H100 GPU hours.""","The cited passage explicitly states that the full alignment, encompassing both dSFT and dDPO fine‑tuning, requires 60 H100 GPU hours, providing a clear numeric answer."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,is_blank,is_blank,"""Continuous batching mitigates this by dynamically replacing completed requests with new ones"" (Doc fernandez2025#0045)",The quoted sentence explicitly names the strategy as Continuous batching and describes its key feature: dynamically replacing completed requests with new ones to reduce idle GPU time.
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,32,32,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)"" (legend indicating the maximum batch size for BlackMamba sparse fine‑tuning on GSM8K with an NVIDIA A40 GPU).","The figure legend lists the batch sizes tested for BlackMamba, showing the largest sparse batch size as 32. This corresponds to the maximum batch size that fits in the 48 GB A40 GPU memory for GSM8K fine‑tuning in a sparse setup."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).""","The context from the 2025 study’s appendix states the minimum GPU energy for 1,000 inference queries is 0.06 Wh and the maximum is over 3,426 Wh. The answer reflects this range with units Wh."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,patterson2021,https://arxiv.org/pdf/2104.10350,"""The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].""","The quoted sentence from Doc [patterson2021] provides the exact gross carbon intensity value for the U.S. average energy mix in 2021, stated as 0.429 kg CO2e per kWh."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams of CO2eq per 1,000 inferences",1594,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"""the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences""","The quoted passage directly states that stable‑diffusion‑xl‑base‑1.0 emits 1,594 gCO2eq per 1,000 inferences, which is the required numeric value."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied context does not contain any information about classification experiments on German public administration texts or which sentence‑embedding model achieved the highest accuracy for that task.
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)""","The passage from luccioni2025b explicitly states that researchers do not believe a universal, one‑size‑fits‑all approach to AI ethics and sustainability can be developed, contradicting the statement in the question."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,is_blank,is_blank,"""Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion."" (Doc [erben2023#0021])

""We used the Hivemind framework for all of our experiments, as it provided the base for training on spot instances in high latency, low bandwidth networks."" (Doc [erben2023#0165])",Both passages identify Hivemind as a PyTorch‑based decentralized framework used for distributed training of spot instances across multiple clouds and continents.
q093,How many parameters does the largest T5 model have?,11 billion parameters,11000000000,parameters,patterson2021,https://arxiv.org/pdf/2104.10350,The largest size has 11B parameters,"The Patterson et al. document states that the largest T5 model contains 11 billion parameters, which directly answers the question."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,Table 2 shows that for the 101B stage the utilization is 52.88%.,"The table entries for the 101B growth stage list a utilization of 52.88 %, which directly answers the question about the achieved FLOPs utilization percentage in the final growth stage."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency,"The table in document ""khan2025"" lists the metric named ""Carbon Intensity"" with the definition ""CO2 emissions per unit of electricity consumed""."
q094,What is the total number of parameters in the JetMoE-8B model?,8B,8,B,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B contains 8 billion parameters, confirming the total parameter count."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the notion of transparency in AI can be expanded to encompass “social transparency”, which involves integrating socio-technical aspects in the description and understanding of AI systems""","The quoted passage from Luccioni et al. explicitly names ""social transparency"" as the proposed extension of AI transparency that includes socio-technical aspects and societal/environmental footprint."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,is_blank,is_blank,"""We develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.""",The excerpts from the Chen et al. documents explicitly state that the system named Lamina was developed and it uses model‑attention disaggregation as its key feature.
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"InferSave selected g4dn.xlarge with a total cost of $2.13, while the Max‑Performance policy selected g6e.xlarge with a total cost of $2.699. The difference is (2.699−2.13)/2.13×100≈26.7% more expensive.","The table for a 100 TPS offline workload lists InferSave’s chosen instance cost ($2.13) and Max‑Performance’s chosen instance cost ($2.699). Computing the relative increase yields approximately 26.7%, which is the percentage more expensive."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810×,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer‑based universal translation model by 810×.""",The cited sentence explicitly states that full‑stack optimization reduces the operational carbon footprint by a factor of 810 compared to a CPU server baseline.
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"Importantly, the company’s data center water consumption increased by ∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4].","The quoted passage from Doc [li2025b] directly states a 20% increase in Google’s data center water consumption from 2021 to 2022, which matches the figure reported in Doc [luccioni2025a]."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,59%,0.59,is_blank,is_blank,is_blank,"""performance drop of 41% (C-8) compared to the fully local experiment (A-8)"" and ""intercontinental training leads to a performance drop of 41% (C-8) compared to the fully local experiment (A-8)""","Both passages state that NLP training across four continents suffered a 41% performance drop relative to a fully local run. Therefore, the throughput achieved is 100% − 41% = 59% of the local throughput."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,"Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.  In particular, with the best configuration, zero‑shot reduced the consumption of about 7%, whereas one‑shot and few‑shots decreased their consumption of about 99% and 83% respectively.","The quoted passage explicitly states that custom tags lower energy use for zero‑shot, one‑shot, and few‑shot source‑code‑completion prompts, confirming the statement is true."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,is_blank,is_blank,"""on average, 44% goes toward AI accelerator chips.""","Both documents state that, on average, 44 % of the total amortized hardware and energy cost is attributed to AI accelerator chips, providing a clear numeric answer."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5 billion liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""3.5B Liters of water returned to communities from replenishment projects in 2023, with additional volume contracted and r""",The context explicitly states that 3.5 billion liters of water were returned to communities through Amazon’s replenishment projects in 2023.
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]""","The quoted passage explicitly names the Finnish project as ""ETAIROS"" and states that it proposed integrating ethics, sustainability, design, and foresight for interdisciplinary AI governance."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,M,cottier2024,https://arxiv.org/pdf/2405.21015,"""the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.""","The context explicitly states that Google’s Gemini Ultra training cost is $30M, indicating the estimated amortized training cost for that model."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any information about the energy consumption of the DS Llama 70B model on the FKTG dataset, so a confident answer cannot be derived."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 µg/m³,9,µg/m3,han2024,https://arxiv.org/pdf/2412.06288,"""The EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m3""","The context explicitly states that the EPA’s recently tightened primary standard for the annual average limit of PM2.5 is 9 µg/m³, and the supporting quote confirms this value."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,"""Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient..."" and ""the PUE of Facebook datacenters is 1.10 (2020).""","Both cited documents state that Facebook’s data centers achieve a PUE of approximately 1.10, providing a clear numeric value for the answer."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Although environmental protection is included in the Act’s objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required.""","The context states that while the AI Act requires risk assessments for GPAI models with systemic risk, it explicitly notes that environmental risks are not currently mandated to be included in those assessments. Thus the statement that the Act requires environmental risks to be included is unsupported and false."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""115 books would produce the same amount of CO2 as a single Amazon Kindle device""","The cited life cycle assessment directly states that 115 physical print books emit the same amount of CO2 as one Amazon Kindle e‑reader, providing the numeric equivalence needed for the answer."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""per-household health burden could be 200x more than that in less-impacted communities.""","The cited passage explicitly states that disadvantaged communities could experience a per‑household health burden 200 times higher than less‑impacted communities, indicating the factor requested."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million,3700000,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023),"The 2025 paper states that NVIDIA shipped 3.7 million GPUs in 2024, directly answering the question."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied passages do not contain any mention of the number of parameters in the large language model analyzed in Dodge et al. (2022).
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,is_blank,is_blank,"Training energy (kWh) 51,686  Finetuning energy (kWh) 7,571","The table lists training energy of 51,686 kWh and fine‑tuning energy of 7,571 kWh for BLOOMz‑7B; adding them yields 59,257 kWh, which is the combined energy cost reported by the Power Hungry Processing study."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40 million,40,million USD,is_blank,is_blank,the most expensive publicly‑announced training runs to date are OpenAI’s GPT‑4 at $40M and Google’s Gemini Ultra at $30M.,"The quoted sentence directly states that the amortized training cost of GPT‑4 is $40M, which is the value requested."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""image generation 2.907 3.31""","The table lists the mean energy for image generation as 2.907 kWh per 1,000 inferences, which directly answers the question."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 lbs",36156,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"""American life, avg, 1 year 36,156""","The table in Doc [strubell2019] lists the CO₂e emissions for an average American life over one year as 36,156 pounds, which directly answers the question."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Rebound effect (Jevons Paradox),Rebound effect,is_blank,luccioni2025b;luccioni2025a,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/2501.16548,"""The relationship between efficiency and sustainability is far from straightforward, given phenomena such as rebound effects, in which improved efficiency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resources"" (Doc [luccioni2025b#0037])
""Technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use"" (Doc [luccioni2025b])
""Jevons Paradox … increased efficiency drives systemic demand"" (Doc [jegham2025#0095])","The quoted passages explain that improved efficiency can paradoxically lead to more usage and higher total resource consumption, a phenomenon known as the rebound effect or Jevons Paradox."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about Mistral-small's emissions or its change after optimization in the financial sentiment classification task, so the answer cannot be determined with confidence."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy""","The quoted passage from the 2024 study specifies the total energy consumption for all model experimentation and evaluation as 754.66 kWh, which is directly used as the answer."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific name of a West Virginia county or its projected per‑household health cost for 2030, so the answer cannot be determined with confidence from the supplied documents."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,Params 175B 280B 540B 130B 70B 101B,The context lists the parameter counts for various models and shows that the final FLM‑101B model has 101 B parameters.
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.""","The quoted sentence from the Luccioni 2025c document explicitly states that 84% of token usage in May 2025, as reported by OpenRouter, went through models that did not disclose any environmental impact data."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided context passages contain any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. Therefore the answer cannot be determined from the given documents.
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,about 3 passengers,3,is_blank,is_blank,is_blank,"Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.",The passage explicitly states that the 3.2 tCO2e emissions of the Evolved Transformer NAS are equivalent to approximately three passengers on a round‑trip San Francisco–New York flight.
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,is_blank,is_blank,"""If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it w""","The cited document states that a full training run of the 6.1‑billion‑parameter transformer would emit between 21 and 78 metric tons of CO₂, providing the required range."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context contains freshwater consumption figures for Meta’s data centers and AI operations for 2023, but no explicit figure for Meta’s Llama 3 inference serving clusters in 2024 is mentioned. Therefore the answer cannot be determined from the documents."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""most carbon footprint analyses gather the information manually by writing to authors.""","The context states that most analyses collect data manually by contacting authors, contradicting the claim that they gather information automatically. Hence the statement is false."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""13B 2 64 1 64"" from the table showing bare minimum hardware required for LLaMA 13B on A100 80GB GPUs.","The table in the cited document lists the bare minimum number of GPUs for each model. For the 13B model, the entry for A100 80GB shows 1 GPU, indicating that a single A100 80GB is sufficient for inference without compression or quantization."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention of a dataset used for German nuclear waste site objection texts, so the answer cannot be determined from the documents."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",$4.63/hr,4.63,USD/hr,chen2024,https://arxiv.org/pdf/2405.01814,"""Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr""","The table in Chen et al. (2025) lists the price per chip for the NVIDIA H20 as $4.63 per hour, directly answering the question."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit numeric value for carbon emissions avoided by pruning and quantizing large language models in 2023, so it cannot be answered confidently."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,This is equivalent to approximately 44% of the data centers’ total electricity cost.,"The context states that in 2023 the public health cost of U.S. data centers was about $6.7 billion, which is approximately 44% of the data centers’ total electricity cost using the average attribution method."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a numeric energy value for a full Meena training run, so the ratio cannot be computed."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592"" – where the first value corresponds to BLOOMz‑7B","The table lists the number of inferences at which inference energy equals training plus fine‑tuning energy for each BLOOMz model. The first entry (592,570,000) is for BLOOMz‑7B, giving the required inference count."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""reduce energy consumption and carbon emissions by up to 45% post quantization""","The quoted passage from Doc [khan2025] explicitly states that sustainable deployment techniques such as quantization can achieve up to a 45% reduction in carbon emissions, supporting the truth of the statement."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95,95,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""we were only able to collect 95 answers""","The context states that after contacting over 500 authors, the researchers received 95 responses, which is the direct answer to the question."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,United Kingdom 36 901,"The table of Amazon Renewable Energy Projects lists the United Kingdom with 36 projects, indicating that 36 projects were announced as of January 2024."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","The bare‑minimum hardware table for LLaMA‑7B lists 1 GPU for both V100 and A100, indicating that a single NVIDIA A100‑80GB GPU is sufficient for inference without compression or quantization."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,is_blank,is_blank,"""Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.""",The quoted sentence explicitly states that the hybrid 2 A100 + 1 A10G configuration achieves a 24% cost saving compared to an A100‑only strategy.
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Approximately $3.3 per H100 GPU‑hour,3.33,USD/h,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B, a new LLM trained with less than$0.1 million, ... 30,000 H100 GPU hours.","The context states the JetMoE-8B training cost was under $100,000 and consumed 30,000 H100 GPU hours. Dividing 100,000 by 30,000 yields about $3.33 per GPU‑hour, which is the approximate cost per H100 GPU‑hour."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific percentage for the health cost relative to electricity cost when training a Llama-3.1 scale model in Altoona, Iowa, so the answer cannot be determined with confidence."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year""","The coalition estimate states the deal could increase emissions by 640 percent, which is equivalent to 6.4 times the annual carbon removal target."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses","The context defines ""water withdrawal"" exactly as the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses, which directly answers the question."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context contains fragmented percentage tables but does not explicitly state the overall percentage of Amazon's U.S. workforce identified as men in 2023. No clear single figure is supported by the documents.
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,li2025b,https://arxiv.org/pdf/2304.03271,Apple reports that its supply chain accounts for 99% of its total water footprint [23].,"The quotation from Doc [li2025b] explicitly states that 99% of Apple’s total water footprint comes from its supply chain, which directly answers the question."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].""","The context from Doc wu2021b explicitly states that the average U.S. household had 25 connected devices in 2021, which is reflected in the answer and supporting quote."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,is_blank,is_blank,"""we introduce thegranularity metric, the ratio of calculation to communicati""",The context states that the authors introduced the granularity metric as the ratio of calculation to communication time for assessing scalability across continents.
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25 trillion tokens,1250000000000.0,tokens,is_blank,is_blank,"JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context explicitly states that JetMoE-8B was trained on 1.25 trillion tokens, which is the number of tokens used for pre‑training."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8–3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)""","The Luccioni 2025 study reports a publicly available range of pre‑training energy consumption for LLMs from 0.8 MWh to 3,500 MWh, which directly answers the question."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011,""","The provided context explicitly states that Watson beat human contenders in Jeopardy, contradicting the claim that it did not. Thus the statement is false."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,is_blank,is_blank,"""the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.""","The quote explicitly states the Standing Committee forms a Study Panel on a five‑year cycle, which directly answers the question about the frequency."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit numeric value for the total execution time of a sparse BlackMamba model fine‑tuned on an NVIDIA A40-48GB with a batch size of 84, nor does it provide sufficient data to compute it from throughput or latency figures. Therefore, I cannot answer the question with confidence based on the documents."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10–50 medium‑length completions,"[10, 50]",is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT‑3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium‑length responses, depending on when and where it is deployed.""","The context specifies that one 500 mL bottle of water supports between 10 and 50 medium‑length GPT‑3 completions, so the number of possible completions is a range of 10–50."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,is_blank,is_blank,"""Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings""","The quoted sentence from Griggs et al. (2024) explicitly states that Mélange can cut deployment costs by up to 77% in conversational chat settings, which directly answers the question."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650","The MT‑Bench table shows JetMoE‑8B‑chat scoring 6.681, which is higher than Llama‑2‑13b‑chat’s 6.650, confirming the post‑alignment boost."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",is_blank,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,"""one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68]"" and ""GPT‑3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium‑length responses"",""","Both documents report that approximately 10 to 50 GPT‑3 queries consume about half a liter of water. The range is directly quoted, supporting the answer of 10–50 queries."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.""","The quoted text explicitly states that the minimum configuration for inference of the 65B LLaMA model, without compression or quantization, is 4 A100 GPUs with 80 GB each. Hence the bare minimum number of such GPUs is 4."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80–90%,"[80, 90]",%,is_blank,is_blank,"""NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].""",The quoted sentence directly states NVIDIA’s 2019 estimate that inference comprises 80–90% of the overall machine learning workload.
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.5164 per hour,7.5164,$/h,griggs2024,https://arxiv.org/pdf/2404.14527,"""On-demand Price ($/h) 0.7 1.01 3.67 7.5164""","The context lists the normalized on‑demand hourly price for the H100 GPU as 7.5164 dollars per hour, which is the value requested."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",Approximately 997 million inferences,997000000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"""we estimate a full training run would consume approximately 103,593 kWh."" (dodge2022#0054) – and from Table 3 of luccioni2024: ""BLOOMz‑7B 7B 14.46 0.104"" indicating 0.104 kWh for 1,000 inferences (i.e., 0.000104 kWh per inference).","The full‑training energy for a 6.1 B‑parameter model is 103,593 kWh (dodge2022#0054). BLOOMz‑7B consumes 0.104 kWh per 1,000 inferences, so each inference uses 0.000104 kWh. Dividing the total training energy by the per‑inference energy gives 103,593 kWh ÷ 0.000104 kWh ≈ 997 million inferences, matching the training cost."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the direct release of environmental information peaked in 2022 ... introduction of increasingly commercial and proprietary models after 2022 ... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.""","The 2025 paper states that after 2022 the direct disclosure of environmental information declined rather than increased, showing a reversal in the trend. Hence the claim that it continued to increase is false."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts contain water usage figures for GPT‑3 training and GPT‑4o inference, but none that specify the amount of water used for cooling during the specific GPT‑4 training run. Thus the documents do not provide enough information to answer the question with confidence."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","1,000× larger",1000,is_blank,is_blank,is_blank,"""with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.""","The cited sentence explicitly states that a 1,000‑fold increase in model size is needed to raise the BLEU score from 5 to 40 for GPT‑3‑based translation. Therefore the model must be 1,000× larger."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000",10000,round trips,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.""","The quoted passage from the context explicitly states that training a Llama‑3.1‑scale model generates air pollutants equivalent to over 10,000 LA‑NYC round trips, which directly answers the question."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,FALSE,0,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"""Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst‑case overestimation of energy."" (Chung et al. 2025)  
""While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding real‑time power consumption is only possible by using a tool like Code Carbon during the training process."" (Luccioni, 2023)","The cited passages indicate that TDP‑based estimates typically overstate real power use and are not considered reliable or accurate, thus the statement is false."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear numeric value for the ground truth throughput of Mixtral-CS-A100-40GB at batch size 1. While throughput for batch size 2 is mentioned (0.5 qps), no explicit value for batch size 1 is given, so the answer cannot be determined with confidence."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes."" ""GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.""","The cited passages state that GPT‑4o mini uses about 20% more energy than GPT‑4o on long queries, and report higher watt‑hour consumption (3.098 Wh vs 2.875 Wh). Therefore GPT‑4o mini does not consume less energy per query than GPT‑4o."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$7.22 per hour,7.22,USD,is_blank,is_blank,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.""","The context states a monthly cost of $5,200 for 2 A100 GPUs. Dividing by 30 days × 24 hours = 720 hours gives an hourly cost of approximately $5,200 ÷ 720 ≈ $7.22."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"""30,000 H100 GPU hours.""","The JetMoE-8B pre-training is reported to have used 30,000 H100 GPU hours, as stated directly in the provided context."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",$1 billion,1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"""If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027,""","The cited passage explicitly states that, under the continuing cost trend, the largest training runs will exceed one billion dollars by 2027, which directly answers the question."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192,192,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.""","Each of the 24 servers contains 8 A800 GPUs, so 24 × 8 = 192 GPUs in total."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain a clear, explicit figure for the total CO2 equivalent emissions generated by the entire Power Hungry Processing study."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents contain the CO2 emission value (626,155 lbs) for training a Transformer model with neural architecture search, but they do not provide the necessary emissions‑to‑driving‑distance ratio or any numeric driving distance that could be used to compute an equivalent driving distance. Therefore the question cannot be answered with confidence using only the supplied context."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"""FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.""","The context explicitly states that training FAIR’s RoBERTa on 160 GB of text required about 25,000 GPU hours, which is the value used in the answer."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages mention AlexNet and ImageNet top-1 accuracy but do not state a specific numeric value for AlexNet’s 2012 performance, so the answer cannot be determined with confidence from the documents."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","≈60,610 MWh",60609.6,MWh,is_blank,is_blank,"""Inference energy (kWh) 1.0 × 10−4"" (Table 5, BLOOMz-7B) and ""BLOOMz-7B has been downloaded 606,096 times"" (Context, Doc luccioni2024#0090).","Each inference consumes 1.0×10⁻⁴ kWh. With 606,096 downloads × 1 000 000 inferences/download = 6.06096×10¹¹ inferences, the total energy is 6.06096×10¹¹ × 1.0×10⁻⁴ kWh = 6.06096×10⁷ kWh, which equals 60,609.6 MWh, rounded to ≈60,610 MWh."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000",35000,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Even a 0.42 Wh short query, when scaled to 700 M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes","The context explicitly states that 700 million daily GPT‑4o queries, each 0.42 Wh, produce yearly electricity consumption equivalent to 35,000 U.S. residential households. Thus the answer is 35,000 homes."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 NVIDIA V100 32GB GPUs,8,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"""8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.""","Both documents state that the bare‑minimum hardware for LLaMA‑65B inference without compression or quantization is 8 NVIDIA V100 GPUs, each with 32 GB of memory. The quoted sentence directly provides the required GPU count, confirming the answer."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons",47400,metric tons,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The report explicitly states that Amazon’s on‑site solar systems avoid about 47,400 metric tons of CO₂e annually, which directly answers the question."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons""","The 2025 paper (Doc luccioni2025a) explicitly states Microsoft’s global water consumption rose by 34% from 2021 to 2022, which is the figure asked for."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain no information about Yelp sentiment analysis benchmarks or comparisons between traditional models and large language models, so a confident answer cannot be derived."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11""","The provided excerpt from Patterson 2021 explicitly states that the PUE for Google's Iowa datacenter during the Evolved Transformer run was 1.11, giving a direct numeric answer."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion GPT-4o queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"""yielding a total of approximately 772 billion GPT-4o queries in 2025""","The quoted passage from Jegham 2025 explicitly states that the total estimated number of GPT-4o queries for 2025 is about 772 billion, which is the figure used in the answer."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].,"The passage from the Patterson 2021 document explicitly states that OpenAI reported 3.14×10^23 floating point operations for training GPT‑3, which is the total FLOPs required."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit energy consumption values for the Llama 3.1 70B model on one node versus two nodes, so a factor increase cannot be determined from the documents."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,Megatron-LM,Megatron-LM,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"M. Shoeybi, M. Patwary, R. Puri et al., “Megatron-lm: Training multi-billion parameter language models using model parallelism,” 2020.","The paper cites Megatron-LM as the framework for training and inference of large language models across multiple GPUs and nodes, indicating it was used for deployment."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The US national datacenter average in 2018 was 1.58, which is the value used ; In 2020, it was 1.59 .""","The passage from the Patterson 2021 document explicitly states that the US national datacenter average PUE in 2020 was 1.59, which directly answers the question."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages contain a specific number of AI training runs conducted globally on renewable-only power in 2022. Therefore, the answer cannot be confidently derived from the documents."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""53% of articles cite the figure of 3 Wh per ChatGPT query or claim it con""","The analysis of 100 news articles found that 53% cited the contested 3 Wh per query estimate, which is the figure in question. This percentage is directly quoted in the provided context.}"
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29%–49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""R&D staff costs including equity are between 29% and 49% of the total amortized cost.""","The cited passages from Cottier et al. (2025) explicitly state that for the four models (GPT‑3, OPT‑175B, GPT‑4, and Gemini Ultra), the R&D staff costs—including equity—constitute 29% to 49% of the total amortized cost. This directly answers the question with the specified percentage range."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].""","The cited passage states that open‑source general‑purpose AI models are excluded from reporting unless they pose systemic risk, which matches the claim that they are fully exempt except in that case."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,is_blank,is_blank,"""When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.""","The provided context explicitly states that for the OPT‑2.7B model on a g4dn.xlarge instance, the KV cache grows to 5.312 GB when the batch size is 32. This directly answers the question."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"""Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.""",The context explicitly names the function as the Compute Time Calibration Function (CTCF) and states it improves instance selection accuracy by adjusting for differences between theoretical and actual GPU performance.
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,percent,shen2024,https://arxiv.org/pdf/2404.07413,OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0,"The table lists the OpenLLM Leaderboard average scores for four models in the same column order as the MT‑Bench table (GPT‑4, GPT‑3.5‑turbo, Claude‑v1, JetMoE‑8B).  The fourth value, 53.0, is therefore JetMoE‑8B’s final average score on the benchmark suite."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","284,019 kg (626,155 lbs) of CO₂, roughly equivalent to 2.3 average American lifetimes.",284019,kg,strubell2019,https://arxiv.org/pdf/1906.02243,"""estimated that the emissions of training and fine‑tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO₂"" and ""cl. fuel, 1 lifetime 126,000""","The Strubell et al. estimate gives 284,019 kg CO₂ for NAS. Dividing this by the reference value of 126,000 kg per average American lifetime yields ≈2.3 lifetimes, so the NAS process is equivalent to about two to three average American lifetimes."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall""","The cited sentence from the context directly states that more GPU shards raise the energy cost per response for the LLaMA‑65B model, confirming the statement is true."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]""","The context from Doc [luccioni2025a] explicitly states that these four companies accounted for almost 30% of all corporate PPAs in 2020, which is the percentage requested."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].""","The passage states that open‑source general‑purpose AI models are excluded from reporting obligations under current EU rules, so they are not required to report energy consumption to authorities."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,fernandez2025,https://arxiv.org/pdf/2504.17674,"""used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference"" (Doc [luccioni2024#0034]); ""measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking"" (Doc [morrison2025#0042]); ""measured using Nvidia Management Library (NVML) via the CodeCarbon library (Courty et al., 2024)"" (Doc [fernandez2025])",The quoted excerpts explicitly state that CodeCarbon was employed to track and measure energy consumption during inference runs in the cited studies.
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give water consumption per query and water consumption per kWh, but they do not provide a conversion factor from liters to gallons. Without that conversion, a gallon‑based answer cannot be derived solely from the supplied context."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",16.07 zettaFLOPs,16.07,zettaFLOPs,cottier2024,https://arxiv.org/pdf/2405.21015,"""Based on the speed test results and total budget, we train the 16B model with 245.37B tokens, the 51B model with 39.64B tokens, and the 101B model with 26.54B tokens."" (li2025a#0026)

""For example, smaller versions of GPT-3 used 4.5e22 FLOP (based on compute = 6× parameters × tokens)…"" (cottier2024)","Using the 101‑B model’s token count (26.54 B) and the standard compute estimate of 6 × parameters × tokens, the total FLOPs for the final stage is 6 × 101 B × 26.54 B ≈ 1.61 × 10²² FLOPs, i.e., about 16.07 zettaFLOPs."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",13-51%,"[13, 51]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Mélange achieves 13-51% cost reduction (120ms SLO)""","The cited passage explicitly states that on the Arena dataset with a 120 ms SLO, Mélange reduces cost by 13 % to 51 % compared to single‑GPU baselines."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"""Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.""","The quoted sentence states that the total public health cost for U.S. data centers in 2023 is about $6.7 billion, which directly answers the question."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].""","The quoted passage directly states that AI’s public health costs are not evenly distributed but are concentrated in low‑income communities, contradicting the claim of even distribution."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",TRUE,1,is_blank,is_blank,is_blank,"Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The cited figure explicitly states that GPU theoretical performance per watt doubles every 3‑4 years, supporting the true statement."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 metric tons CO2e,26,tCO2e,li2025a,https://arxiv.org/pdf/2309.03852,net tCO2e 552 380 271 257 291 26,"The table for FLM-101B lists the net carbon emissions as the last value, 26 metric tons of CO2 equivalent, indicating the pre‑training emissions for that model."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""","The context explicitly states that the AI Environmental Impacts Act bill was introduced by Senator Edward J. Markey on 1 February 2024, identifying him as the sponsor."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",2.18e-5 kL,2.18e-05,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"""Mining 1 kg of rare earth materials consumes about 11 kL of water"" and ""one 12‑inch silicon wafer weighs 125 grams and produces about 63 H100s"" and ""assuming an H100 is 0.1% rare earth metal by mass.""","The context states that 1 kg of rare earth mining uses 11 kL of water. An H100 GPU weighs 125 g/63 ≈ 1.984 g, and contains 0.1% rare earth, i.e., 1.984 × 10⁻⁶ kg. Multiplying this by 11 kL/kg gives ≈ 2.18 × 10⁻⁵ kL of water per GPU."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The quoted sentence from Doc [khan2025#0033] explicitly states that 4‑bit quantization was applied via Ollama, which also supports local deployment of large language models for the financial sentiment case study."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,2,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Table: Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","The table lists the bare minimum number of V100 GPUs needed for each LLaMA model. For the 13B model it shows a count of 2 V100 GPUs, which is the required minimum. The answer is therefore 2 GPUs."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The pre‑training of Google’s Gemma family of language models emitted 1,247.61 tCO₂e, which is roughly four times the ‘five cars’ estimate.",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the ‘five cars’ number,""","The context states that Gemma pre‑training produced 1,247.61 tCO₂e, which is explicitly described as more than four times the emissions of the ‘five cars’ benchmark. This directly supports the numerical answer and the comparison claim."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,3.7 years,3.7,years,cottier2024,https://arxiv.org/pdf/2405.21015,"Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.","The cited passage from the Cottier 2024 document states an expected GPU hardware lifetime of 3.7 years, which is the estimated average GPU lifetime before retirement in AI data centers for 2024."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain any explicit statement indicating that the relationship between runtime and energy consumption is nearly linear in the referenced experiments. Therefore, I cannot confidently answer the question based solely on the supplied material."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy","The context from the Amazon Sustainability Report states that in North America AWS can reduce customers’ workload carbon footprints by up to 96% when powered by 100% renewable energy, which directly answers the question."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,is_blank,is_blank,"""Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).""","The cited paper states that ELMo requires 2 weeks, equivalent to 336 hours, when trained on three GTX 1080 GPUs, which matches the configuration of three GTX 1080 Ti GPUs used in the question."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",$11.06/hr,11.06,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr,"The table from Chen et al. 2025 lists the price per chip for the NVIDIA H100 as $11.06 per hour, which is the requested hourly price."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided passages do not contain a specific numeric value for the total execution time of a sparse Mixtral model with batch size 1 fine-tuned on an NVIDIA A40-48 GB GPU. No explicit time measurement or dataset size is given to calculate or infer the total training time.
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,SpotLake,SpotLake,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""SpotLake: Diverse Spot Instance Dataset Archive Service.""","The cited reference identifies SpotLake as a dataset archive service for spot instances, indicating it is used to shard and stream datasets to VMs that may be terminated at any time."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,is_blank,is_blank,"""predict that fine‑tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.""","Both cited passages state that the analytical model estimates the net cost for fine‑tuning a sparse Mixtral model on 2 million queries using an NVIDIA H100 GPU to be $3460, which directly answers the question."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context states that the U.S. national average water consumption for electricity generation is 3.1 L/kWh, directly answering the question."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,"""Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021"" (Doc [dodge2022])
""Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021)"" (Doc [wu2021b])","Both cited documents report a PUE of 1.10 for Google’s hyperscale data centers in 2021, confirming the value."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"""When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W...""",The cited passage from Morrison 2025 explicitly states that during active training the average GPU power exceeds 600 watts. Thus the answer reflects that value.
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96,96,GPUs,shen2024,https://arxiv.org/pdf/2404.07413,"""12 nodes and 96 H100s.""","The context explicitly states that the JetMoE-8B training cluster consisted of 12 nodes and a total of 96 H100 GPUs, so the total number of GPUs used is 96."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 lbs",11023,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"""Human life, avg, 1 year 11,023""","The Strubell 2019 document lists the average CO2e consumption for an average human life over one year as 11,023 pounds, which directly answers the question."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50–70%,"[50, 70]",%,is_blank,is_blank,"GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].","The quoted passage from Chung et al. explicitly states that GPUs consume 50–70% of the total provisioned power in a typical datacenter, which directly answers the question."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25×,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The context states that for LLaMA‑13B the inference throughput improves by about 1.25× when using A100 GPUs versus V100 GPUs, which directly answers the question."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Reporting the computational price tag of finding, training, and running models is a key Green AI practice""; ""We propose reporting the financial cost or ‘price tag’ of developing, training, and running models""","The cited passages from Schwartz et al. explicitly state that Green AI includes reporting the financial cost of finding, training, and running models, confirming the statement."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,is_blank,is_blank,"""A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)"" (Doc [jegham2025#0079])","The cited passage directly states that a short GPT‑4o query consumes 0.42 Wh, providing the required energy value."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"""about 280% more expensive than InferSave’s top choice.""","The quoted sentence from the document directly states that the Max-Performance instance g6e.xlarge costs about 280% more than InferSave’s top choice for the 400 TPS SLO, which provides the required percentage difference."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20×,20,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021.""","The document states that Facebook’s recommendation and ranking model sizes grew 20 times from 2019 to 2021, indicating a 20× increase."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,patterson2021,https://arxiv.org/pdf/2104.10350,"""Processor Average (Watts)  StDev   %  DNNs   used to calculate average power  TPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural  Architecture  Search   [So19] …  V100   GPU   325   2%   Transforme""",The table lists the average system power per processor: 221 W for TPU v2 and 325 W for V100. Subtracting gives 325 W − 221 W = 104 W.
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"""Training the GPT-3 language model in Microsoft’s state‑of‑the‑art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.""","The context explicitly states that GPT‑3 training in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater, providing the numeric value and unit required."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""electronic waste (e‑waste), which is now the fastest‑growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.""","The 2025 paper cites 62 million tonnes of e‑waste generated worldwide in 2022, directly answering the question."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]""","The quoted passage explicitly states that current average smartphone lifetimes are less than 3 years, supporting the true statement."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256.""","The quoted text explicitly states that the context window for FLM-101B is 2,048 tokens, which is the requested value."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825).","The quoted passage states that o3-mini has the highest cross‑efficiency score in the DEA analysis, indicating it ranked highest among the evaluated models."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Approximately 41 times,41,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""short, minimal reasoning query consumes only 0.67 Wh, ... long, high‑reasoning query reaches an average of 33.8 Wh"" (doc jegham2025). ""GPT‑4.1 nano … 0.827 Wh for long prompts"" (doc jegham2025#0068).","The long‑prompt energy for the o3 model is reported as 33.8 Wh, while GPT‑4.1 nano uses 0.827 Wh for the same prompt length. Dividing 33.8 by 0.827 yields a factor of ≈41, indicating the o3 model consumes about forty‑one times more energy than GPT‑4.1 nano for a long prompt."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 – 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,"""a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027""","The context from Doc li2025b explicitly states the projected electricity consumption range for global AI in 2027 as 85–134 TWh, which directly answers the question."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",FALSE,0,is_blank,is_blank,is_blank,"Doc [khan2025#0046] states that ""metrics like accuracy and F1 score are slightly lower after optimization"". Doc [khan2025#0053] notes that ""metrics such as F1 score and overall accuracy may decline slightly post-optimization"". Doc [khan2025#0027] indicates that optimizing can ""risk performance drops"" in sentiment analysis.","The cited passages explicitly report declines in accuracy and F1 after optimization, contradicting the claim that they always improved. Therefore the statement is false."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61–76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""excludes equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.""","The cited passage states that when equity is excluded, computing hardware accounts for 61–76% of the total amortized cost for the four key models, providing the requested percentage range."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,True,1,is_blank,is_blank,is_blank,"""LLM decoding is characterized by low compute‑intensity, meaning that the number of arithmetic operations per byte of memory loaded is low…This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth…Diffusion models are significantly more compute‑intensive compared to LLM decoding.""","The excerpts state that LLM decoding consumes much less power than the GPU’s maximum because it is low compute‑intensity and bandwidth‑bound, whereas diffusion models are more compute‑intensive and draw power close to the GPU’s TDP. This directly supports the claim that LLMs generally have lower power draw during inference than diffusion models."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain a clear statement indicating which GPU architecture is most energy‑efficient for generating a single classification token. Therefore, the answer cannot be determined with confidence from the supplied documents."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,is_blank,is_blank,"""time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.""","The online inference workload uses 128 input tokens plus 512 output tokens per request, totaling 640 tokens per request. With 3,000 requests, the total tokens processed are 640 × 3,000 = 1,920,000 tokens."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150,150,million packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We delivered 150 million packages via EVs.""","The document states that Amazon delivered 150 million packages via electric vehicles in Europe in 2023, giving the required value in millions."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration.""","The cited passage from ebert2024 states that the AI Act does not address GHG emissions from AI applications, indicating it does not mandate disclosure of such emissions. Therefore the statement is false."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"""up to 80% in West US""","The provided figure caption states that for the very short DenseNet 201 experiment, the Flexible Start optimization can achieve up to 80% CO₂ reduction in the West US region."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"""CO2e = 0.954pt"" (from Strubell et al., 2019)","The Strubell et al. paper cites the EPA’s average CO₂ production as 0.954 lbs per kilowatt‑hour, which directly answers the question."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450,1450,is_blank,is_blank,is_blank,"""least energy‑intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy‑intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450""","The excerpt states that image generation consumes 2.9 kWh per 1,000 inferences while text classification consumes 0.002 kWh per 1,000 inferences, yielding a ratio of 2.9 / 0.002 ≈ 1450. The document explicitly mentions a factor of over 1450, confirming this calculation."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Energy consumption should be reported at the cumulative server level (see also [4]).""","The authors explicitly state that energy consumption should be reported at the cumulative server level, balancing accuracy of measurement with practical feasibility."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents discuss Yelp sentiment analysis benchmarks or provide a comparison of traditional models’ accuracy to that of large language models, so the truth value cannot be determined from the given information."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Amazon’s AWS only covered fifty percent of its power usage with renewable energy.,"The context from Doc [schwartz2019] explicitly states that AWS covered 50 % of its power usage with renewable energy, which answers the question for 2018."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear, unambiguous statement of the percentage of Amazon's People Managers globally who identified as women in 2023. The relevant table information is garbled and does not explicitly identify the gender statistic for People Managers for that year."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""The GPU alone accounts for 74% of the total energy consumption""","The cited experiment training BERT‑base on a single TITAN X GPU reports that the GPU accounts for 74 % of total electricity consumption, directly answering the question."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5%,28.5,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""28.5% operational energy footprint reduction over the two-year time period""",The context explicitly states that Facebook’s iterative hardware‑software optimization led to a 28.5% reduction in operational energy footprint over the two‑year period from 2019 to 2021.
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any reference to McKinsey projections for data center electricity consumption in 2030, so the answer cannot be inferred from these documents."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google reports a 48% increase in GHG emissions since 2019""",The quoted sentence directly states that Google’s 2024 environmental report cites a 48% increase in greenhouse gas emissions since 2019.
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244,244,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"United States 244 17,706","The table of Amazon Renewable Energy Projects announced as of January 2024 lists 244 projects in the United States, providing the count requested."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,is_blank,is_blank,is_blank,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs""","The provided context explicitly states that serving a Llama2-70b model at BF16 precision requires two NVIDIA A100-80GB GPUs, which directly answers the question."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context excerpts contain no mention of the quantity (in kilometers) of fiber optic cable installed worldwide in 2023 to support AI workloads, so the answer cannot be determined from the documents provided."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""bsz=28"" (Fig. 13 ground truth for A100‑40GB)\n""maximum batch sizes supported for fine‑tuning Mixtral will be 28 and 35, respectively.""",The figure and accompanying text state that the ground‑truth maximum batch size for fine‑tuning Mixtral on an NVIDIA A100‑40GB GPU is 28 samples. This matches the reported value in the analytical model and experimental results.
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,"""JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.""","The quoted sentence explicitly states that JetMoE-8B reduces inference computation by approximately 70% relative to the Llama2-7B model, which directly answers the question."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800 million,800,M USD,is_blank,is_blank,"""we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.""","The quoted sentence explicitly states that the upfront hardware acquisition cost for training GPT‑4 is estimated at $800 million, answering the question directly."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,TRUE,1,is_blank,is_blank,is_blank,"""CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)"" (Doc erben2023#0076) and ""Figure 7: (A) Intra-zone performance for CV and NLP."" (Doc erben2023#0075)","The quoted per‑GPU speedup for CV is described as almost linear, and the figure referenced is explicitly labeled as intra‑zone performance, confirming that the scaling observed is within a single zone. Therefore the statement is true."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any specific numeric information about the land area in hectares occupied by new AI data centers worldwide in 2022, so a confident answer cannot be derived from them."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the seminal 2019 article by Strubell et al. which quantified the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of ... emissions [192]""","The quoted passage from Doc [luccioni2025b] directly states the carbon footprint of training BERT as 626,155 pounds of CO2e, which is the value used in the answer."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2 samples,2,samples,xia2024,https://arxiv.org/pdf/2408.04693,TABLE III shows that for the CS dataset (Hellaswag) the maximum batch size supported by dense fine‑tuning of Mixtral is 2.,"The table lists Mixtral-D (dense) maximum batch sizes per dataset. For CS (Hellaswag) the value is 2, which directly answers the question for the A40 GPU with 48 GB memory under a dense setup."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping consumes less energy.,Swapping,is_blank,is_blank,is_blank,"""when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.""","The quoted passage directly states that Swapping consistently consumes less energy than Recomputation under overload, explaining the reason as lower computation cost versus memory operations."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate‑positive applications, as well as improving upon the environmental sustainability of AI approaches themselves.""","The quotation shows that Sustainable AI was intended to encompass both climate‑positive applications and the improvement of AI’s own environmental sustainability, contradicting the claim that it was proposed only for climate‑positive use."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 gCO2eq,0.32,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"""bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries""","The cited passage from the 2024 study explicitly states that the BERT-based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 gCO2eq for every 1,000 text classification queries."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",True,1,is_blank,is_blank,is_blank,"""The MoE layer is the most time‑consuming, accounting for 85% of the overall execution time on average. ... Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine‑tuning.""","The quoted passage explicitly states that the MoE layer is a prime target for optimization to improve LLM fine‑tuning performance, confirming the statement is true."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.""","The provided document explicitly states that 2022 was the year when direct environmental disclosures peaked, followed by a decline in subsequent years."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"""with an estimated training energy of 1287 MWh [29].""; ""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity""","Both documents explicitly state that GPT‑3’s training energy was estimated at 1,287 MWh, which is the answer to the question."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MkWh,40,MkWh,li2025a,https://arxiv.org/pdf/2309.03852,"""Energy (MkWh) 1171 1066 3179 444 688 40""","The carbon‑footprint analysis table for FLM‑101B lists its total energy consumption as 40 MkWh, which directly answers the question."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention or numerical estimate of the freshwater consumption by Google's DeepMind AlphaFold servers in 2023, so the answer cannot be determined from these documents."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context documents contain projections for 2028 but do not provide any projection for 2030 regarding the public health burden of U.S. data centers. Therefore, the question cannot be answered with confidence from the given information."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,$32.7,32.7,$,xia2024,https://arxiv.org/pdf/2408.04693,"""A40 48GB 4 1.01 0.79 32.7""","The table lists the total fine‑tuning cost for a sparse Mixtral model on GSM8K using an NVIDIA A40‑48GB GPU as $32.7, which directly answers the question."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,False,0,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.""","The quoted passage states that adding compute resources reduces cost, implying it does not increase costs; therefore the statement is false."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",19 thousand grams,19,thousand grams,is_blank,is_blank,"""7k grams vs. 26k grams, for the most efficient vs. least efficient regions""","The context states that the most efficient region emitted 7k grams and the least efficient 26k grams of CO₂. The difference (range) between them is 26k – 7k = 19k grams, i.e., approximately 19 thousand grams."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",True,1,is_blank,is_blank,is_blank,"""on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment."" ""CV 7%, NLP 35%).""","The quoted passages state that intercontinental training for CV models slowed performance by 7% compared to local training, confirming the statement as true."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,training accounted for only half of the model’s overall emissions [121],"The 2023 study by Luccioni et al. reported that training represented 50 % of BLOOM’s total emissions, as indicated in the Life Cycle Assessment analysis."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10 – 50 user requests,"[10, 50]",responses,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.""","The cited passage from li2025b directly states that consuming a 500 ml bottle of water corresponds to about 10–50 medium-length user responses, so the number of user requests needed falls within that range."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",8.5%,8.5,%,dodge2022,https://arxiv.org/pdf/2206.05229,Table 13 shows the gain in percent for the Pause and Resume (P&R) optimization; for the BERT 6B parameter transformer the value is 8.5%.,"The table lists the percent emissions reduction achieved by the P&R optimization for each model. The 6B transformer row shows the highest attainable saving of 8.5%, which is the maximum potential emissions saving reported for that model in the provided documents."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA V100 32GB GPU,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Model Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64""","The table in the document lists that the 7B model requires 1 V100 32GB GPU for the bare minimum settings, confirming that a single GPU suffices for inference without compression or quantization."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.""","The cited passage from ebert2024 explicitly states that GPU-level monitoring is not recommended for overall AI energy reporting, supporting a FALSE answer."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.30 metric tons,8.3,metric tons,is_blank,is_blank,"""one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)"" (Doc [dodge2022#0065])
""emissions from the average US home for a year ... totaling 8.3 metric tons CO2 per year"" (Doc [dodge2022#0069])","Both cited documents state that the annual CO2 emissions from an average U.S. home amount to approximately 8.3 metric tons, providing a consistent estimate for the question."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,GSM8k 14.5 17.3 16.9 27.8,"Table 3 lists GSM8k scores for four models; the last column corresponds to JetMoE‑8B, which achieves a score of 27.8."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",84,84,is_blank,is_blank,is_blank,"Table III shows the maximum batch size for a sparse Mixtral model on an NVIDIA A40 (48 GB) GPU as ""Sparse(bsz=84)"" under the Forward Backward Optimizer column.","The table lists the largest batch size that can be supported for the sparse Mixtral model on the A40 GPU; the longest‑running MoE layer corresponds to this maximum, which is 84 samples."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",8943 days,8943,days,is_blank,is_blank,"""consumed 2.769 million liters of water , equivalent to about 24.5 years of water usage by a person in the United States.""","The context states the training of the OLMo series (including the 60M model trained on 1.7–5.6 trillion tokens) consumed water equivalent to 24.5 years of a single U.S. person’s usage. 24.5 years × 365 days/year = 8,943 days, rounded to the nearest day."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a specific numeric value for the total execution time (in seconds) of a sparse Mixtral model fine‑tuned on an NVIDIA A40‑48GB with a batch size of 10. No sentence or table gives this metric, so the answer cannot be determined with confidence from the given documents."
