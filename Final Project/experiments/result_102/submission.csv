id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about nitrogen oxide emission limits for data center backup generators in northern Virginia.
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for FLM-101B’s final average performance score on the Open LLM Leaderboard, so I cannot answer the question based on the available context."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the Amazon Solar Farm Maryland-CPV Backbone or its avoided CO2e emissions, so the question cannot be answered with confidence based on the available context."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a specific CO2e value for a single passenger round trip from San Francisco to New York.
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,luccioni2023,https://arxiv.org/pdf/2302.08476,"Doc [luccioni2023]: 'note that the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs).'","The question asks for the training duration of GPT‑3 with 10,000 V100 GPUs. Doc [luccioni2023] explicitly states that this duration is 14.8 days, so that is the answer."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention AWS using recycled water for cooling or specify the number of data centers in 2023.
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention premature deaths in 2030 from scope-2 pollutants of U.S. data centers, so the answer cannot be determined from the given context."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The document states that training with the growth strategy takes 21.54 days versus 76.74 days from scratch, representing a 72% reduction in time, so the answer is 72%."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as , or 3700 kg of COeq per 8x server node, equal 463 kg per GPU.","The question asks for the estimated embodied carbon emissions per GPU. Doc [morrison2025] explicitly states that the embodied emissions are 3700 kg per 8‑GPU server node, which equals 463 kg per GPU, providing the required numeric answer."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a numeric estimate of the total training budget for FLM-101B or a direct comparison to GPT-4’s amortized training cost, so the factor cannot be determined from the available context."
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: The total energy consumption of the US data centers increased by about 4 from 2010-2014, compared with the estimated 24 increase from 2005-10 and nearly 90 increase from 2000-05.","The question asks for the average increase in U.S. data center electricity consumption between 2010 and 2014. Doc [wu2021b] explicitly states that the consumption increased by about 4 (percent) over that period, providing the required value."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'This is a more than 6,750 fold improvement in processor clock speed'","The question asks for the fold increase in clock speed from the Intel 4004 to typical 2021 microprocessors. Doc [wu2021b] explicitly states a more than 6,750‑fold improvement, so that value is used as the answer."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on…'","The appendix excerpt from Doc [luccioni2025c] lists the GPU energy consumption for 1,000 queries and identifies Command-R Plus as the model with the highest value (over 3,426 Wh), so it is the answer."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not mention the JetMoE-8B model or the number of experts per MoE layer for that architecture, so I cannot answer the question based on the provided context."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the One Hundred Year Study on Artificial Intelligence or its launch year, so I cannot answer the question with confidence."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: The UN’s Global E-Waste Monitor 2024 showed that about 22 of e-waste has been shown to be formally collected and recycled,"The question asks for the percentage of e‑waste formally collected and recycled. Doc [luccioni2025a] states that about 22% of e‑waste meets this criterion, so 22% is the correct answer."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13; a full training run would take 60 days.","The question asks for the duration of a full training run. The cited passage from Doc [dodge2022] explicitly states that a full training run would take 60 days, so that is the answer."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention a hardware processor used for energy‑efficient local inference in financial sentiment classification, so the answer cannot be determined from the given context."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a clear numeric estimate of GPU Power Usage in kWh for the specified SGLang benchmarking scenario.
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific execution time for a dense BlackMamba model with a batch size of 30 fine‑tuned on an NVIDIA A40-48 GB GPU, so the answer cannot be derived from the context."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not contain a specific numeric value for the zettaFLOPs required to train the English portion of the FLM-101B model, so I cannot provide an answer based on the available context."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,models,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024#0027]: 'We sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or multi-task models, to allow comparisons both for differen.'","The question asks how many models were sampled in the Power Hungry Processing study. Doc [luccioni2024#0027] explicitly states that 88 models were sampled, so the answer is 88 models."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x,2.2,x,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.'","The question asks for the factor by which total compute exceeds the final training run. Doc [cottier2024] reports a median ratio of 2.2x, so that is the value to report."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,is_blank,is_blank,"Doc [jegham2025#0004]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity...'","The question asks for the electricity consumption of GPT-3 training in MWh. Doc [jegham2025#0004] explicitly states that it consumed 1,287 MWh, which is directly used as the answer."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention GPU utilization percentages in Facebook model experimentation workflows, so I cannot answer this question based on the available context."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons Paradox,Jevons Paradox,is_blank,is_blank,is_blank,"Doc [jegham2025#0108]: 'Although large language models consume significantly less energy, water, and carbon per task than human labor , these efficiency gains do not inherently reduce overall environmental impact. As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox , where increased efficiency drives systemic demand.'","The question asks for the economic principle that explains why AI efficiency gains may not yield net environmental benefits. The cited paper explicitly references the Jevons Paradox as the mechanism behind the rebound effect, so that is the answer."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The question asks for the wall‑clock time using the growth strategy. Doc [li2025a] explicitly states that the total time cost for training FLM-101B under the growth schedule is 21.54 days, so that is the answer."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",6.6 billion cubic meters,6.6,billion cubic meters,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: Li et al.  analyzed GPT-3’s freshwater consumption, estimating over 5 million liters used during training and projecting that AI-related withdrawals could reach 6.6 trillion lit","The question asks for the projected global AI water withdrawal by 2027. Doc [jegham2025] states that AI-related withdrawals could reach 6.6 trillion liters, which is 6.6 billion cubic meters, matching the requested unit."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'AI Energy Score , a project aiming to establish a unified approach for comparing the inference efficiency of AI models.'
Doc [luccioni2025c#0055]: 'the AI Energy Score project provides a standardized methodology for comparing models across different tasks.'","The question asks for the name of the collaborative project. Both Doc [luccioni2025c] and Doc [luccioni2025c#0055] explicitly state that the project is called ""AI Energy Score"", which is the answer."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the age of the field of Artificial Intelligence in 2025, so I cannot answer this question based on the available context."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain enough information to determine whether Red AI is on the decline in relation to diminishing returns of increased cost.
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any specific information about the number of AWS data center regions that achieved 100% renewable energy matching in 2023.
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give enough information to determine the execution time for the longest kernel of the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48 GB GPU.
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: 'we estimate the total energy consumption to train this model to completion would be approximately  MWh, or 103,500 kWh  almost 2800 times more than training the BERT-small model!'","The context provides a direct estimate of 103,500 kWh for the full training run of a 6.1 billion‑parameter transformer. Converting 103,500 kWh to megawatt‑hours gives 103.5 MWh, which is the requested value."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",training a large language model (LLM),Training a large language model (LLM),is_blank,is_blank,is_blank,"Doc [luccioni2025b#0033]: 'The first research to formally address the environmental impacts of training AI models was the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of emissions.'","The question asks for the AI process underlying the five cars estimate. The cited 2019 study by Strubell et al. measured emissions from training a large language model (LLM), which is the specific and infrequently performed process referenced in the context."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019#0002]: 'The amount of compute used to train deep learning models has increased 300,000x in 6 years.'","The documents report a 300,000x increase in compute from 2012 to 2018, not 200,000x, so the statement is false."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4 percent,6.4,percent,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'In addition, the global carbon emissions for 2020 dropped by 6.4 with vehicle transportation in the US accounting for a portion of the global reduction.'","The question asks for the reported drop in global carbon emissions in 2020. Doc [wu2021b] states the emissions dropped by 6.4, which is interpreted as a 6.4% reduction, so the answer is 6.4 percent."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Amazon's energy storage capacity, so I cannot answer this question based on the available context."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about the percentage decrease in energy use for Llama 3.1 8B when targeting a TPOT of 100 ms versus minimizing latency.
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific percentage of activated parameters for the Switch Transformer, so I cannot determine the answer with confidence."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a clear, separate figure for the pre‑training energy consumption of the BLOOM model, only a combined training figure that includes fine‑tuning, so the answer cannot be determined with confidence."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the percentage of AI inference workloads in Asia powered by coal in 2023.
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,transatlantic flights,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: ual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.",The question asks for the number of transatlantic flights comparable to GPT‑4o’s annual emissions. Doc [jegham2025] explicitly states that the emissions are comparable to the cumulative emissions of approximately 272 transatlantic flights.
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts (top‑2),2,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 'Both models incorporate eight experts in their MoE layers. For dense fine‑tuning, all experts are activated, whereas for sparse fine‑tuning, only the top two experts are selected for each token.'","The question asks how many experts are activated per token in each JetMoE‑8B layer. The cited passage explicitly states that for sparse fine‑tuning, only the top two experts are selected for each token, so the answer is 2."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Amazon electric delivery vans added in 2022 or 2023, so the answer cannot be determined from the given context."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information regarding GPU utilization or how increasing it to 80% would affect the overall carbon footprint, so the answer cannot be determined with confidence."
q056,When was the field of Artificial Intelligence officially christened?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention when the field of Artificial Intelligence was officially christened, so I cannot answer this question based on the available context."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,8 samples,8,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 'Based on our experimental study on NVIDIA A40 GPU with 48GB memory, we empirically find and report the maximum batch size supported by different model and dataset combinations in Table. Maximum batch size supported by LLM fine‑tuning; D: dense and S:sparse. -D  -S  -D  -S   2  8  6  20   1  3  2  8'","The question asks for the maximum batch size for sparse fine‑tuning of BlackMamba on GSM8K with an NVIDIA A40 GPU. In Doc [xia2024] the table lists the maximum batch sizes, showing 8 for the sparse configuration of BlackMamba GSM8K, so the answer is 8 samples."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about Facebook's RM2 model or its quantization percentage from 32-bit to 16-bit.
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific PUE value for AI‑dedicated data centers in 2023.
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The question states that operational impacts do not include GHG emissions from servers and cooling. Doc [morrison2025] explicitly says operational impacts do include those emissions, so the statement is false."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a numeric energy consumption for the o3 model for a long prompt, so the answer cannot be determined from the available context."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a clear numeric value for the GHG emissions (in tCO2e) associated with pre‑training the Llama 7B model.
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",1 billion parameters,1000000000,parameters,morrison2025,https://arxiv.org/pdf/2503.05804,Doc [morrison2025]: We additionally evaluate a mixture-of-experts (MoE) model with 1 billion active and 7 billion total parameters,"The question asks how many of JetMoE-8B's parameters are activated per token. Doc [morrison2025] states that the MoE model has 1 billion active parameters, which corresponds to the number activated for each token during inference."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","25,000 USD",25000,USD,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of 25,000.'","The question asks for the estimated cost of training Grover on 256 TPU chips for two weeks. Doc [schwartz2019] explicitly states that this training incurred an estimated cost of 25,000 USD, so that is the answer."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,1.29 L/kWh,1.29,L/kWh,is_blank,is_blank,"Doc [morrison2025#0035]: 'For Jupiter, the cluster used to train all models but the 13B, we assume a total water usage effectiveness () of 1.29 liters per kWh.'","The question asks for the average WUE of Google's AI‑dedicated data centers in 2024. Doc [morrison2025#0035] provides the WUE for the Jupiter cluster, used for AI training, as 1.29 L/kWh, which is the only explicit WUE value in the context."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53 %,53,%,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'The optimizer stage in fine-tuning takes a considerable portion of the running time (up to 53 when conducting sparse fine-tuning with batch size = 1)',"The question asks for the optimizer stage percentage for BlackMamba sparse fine-tuning on an NVIDIA A40-48GB GPU with batch size 1. Doc [xia2024] explicitly states the optimizer stage takes up to 53% of the running time, which directly answers the question."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the number of members of the 2015 Study Panel, so I cannot answer the question based on the available context."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'The average data center PUE in 2023 was 1.58 globally',"The question asks for the average global data center PUE in 2023, and Doc [ebert2024] explicitly states that value as 1.58, so that is the answer."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49 %,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: Gemini Ultra has the highest fraction of RD staff cost at 49, but we expect this is unusually high among frontier models.",The question asks for the R&D staff share of Gemini Ultra’s development cost. Doc [cottier2024] states that Gemini Ultra’s RD staff cost fraction is 49 %. This directly answers the question with the given percentage.
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity.'","The statement matches the number reported in Doc [wu2021b], which explicitly states that roughly 770 million people lack stable electricity access."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a clear comparison of energy consumption between sparsely activated DNNs and large dense DNNs, nor do they state a 1/10th energy ratio or accuracy equivalence, so the claim cannot be verified."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: 'The reasoning behind the 5-10 reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes.',"The question asks whether the 5‑10% global GHG reduction claim is supported by clear, publicly available calculations and sound scientific grounding. Doc [luccioni2025c] states that the reasoning is unclear and that the calculations are not detailed beyond an explanation of BCG’s experience, indicating a lack of clear, publicly available calculations or rigorous scientific grounding. Therefore the claim is not supported, so the answer is False."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the documents mention wind turbines or Microsoft contracts for Azure AI clusters in 2023, so the answer cannot be determined from the provided context."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the Study Panel from the 100 Year Study on AI or their concerns, so the statement cannot be verified from the given context."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the Earth–Sun distance, so I cannot answer this question based on the available context."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the CO2 emissions from OpenAI’s API requests in January 2024, so the answer cannot be determined from the given context."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the factor by which Facebook’s AI training infrastructure capacity increased between 2019 and 2021.
q080,True or False: The AlphaGo program defeated the human Go champion.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain information about AlphaGo defeating a human Go champion, so the truth value cannot be determined from the context."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about Flan‑T5‑xxl or its energy usage per 1,000 queries, so the daily energy consumption cannot be derived from the context alone."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the H100 GPU hours required for the JetMoE-8B alignment process, including dSFT and dDPO fine-tuning."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'different models with a similar number of parameters often perform different amounts of work.' Doc [schwartz2019#0076]: 'Second, the number of model parameters does not tell the whole story: AlexNet actually has more parameters than ResNet, but dramatically less work and lower accuracy.'","The documents show that parameter count is only correlated with inference energy and that models with similar or even higher parameter counts can use less energy, so the claim that more parameters always mean more inference energy is false."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e, over 40x the five cars estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Meta reports that their Llama 3 family of models emitted 11,390 tons COe or over 40x the five cars estimate.'","The question asks for the pre‑training GHG emissions of Meta’s Llama 3 family and how they compare to the five‑cars estimate. Doc [luccioni2025c] explicitly states that the Llama 3 family emitted 11,390 tCO2e and that this is over 40 times the five‑cars estimate, so the answer reflects both the value and the comparison."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh – 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)'","The question asks for the range of GPU energy usage for 1,000 inference queries. Doc [luccioni2025c] states that the energy spans from 0.06 Wh to over 3,426 Wh, so this is the reported range."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Dynamic batching,Dynamic batching,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization while maintaining low latency.',"The question asks for the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones. Doc [jegham2025] explicitly states that dynamic batching is used to optimize GPU utilization, which aligns with the described behavior."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the percentage of a client device's total carbon footprint attributable to manufacturing, so I cannot answer this question based on the available context."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams CO2eq per 1,000 inferences",1594,grams CO2eq,is_blank,is_blank,"Doc [luccioni2024#0048]: 'the most carbon-intensive image generation model () generates 1,594 grams of  for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle,'","The 2024 study identifies the most carbon-intensive image generation model—stable-diffusion-xl-base-1.0—as emitting 1,594 grams CO2eq per 1,000 inferences, as stated in Doc [luccioni2024#0048]."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'ergy per token between max generation length 512 and 1024. For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.'","The question asks for the energy per token at a maximum generation length of 512 tokens. Doc [samsi2024] reports that it takes about 3‑4 Joules per output token at length 512, so the answer is 3‑4 Joules."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,social transparency,social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems.'","The question asks for the term proposed for expanding transparency to include socio-technical and environmental aspects. Doc [luccioni2025b] explicitly defines this term as ""social transparency"", so that is the answer."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a clear statement of the name of the LLM inference system developed by Chen et al. in 2025 that uses model-attention disaggregation.
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a clear statement of the decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents, so I cannot answer the question based on the available context."
q094,What is the total number of parameters in the JetMoE-8B model?,7 billion parameters,7000000000.0,parameters,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'We additionally evaluate a mixture-of-experts (MoE) model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens.'","The question asks for the total parameter count of the JetMoE-8B model. Doc [morrison2025] explicitly states that the MoE model has 7 billion total parameters, which matches the JetMoE-8B designation."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34 increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20 uptick in the same period.'","The context states that Google’s water consumption increased by a 20% uptick from 2021 to 2022, directly answering the question with a 20% increase."
q093,How many parameters does the largest T5 model have?,11 billion parameters,11,billion parameters,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'Flan-T5-XXL, which is the biggest model in terms of parameter count in the multi-purpose models that we tested (11 billion)'","The question asks for the parameter count of the largest T5 model. Doc [luccioni2024] explicitly states that Flan‑T5‑XXL, the largest T5 model in the study, has 11 billion parameters, so that is the answer."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon intensity,Carbon intensity,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"Doc [luccioni2023]: 'The unit of measurement typically used for quantifying and comparing carbon emissions is . This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of  emitted per kilowatt hour of electricity generated ()'","The question asks for the metric name defined as CO₂ emissions per unit of electricity consumed. In Doc [luccioni2023] the metric is described as the unit used to compare emissions per kWh, which is known as the carbon intensity metric."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b#0115]: 'We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)' ; Doc [luccioni2025b]: 'There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.'","The question asks whether researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. The passages from Doc [luccioni2025b#0115] and Doc [luccioni2025b] state that researchers do not believe such a universal approach exists and that there is no one-size-fits-all solution, so the answer is False."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Amazon’s replenishment projects or the amount of water returned to communities in 2023, so the answer cannot be determined from the context."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",51.3%,51.3,%,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3.',"The passage states that all three training stages, including the final FLM‑101B stage, achieved a FLOPs utilization of at least 51.3%, so the final stage’s utilization percentage is 51.3%."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,40M USD,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Currently, GPT-4 has the largest amortized hardware and energy cost, at 40M.'","The question asks for the estimated amortized training cost of GPT‑4. Doc [cottier2024] explicitly states that the amortized hardware and energy cost for GPT‑4 is 40 million, so the answer is 40 M USD."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the fraction of local throughput achieved for NLP when training was spread across four continents versus remaining local.
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,Doc [wu2021b]: 'the PUE of Facebook datacenters is 1.10 (2020)',"The question asks for Facebook's data center PUE. Doc [wu2021b] explicitly states that the PUE of Facebook datacenters is 1.10, so that is the answer."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a clear numeric factor for how much full-stack optimization can reduce the operational carbon footprint compared to a CPU server baseline.
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,"Doc [rubei2025]: 'Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts.' 
Doc [rubei2025]: 'We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment.'","The question asks whether custom tags in source code completion tasks can reduce energy consumption. Doc [rubei2025] explicitly states that using specific tags reduces energy consumption during inference on the CodeXGLUE code generation benchmark, supporting a True answer."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems.'","The question asks for the acronym of the Finnish project. The passage explicitly names the project as ETAIROS, providing the required acronym."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not contain a clear statement of the gross carbon intensity of energy for the U.S. average mix in 2021, so I cannot answer this question with confidence."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.75%,26.75,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: Given a SLO requirement of 100 TPS,  selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of 2.13. On the other hand, both Max-Performance and  without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, bu","The context gives the costs for the InferSave-selected instance (g4dn.xlarge, 2.13) and the Max-Performance-selected instance (g6e.xlarge, 2.699) under a 100 TPS offline workload. The percentage increase is (2.699-2.13)/2.13 ≈ 0.2675, i.e., 26.75%, which is the answer."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3700000,GPUs,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: 'NVIDIA shipped 3.7 million GPUs in 2024',"The question asks for the number of GPUs shipped by NVIDIA in 2024. Doc [luccioni2025a] explicitly states that NVIDIA shipped 3.7 million GPUs that year, so the answer is 3.7 million GPUs."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the EPA's primary standard for PM2.5, so the answer cannot be determined from the context."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: components of amortized hardware CapEx + energy in , we find that on average, 44 goes toward AI accelerator chips.","The passage from Doc [cottier2024] explicitly states that 44% of the total amortized hardware CapEx + energy cost is attributed to AI accelerator chips, which directly answers the question."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO as a single Amazon Kindle device.'","The question asks how many print books equal the CO2 impact of one Kindle. Doc [luccioni2025a] explicitly states that 115 books produce the same amount of CO, which is used as the answer."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024]: 'approximately 200x per-household health costs',"The question asks for the factor by which the per-household health burden in the most affected, economically‑disadvantaged communities exceeds that in less‑impacted communities. Doc [han2024] states that the ratio of the highest to lowest county‑level per‑household health cost reaches approximately 200, providing the required factor."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the energy consumption of the DS Llama 70B model on the FKTG dataset, so the answer cannot be determined from the given context."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention German public administration texts or specify which sentence‑embedding model achieved the highest accuracy for that task.
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Meena or provide energy consumption data for Meena training runs, so the question cannot be answered with the given context."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,jegham2025;luccioni2025a,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2501.16548,"Doc [jegham2025]: '..., a phenomenon aligned with the Jevons Paradox , where increased efficiency drives systemic demand.'","The question asks for the phenomenon where efficiency gains lead to higher usage and resource consumption. The context explicitly names this as the Jevons Paradox, so that is the answer."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the 2022 paper by Dodge et al. or the number of parameters in the model they analyzed.
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors.'","The question asks whether the AI Act requires that risk assessments for GPAI models with systemic risk include environmental risks. The cited passage states that while the Act requires risk assessment and mitigation, it does not mandate inclusion of environmental risks, indicating the statement is false."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents identify a specific West Virginia county with the highest projected per-household health cost, so I cannot answer the question based on the available context."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process'","The question asks for the combined training and fine‑tuning energy for BLOOMz‑7B. Doc [ebert2024] gives training energy of 51,686 kWh and fine‑tuning energy of 7,571 kWh. Summing these gives 59,257 kWh, which is reported in the Power Hungry Processing study."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 pounds CO2e",36156,lbs CO2e,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'American life, avg, 1 year  36,156'","The question asks for the CO2-equivalent emissions for an average American life in one year. Doc [strubell2019] lists ""American life, avg, 1 year  36,156"" lbs of COe, so 36,156 lbs CO2e is the answer."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of .'","The question asks for the total energy consumed for all model experimentation and evaluation in the 2024 study. The passage from Doc [luccioni2024] explicitly states that 754.66 kWh of energy was used, providing the required value."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,30M USD,30.0,M USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'the most expensive publicly announced model to date are OpenAI's GPT-4 at 40M and Google's Gemini Ultra at 30M.',"The question requests the estimated amortized training cost for Google's Gemini Ultra. Doc [cottier2024] explicitly states that Gemini Ultra’s cost is 30M, which is the value used as the answer."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'We train three models,  with 16B, 51B, and 101B parameters, respectively.'","The question asks for the parameter count of the final FLM-101B model. Doc [li2025a] explicitly states that FLM-101B has 101B parameters, so the answer is 101B."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific figure for freshwater consumption by Meta's Llama 3 inference serving clusters in 2024, so I cannot answer the question based on the available context."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals, so I cannot answer this question based on the available context."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'In terms of token usage, 84 of LLM usage is through models with no disclosure, 14 for indirectly disclosed models, and only 2 for models with direct disclosure.'","The question asks for the percentage of token usage through undisclosed models. Doc [luccioni2025c] states that 84% of token usage is through models with no disclosure, so 84% is the answer."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",1.35 kWh,1.35,kWh,is_blank,is_blank,Doc [luccioni2024#0040]: 'image generation (1.35 kWh)',"The question asks for the average energy consumption for 1,000 image generation inferences. Doc [luccioni2024#0040] states that image generation consumes 1.35 kWh for 1,000 inferences, so that is the answer."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any mention of a dataset name for German nuclear waste site objection texts classified in the experiments.
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give enough information to determine the total avoided emissions from pruning and quantizing LLMs in 2023.
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a direct link between 3.2 tCO2e for the Evolved Transformer NAS and the number of passengers on a San Francisco–New York round trip, so the answer cannot be determined from the available context."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a specific price per hour for an NVIDIA H20, so I cannot answer this question with confidence."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 A100 GPU,1,GPU,is_blank,is_blank,Doc [samsi2024#0016]: '13B 2 64 1 64',"The table in Doc [samsi2024#0016] lists the bare‑minimum GPU counts for LLaMA models. For the 13B model, the second column (A100 80GB) shows a count of 1, indicating that only one A100 GPU is required for inference without compression or quantization."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'health cost of about 6.7 billion, or 47.5 per household, in 2023. This is equivalent to approximately 44 of the data centers' total electricity cost.'","The question asks for the percentage of the data centers' total electricity cost represented by their public health cost in 2023. The document states that this cost is approximately 44% of the total electricity cost, so 44% is the answer."
q149,How many tokens were used to pre-train the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE-8B model or its training token count, so the answer cannot be determined from the available context."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: 'we estimate it would have emitted 21 to 78 metric tons of (depending on the region it was run in).',"The question asks for the estimated emissions range of a complete 6.1 billion‑parameter transformer training run. Doc [dodge2022] explicitly states that a full run would emit 21 to 78 metric tons, which directly answers the question."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: A10G results in a 24 cost saving over A100-only and 31 over A10G-only.,"The question asks for the percentage cost savings achieved when blending A100 and A10G GPUs compared to an A100-only strategy. Doc [griggs2024] explicitly states a 24 cost saving over A100-only, which is interpreted as a 24% cost saving, so the answer is 24%."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 A100 80GB GPU,1,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024#0016]: '7B   1  64  1  64' and Doc [samsi2024]: 'the 7B model was run on a single GPU',"The table in Doc [samsi2024#0016] shows that the LLaMA‑7B model requires one A100 80GB GPU for inference, and the narrative in Doc [samsi2024] confirms that the 7B model was run on a single GPU. Thus the bare minimum is one A100 80GB GPU."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'In fact, most carbon footprint analyses gather the information manually by writing to authors.'","The question states that most analyses gather information automatically without contacting authors, but Doc [luccioni2025b] explicitly says that most analyses gather the information manually by writing to authors, so the statement is false."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the JetMoE project’s training budget or GPU hours, so the cost per H100 GPU-hour cannot be determined from the context."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","103,593 kWh of training energy; about 103,593 BLOOMz-7B inferences to match.","[103593, 103593]",kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [dodge2022#0049]: 'we estimate a full training run would consume approximately 103,593 kWh.'; Doc [luccioni2024]: '...to 1.0 × kWh for the biggest one, BLOOMz-7B.'","The question asks for the full training energy of a 6.1B parameter model and the number of BLOOMz-7B inferences needed to equal that energy. Doc [dodge2022#0049] provides the training energy of ~103,593 kWh. Doc [luccioni2024] gives the per‑inference energy for BLOOMz-7B as ~1.0 kWh. Dividing 103,593 kWh by 1.0 kWh per inference yields ~103,593 inferences, matching the training energy cost."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",120%,120,%,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024]: 'resulting in a health cost that even exceeds 120 of the training electricity cost.',"The question asks for the health cost as a percentage of the electricity cost when training a Llama‑3.1 model in Altoona, Iowa. Doc [han2024] states that the health cost even exceeds 120 of the training electricity cost, which indicates 120 %. Therefore the answer is 120 %."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the Standing Committee of the One Hundred Year Study or the formation frequency of a Study Panel.
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain clear information about the percentage of Amazon's U.S. workforce identified as men in 2023.
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give Apple’s water footprint details or the share attributed to its supply chain.
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,TRUE,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Doc [khan2025]: 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45 post quantization, making them particularly suitable for resource‑constrained environments.'","The question asks whether the described deployment techniques achieve up to a 45% reduction in carbon emissions after quantization. Doc [khan2025] explicitly states that quantization and local inference reduce carbon emissions by up to 45, which supports the claim."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers'","The question asks for the number of answers collected after contacting over 500 authors. Both Doc [luccioni2025b] and Doc [luccioni2025b#0064] state that only 95 answers were obtained, so 95 is the correct number."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information on the total execution time for a sparse BlackMamba model fine‑tuned on an NVIDIA A40‑48GB with batch size 84.
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,water withdrawal,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'WUE can be computed based on either water withdrawal (the total volume drawn from natural or municipal sources) or water consumption...',"The question asks for the term describing freshwater taken from ground or surface sources. Doc [jegham2025] explicitly defines ""water withdrawal"" as the total volume drawn from natural or municipal sources, matching the description. Thus the term is ""Water withdrawal""."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about a metric introduced to assess the ratio of computation to communication time when scaling distributed training across continents.
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give enough information to determine whether IBM's Watson beat human contenders in Jeopardy.
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 connected devices,25,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.'","The question asks for the average number of connected devices per U.S. household in 2021. Doc [wu2021b] explicitly states that households have an average of 25 connected devices, providing the required value."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not mention the number of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024.
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year'","The question asks how many times more emissions a single deal could produce. The context states that the deal could add up to 640 percent more emissions than the yearly removal targets, which translates to 6.4 times more emissions."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8–3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)'","The question asks for the range of energy consumption to pre-train an LLM in MWh. Doc [luccioni2025c] explicitly states that the pre‑training energy ranges from 0.8 MWh to 3,500 MWh, so that is the answer."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: ... one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water.,"The question asks how many GPT‑3 queries consume about half a liter of water. Doc [luccioni2025a] explicitly states that 10–50 queries on GPT‑3 consume approximately half a liter of water, so the answer is 10–50 queries."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the JetMoE-8B-Chat model or its MT-Bench score.
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific percentage for deployment cost reduction of Mélange in conversational chat settings, so the answer cannot be determined with confidence."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the percentage of the ML workload that is inference processing by NVIDIA in 2019, so the answer cannot be determined from the available context."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg CO2e,178.97,kg CO2e,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of .'","The question asks for the total CO2 equivalent emissions in the 2024 Power Hungry Processing study. Doc [luccioni2024] states that the entire study emitted 178.97 kg of CO₂e, so that is the required value."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the throughput of a dense Mixtral-CS-A100-40GB at batch size 1, so I cannot answer."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,luccioni2025c;han2024,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2412.06288,"Doc [luccioni2025c]: 'training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.'","The question asks how many Los Angeles–New York City round‑trip car trips the air pollution from training a Llama‑3.1 model equals. Doc [luccioni2025c] explicitly states it equals more than 10,000 round trips, which is the value used in the answer."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not include any information relating BLEU score improvements to model size increases for GPT-3-based translation tasks, so the question cannot be answered with confidence based on the available context."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'GPT-4o consumes around 2.875Wh while GPT-4omini's consumption is slightly higher at 3.098Wh',"The provided document states that GPT-4o consumes 2.875 Wh per query whereas GPT‑4o mini consumes 3.098 Wh, showing that the mini version uses more energy, not less."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10–50 completions,"[10, 50]",is_blank,is_blank,is_blank,Doc [luccioni2025a#0022]: 'one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water.',"The context states that about 0.5 L of water is required for 10–50 GPT-3 queries. A medium‑length completion corresponds to one query, so 0.5 L of water can support between 10 and 50 such completions."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: peaked in 2022, with 10 of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The passage from Doc [luccioni2025c] states that after the 2022 peak, the trend reversed and direct environmental disclosures dramatically decreased, contradicting the claim that the trend continued to increase."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1,billion dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'our findings suggest that if the current trend of 2.4x per year growth continues, then the amortized cost of frontier training runs will exceed one billion dollars by 2027.'","The question asks for the cost threshold the largest training runs will exceed by 2027. Doc [cottier2024] explicitly states that the cost will exceed one billion dollars, so the answer is 1 billion dollars."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement of the monthly on-demand rental cost for serving Llama‑2‑70B with 2 NVIDIA A100 GPUs, so I cannot compute an hourly cost from the given information."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'typically higher than the actual consumption, as GPU utilization rarely reaches its theoretical peak due to other resource constraints.' Doc [ebert2024]: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.'","The question asks whether estimating GPU energy consumption based on TDP is reliable and accurate. Doc [ebert2024] states that TDP usually overestimates actual consumption and advises against using GPU-level power tracking for overall energy measurements. Therefore the method is not reliable or accurate, so the answer is FALSE."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a specific total floating point operation count for GPT-3 as published by OpenAI.
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the emissions for training a Transformer model with NAS (626,155 lbs) but do not give the emissions-to-driving-distance ratio needed to convert this to miles, so the question cannot be answered from the provided context."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not contain a specific figure for the H100 GPU hours used for pre‑training the JetMoE‑8B model, so the answer cannot be determined from the context."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","606,096,000 MWh",606096000,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'We can see that the amount of energy required per inference varies from 5.4 ×  for the smallest model, BLOOMz-560M to 1.0 ×  kWh for the biggest one, BLOOMz-7B.'","The passage indicates BLOOMz-7B consumes 1.0 kWh per inference. With 606,096 downloads each generating 1,000,000 inferences, the total inferences are 606,096,000,000. Multiplying by 1.0 kWh per inference gives 606,096,000,000 kWh, which equals 606,096,000 MWh."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide throughput data for the FLM-101B training stages but do not give the duration of the final 101B stage or the total tokens processed in that stage, so the total computational work cannot be determined from the available information."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,"more than 700,000 liters",700000,liters,is_blank,is_blank,Doc [jegham2025#0005]: 'more than 700 kiloliters (kL) of water for cooling alone',"The passage explicitly states that more than 700 kiloliters of water were used for cooling, which converts to more than 700,000 liters, answering the question for GPT‑4 training."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The question asks for the GPU hours used to train FAIR's RoBERTa on 160GB of text. Doc [schwartz2019] explicitly states that this training required around 25,000 GPU hours, which provides the required numeric answer."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 A800 GPUs,192,A800 GPUs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8×80G) servers.',"The context states that 24 servers are used and each server has 8 A800 GPUs, so 24 × 8 = 192 A800 GPUs in total."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,56.4%,56.4,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Doc [schwartz2019]: 'AlexNet 61.1 0.7 56.4 2012',"The table in Doc [schwartz2019] lists AlexNet’s top‑1 ImageNet accuracy as 56.4 %, so that is the requested value."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give a specific numeric value for the normalized on-demand hourly price of an H100 GPU in the Griggs et al. (2024) evaluation.
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the water consumption per ChatGPT user session in 2023, so I cannot answer this question based on the available context."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 V100 GPUs,8,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024#0016]: 'the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.'","The question asks for the bare minimum number of V100 GPUs for LLaMA‑65B inference. Both Doc [samsi2024#0016] and Doc [samsi2024] state that 8 V100 GPUs are used for balanced sharding of the 65B model, indicating the required minimum without compression or quantization."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4 A100 GPUs,4,A100 GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'the 65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).',"The question asks for the bare minimum number of 80GB A100 GPUs needed for LLaMA‑65B inference. Doc [samsi2024] explicitly states that the 65B model was run on four 80GB A100 GPUs, indicating that four is the minimum required without compression or quantization."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,homes,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes'","The question asks how many U.S. homes the annual electricity use of 700 million daily GPT‑4o queries would match. Doc [jegham2025] explicitly states that scaling a 0.42 Wh query to 700 million per day results in electricity comparable to 35,000 U.S. homes, so that is the answer."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a specific numeric value for the metric tons of CO2e avoided by Amazon’s on‑site solar energy systems compared to non‑renewable sources.
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34% increase,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34 increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20 uptick in the same period.'","The question asks for the percentage increase reported by Microsoft. Doc [luccioni2025a] states a 34 increase in global water consumption between 2021 and 2022, which is interpreted as a 34% increase, matching the requested value."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,SGLang,SGLang,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: '...following default parameters defined in SGLang...','...inference time optimizations such as quantization; perform generation with different decoding algorithms; and/or deploy to and run inference on edge devices, sometimes even without GPUs.'","The question asks which framework was used to deploy large language models across multiple GPUs and nodes. Doc [morrison2025] explicitly states that deployment settings use the framework SGLang, indicating it is the chosen framework for multi‑GPU, multi‑node inference."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Yelp sentiment analysis benchmarks or compare traditional models to large language models in that context, so the answer cannot be determined from the given information."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion GPT-4o queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'yielding a total of approximately 772 billion GPT-4o queries in 2025',"The question asks for the estimated total number of GPT-4o queries in 2025. Doc [jegham2025] explicitly states that the total is approximately 772 billion queries, so that value is used for the answer."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the emissions estimate for NAS on a Transformer model (284,019 kg CO2e) and compare it to five car lifetimes, but they do not give a direct equivalence to average American lifetimes. Therefore the question cannot be answered confidently from the provided context."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the energy consumption increase factor for deploying Llama 3.1 70B on two nodes versus one.
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE-8B model or its final average score on the OpenLLM Leaderboard, so the answer cannot be determined from the context."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the PUE for Google's Iowa datacenter when the Evolved Transformer was run, so we cannot answer."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,True,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).'","The AI Act, as described in Doc [ebert2024], specifies that open‑source general‑purpose AI models are largely exempt from transparency (including energy consumption reporting) unless they pose systemic risk, matching the statement in the question."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: For these models, we find that RD staff costs including equity are between 29 and 49 of the total amortized cost.","The question asks for the percentage range of R&D staff costs (including equity) relative to total amortized cost for the four models. Doc [cottier2024] explicitly states that this range is 29 to 49 percent, so that is the answer."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: '53 of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search',"The question asks for the percentage of the 100 analyzed news articles that cited the contested 3‑Wh/10‑times‑Google‑search estimate. Doc [luccioni2025c] states that 53 of the 100 articles did so, yielding 53%. This matches the requested percentage."
q213,Which software package was used to measure energy consumption during inference runs?,Code Carbon,Code Carbon,is_blank,luccioni2024;morrison2025,https://arxiv.org/pdf/2311.16863;https://arxiv.org/pdf/2503.05804,"Doc [luccioni2024]: 'we ran all of our experiments on a node of 8 NVIDIA A100-SXM4-80GB GPUs hosted on Amazon Web Services, and used the Code Carbon package to measure both the energy consumed and the carbon emitted during inference.' Doc [morrison2025]: 'In our inference experiments, we measure cumulative energy consumption using CodeCarbon tracking.'","The question asks for the software package used to monitor energy consumption during inference. Both Doc [luccioni2024] and Doc [morrison2025] explicitly state that the Code Carbon package (CodeCarbon tracking) was used, so the answer is Code Carbon."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,Doc [kim2025]: proposes the Compute Time Calibration Function (CTCF) to accurately determine the optimal instance based on user requirements.,"The question asks for the function that adjusts for discrepancies between theoretical and actual GPU performance. Doc [kim2025] explicitly names this function as the Compute Time Calibration Function (CTCF), which is used to improve instance selection accuracy."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a US national average PUE value for 2020, so the answer cannot be determined from the available context."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'Figures  and  show energy metrics in terms of responses from the 65B model. Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure ) to 1024 (Figure ) does not induce a clear or significant effect in inference energy cost'","The question asks whether more GPU shards raise the energy cost per response for LLaMA‑65B. The cited passage from Doc [samsi2024] explicitly states that increasing shards tends to increase the energy costs of inference per response, so the answer is True."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).'","The question asks whether open‑source general‑purpose AI models must report energy consumption under current EU rules. The cited passage states that OS GPAI models are largely excluded from transparency requirements, indicating they are not required to report such data. Therefore the statement is false."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific count of AI training runs conducted globally on renewable-only power in 2022, so I cannot answer this question with confidence."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain an explicit total execution time in seconds for a sparse Mixtral model with batch size 1 on an NVIDIA A40-48 GB GPU.
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific numeric value for the KV Cache size for the OPT-2.7B model at a batch size of 32, so I cannot answer this question based on the available context."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30 of all PPAs purchased by corporations worldwide, changing the scope and extent of the mechanism as a whole.'","The question asks for the percentage of corporate PPAs attributed to Amazon, Microsoft, Meta, and Google in 2020. The cited passage from Doc [luccioni2025a] states that they accounted for almost 30 of all PPAs, which indicates approximately 30%. Therefore the answer is 30%."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,14.8,14.8,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: GPT-4.1 nano ... 0.827 ± 0.094  ... o3 ... 12.222 ± 1.082,"The table in Doc [jegham2025] lists the mean energy consumption for a long prompt: GPT-4.1 nano uses 0.827 Wh while o3 uses 12.222 Wh. Dividing 12.222 by 0.827 gives a factor of approximately 14.8, which is the factor by which o3’s energy consumption exceeds GPT-4.1 nano’s for a long prompt."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific numeric estimate for the pre‑training emissions of FLM‑101B.
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give enough information to determine this quantity.
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention a storage service used to shard and stream datasets for spot VMs that could terminate at any time.
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Doc [khan2025]: 'We use Ollama for local AI model deployment, which ensures data privacy by processing entirely on-device.'","The question asks for the open‑source tool used to apply 4‑bit quantization and enable local deployment in the financial sentiment case study. Doc [khan2025] states that Ollama was used for local AI model deployment, and earlier it indicates that 4‑bit quantization was applied via an open‑source platform, implying the same tool. Hence the tool is Ollama."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a statement specifying the percentage range of cost reductions achieved by Mélange for short-context workloads (Arena dataset) with a 120 ms SLO compared to single‑GPU baselines.
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,False,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'Importantly, the health costs are unevenly distributed across counties and communities, particularly affecting low-income counties that could experience approximately 200x per-household health costs than others.' Doc [han2024]: 'The results reveal a highly disproportionate distribution of health impacts across counties, with low-income communities particularly affected. The ratio of the highest to lowest county-level per-household health cost reaches approximately 200.' Doc [han2024#0016]: 'Table further identifies the top-10 counties with the highest total health costs, illustrating how local population density and proximity to power generation infrastructure combine to amplify public health risks in specific communities.' Doc [han2024#0008]: 'The ratio of the highest to lowest county-level per-household health cost reaches approximately 200.'","The question asks whether public health costs of AI are evenly distributed across U.S. communities. The cited passages state that health costs are unevenly and highly disproportionate, with ratios up to 200 and low-income counties bearing the brunt. Thus the claim of even distribution is false."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 . The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been","The question asks which senator introduced the AI Environmental Impacts Act bill in February 2024. Doc [ebert2024] explicitly names Senator Edward J. Markey (D-MA) as the introducer, so that is the answer."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",6.7 billion USD,6.7,billion USD,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about 6.7 billion, or 47.5 per household, in 2023'","The question asks for the total public health cost of U.S. data centers in 2023 based on the average attribution method. The cited passage explicitly states that this cost is about 6.7 billion USD, so that is the answer."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,3.7 years,3.7,years,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.'","The question asks for the estimated average GPU lifetime before retirement in AI data centers in 2024. Doc [cottier2024] provides that the expected hardware lifetime is 3.7 years, which directly answers the question."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents confirm that GPU theoretical performance per watt doubles every 3-4 years, but they do not provide the specific 2019 product data referenced in the statement, so we cannot confirm the entire claim."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, over 4x the five cars estimate",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Google reports that training their open source Gemma family of language models emitted 1247.61 tons COe, over 4x the estimate that forms the basis for the five cars number,'","The context states that Gemma’s pre‑training emissions are 1247.61 tCO2e and that this is over four times the five cars estimate, providing both the absolute value and the comparison."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,336 hours,336,hours,is_blank,is_blank,Doc [strubell2019#0022]: 'report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).',"The question asks for the training duration of ELMo on 3 NVIDIA GTX 1080 Ti GPUs. Doc [strubell2019#0022] explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, equivalent to 336 hours, which directly answers the question."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain sufficient information to determine the net cost of fine‑tuning a sparse Mixtral model using 2 million queries on an NVIDIA H100 GPU.
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: The GPU alone accounts for 74 of the total energy consumption due to these components.  GPU accounts for almost 3/4 of electricity consumption.,"The question asks for the GPU share of total provisioned power. Doc [dodge2022] states that the GPU accounts for 74% (approximately 3/4) of electricity consumption, so 74% is the answer."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a statement indicating that the relationship between runtime and energy consumption during inference with large language models was found to be nearly linear.
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give the estimated pounds of CO2e for an average human life in one year globally, so I cannot answer this question based on the available context."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,wu2021b;dodge2022,https://arxiv.org/pdf/2108.06738;https://arxiv.org/pdf/2206.05229,"Doc [wu2021b]: 'PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021)'","The question asks for Google's reported PUE in 2021. Doc [wu2021b] states it was 1.10 in 2021, which directly answers the question."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a statement giving the percent reduction in carbon footprint that AWS claims customers can expect by moving workloads to AWS in North America.
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 V100 32GB GPUs,2,V100 32GB GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024#0016]: '13B 2 64 1 64',The table of bare minimum hardware requirements lists for LLaMA-13B a count of 2 V100 32GB GPUs (and 1 A100 80GB GPU). Thus the minimum number of V100 GPUs needed for inference is 2.
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'A single short GPT-4o query consumes 0.42 Wh ( Wh)',"The question asks for the energy consumption of a single short GPT-4o query. Doc [jegham2025] explicitly states the consumption is 0.42 Wh, so that is the answer."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.14 L/kWh,3.14,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'our EWIF for the U.S. (3.14 L/kWh) is conservative...','U.S. Average 1.170 0.550 3.142 0.708 4.731 2.200 14.704'","The question asks for the U.S. national average water use for electricity generation. Doc [li2025b] provides the electricity water intensity factor (EWIF) for the U.S. as 3.14 L/kWh (also shown as 3.142 L/kWh in the table), which is the direct estimate of water consumption per kWh from power plants."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents mention TPU v3 and V100 GPU, but there is no information about TPU v2 or its average system power per processor, so the question cannot be answered with confidence."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,morrison2025,https://arxiv.org/pdf/2503.05804,Doc [morrison2025]: Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server,"The question states the training cluster had 12 nodes, and the document shows each server contains 8 H100 GPUs, so 12 × 8 = 96 H100 GPUs were used."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25x,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100',"The question asks for the speedup in inference throughput of LLaMA‑13B on A100 versus V100. Doc [samsi2024] states a 1.25‑times increase for the 13B model, which directly provides the answer."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,over 600W,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: r for a single node for the first 300 logging steps during OLMo 2 7B training. ... When actively training, the average GPU power is over 600W, over 85 of an H100's maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W...","The question asks for the average GPU power during the first 300 logging steps while actively training. Doc [morrison2025] explicitly states that the average GPU power is over 600W, which directly answers the question."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the change in Facebook's recommendation and ranking model sizes between 2019 and 2021, so I cannot answer based on the available context."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62000000,tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '... reaching 62 million tonnes in 2022.',"The 2025 paper reports that worldwide electronic waste reached 62 million tonnes in 2022, which directly answers the question."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information comparing LLM inference power draw to diffusion model inference, so I cannot answer the question based on the available context."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a clear statement of which specific model achieved the highest eco‑efficiency rank in the DEA analysis.
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,Doc [wu2021b]: 'current averages of less than 3 years for cell phones and 4 to 5 years for servers.',"The document states that smartphones average less than 3 years, which is a known factor contributing to electronic waste concerns."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'The FLM-101B model is structured with a hidden state dimension of , a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of .'","The question asks for the context window size of FLM-101B. Doc [li2025a] explicitly states that the context window is 2,048 tokens, which is a numeric answer of 2048 tokens. The answer is supported by the documented value and the unit is tokens."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the percentage of Amazon's People Managers who identified as women in 2023, so the answer cannot be determined from the context."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention intra‑zone scaling with T4 GPUs or linear per‑GPU speedup for CV models, so the answer cannot be determined from the context."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'In addition, we propose reporting the financial cost or price tag of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.'","The question asks whether Green AI involves reporting the financial cost of finding, training, and running models. Both Doc [schwartz2019] and Doc [schwartz2019#0008] explicitly state that reporting this cost is a key practice in Green AI, so the statement is true."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",21% to 33%,"[21, 33]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Excluding equity, the fraction decreases to 21 to 33 (see for additional plots).'","The question asks for the percentage range of total amortized cost attributed to computing hardware when equity is excluded. Doc [cottier2024] explicitly states that this fraction is 21 to 33%, so that is the answer."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the number of packages Amazon delivered via EVs in Europe in 2023, so the answer cannot be determined from the context."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,A10G,A10G,is_blank,is_blank,is_blank,"Doc [griggs2024#0008]: 'Each dimension influences cost efficiency similarly: smaller sizes are best served on A10G, and larger sizes are best served on A100.'","The question asks for the most energy‑efficient GPU for generating a single classification token, which is a very small request size.  The quoted passage states that smaller request sizes are best served on the A10G GPU, indicating it is the most energy‑efficient choice for such minimal token generation."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: 'metrics such as accuracy and F1 score are slightly lower after optimization',"The question asks whether accuracy and F1 scores always improved after optimization in the financial sentiment case study. The cited passage from Doc [khan2025] states that accuracy and F1 score are slightly lower after optimization, indicating they did not always improve. Therefore the statement is false."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.000022 kL,2.2e-05,kL,morrison2025,https://arxiv.org/pdf/2503.05804,Doc [morrison2025]: 'Mining 1 kg of rare earth materials consumes about 11 kL of water ... and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.',"The document states that 1 kg of rare earth consumes 11 kL of water and that a 12‑inch wafer (125 g) yields 63 H100s, giving ≈1.984 g per GPU. With 0.1 % rare earth by mass, the GPU contains ≈1.984 µg of rare earth, which at 11 kL/kg results in ≈2.2×10⁻⁵ kL of water consumption for mining per GPU."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents mention Yelp sentiment analysis benchmarks or provide any comparison between traditional models and large language models, so the statement cannot be evaluated with the given context."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents reference the EPA’s average CO2 per kWh but do not include the numeric value, so the answer cannot be determined from the available context."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a projected electricity consumption range for global AI specifically for the year 2027.
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",15%,15,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: '... total cost increased to 2.699, resulting in about a 15 higher cost.'","The passage shows Max-Performance g6e.xlarge cost 2.699 versus InferSave’s top choice cost 2.344, and states the cost increase is about 15%. Therefore the Max-Performance instance was roughly 15% more expensive."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024, so the answer cannot be determined from the context."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",over 1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,Doc [luccioni2024]: 'the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.',"The question asks for the factor between the most and least energy-intensive tasks. Doc [luccioni2024] states that image generation exceeds text classification by over a factor of 1450, so 1450 is the reported factor."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,700 kL,700,kL,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO equivalent (), while requiring more than 700 kiloliters (kL) of water for cooling alone , enoug'","The question asks for the amount of clean freshwater evaporated during GPT‑3 training. Doc [jegham2025] states that more than 700 kL of water is used for cooling, which implies that this volume is evaporated. Thus 700 kL is the best supported figure."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: The AI Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration ) and water consumption .","The passage from Doc [ebert2024] states that the AI Act omits indirect emissions from AI applications such as oil and gas exploration, indicating it does not mandate their disclosure."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: 'The GPU alone accounts for 74 of the total energy consumption due to these components.',"The question asks for the proportion of total electricity attributed to the GPU when training BERT‑base. Doc [dodge2022] explicitly states that the GPU accounts for 74% of the total energy consumption, so 74% is the correct answer."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention a Mixtral model or specify a maximum batch size for fine‑tuning on an NVIDIA A100‑40GB GPU, so the answer cannot be determined from the given context."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Doc [schwartz2019]: 'Amazon's AWS (the largest cloud computing platform) only covered fifty percent of its power usage with renewable energy.'\nDoc [schwartz2019#0083]: 'y covered fifty percent of its power usage with renewable energy.',"The question asks for the percentage of power usage covered by renewable energy in 2018. Both Doc [schwartz2019] and Doc [schwartz2019#0083] state that Amazon's AWS covered fifty percent of its power usage with renewable energy, so the answer is 50%."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention fiber optic cable installation or kilometers for AI workloads in 2023.
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,800M USD,800,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'we estimate that it cost 800M to acquire the hardware used to train GPT-4, compared to 40M for the amortized hardware CapEx + energy cost.'","The question asks for the upfront hardware acquisition cost to train GPT-4. Doc [cottier2024] explicitly states that the acquisition cost is 800 M, so that is the answer, expressed in USD."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: 'Google reports a 48 increase in GHG emissions since 2019',"The question asks for the percentage increase in GHG emissions reported by Google in its 2024 environmental report. Doc [luccioni2025a] states that Google reports a 48 increase in GHG emissions since 2019, which is interpreted as 48 %. Thus the answer is 48 %."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'Energy consumption should be reported at the cumulative server level (see also ).',"The question asks for the recommended measurement level for reporting AI energy consumption. The cited passages explicitly state that the authors recommend reporting at the cumulative server level, so that is the answer."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE-8B architecture or its inference computation reduction compared to the Llama2-7B model, so I cannot determine the percentage from the available context."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention preemption mechanisms such as recomputation or swapping or compare their energy consumption when an LLM inference server is overloaded, so the answer cannot be determined from the given context."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,GPUs,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: '0.48 comparison between H100x2 and A100x2 serving Llama2-70b.',"The document states a comparison between H100x2 and A100x2 serving Llama2-70b, indicating that two A100 GPUs are used for serving the model at BF16 precision."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about McKinsey projections for data center electricity consumption in 2030.
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'The umbrella term `Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.'","The question claims the term was proposed to only encompass climate-positive applications. The cited passage shows it also aims to improve the sustainability of AI approaches, so the statement is false."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a clear numeric estimate of the full GPT‑3 training energy in MWh, so I cannot answer confidently based on the available context."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: What proportion of emissions can we expect to save if we change the start time by up to 24 hours? For very short experiments like DenseNet 201 (a), we can find significant reduction, greater than 30 in multiple regions, and up to 80 in West US; for very long runs like t","The question asks for the maximum potential percentage reduction for DenseNet 201 in the West US region using the Flexible Start optimization.  Doc [dodge2022] states that up to 80% reduction is possible, so the answer is 80%."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about intercontinental training performance or a 7% slowdown for high-granularity CV models, so the statement cannot be verified."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b;strubell2019,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/1906.02243,"Doc [luccioni2025b]: '... 626,155 pounds of emissions.'
Doc [strubell2019]: '... 626,155'","The question asks for the carbon footprint in lbs CO2e of training BERT as reported in the 2019 Strubell et al. study. Doc [luccioni2025b] directly cites that figure as 626,155 pounds, and Doc [strubell2019] also contains the same number. Therefore the answer is 626,155 lbs CO2e."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the land area occupied by new AI data centers globally in 2022, so I cannot answer this question."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific numeric value for Facebook’s total operational energy footprint reduction over 2019‑2021 due to iterative hardware‑software optimization.
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so I cannot answer the question based on the available context."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",7–26 thousand grams,"[7, 26]",thousands of grams,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: '7k grams vs.26k grams, for the most effic'","The question asks for the approximate range of CO2 emissions between the most and least efficient regions in the BERT training experiment. Doc [dodge2022] states that emissions vary from 7k grams to 26k grams, giving a range of 7 to 26 thousand grams."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'peaked in 2022, with 10 of notable models that year releasing some degree of information.'","The passage explicitly states that the direct release of environmental information peaked in 2022, indicating that year as the peak before a decline."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: As shown in Fig., the MoE layer is the most time‑consuming, accounting for 85% of the overall execution time on average. Doc [xia2024]: To concretely understand the opportunity for improving MoE layer performance, we also perform a kernel‑level analysis within the MoE layer.","The passages state that the MoE layer dominates execution time and is the focus of detailed performance analysis, indicating that it is a common target when enhancing fine‑tuning performance."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention a projected public health burden for U.S. data centers in 2030, so the question cannot be answered with confidence from the given context."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide a clear, unambiguous value for the maximum batch size of Mixtral fine‑tuning on Hellaswag with a dense setup on a NVIDIA A40 GPU; the available table snippet is ambiguous and does not specify which number corresponds to Mixtral dense on Hellaswag."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a numeric value for the total energy consumption of training FLM-101B, so the answer cannot be determined from the context."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",9.6%,9.6,percent,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: Model  BERT   BERT   6B  Dense   Dense   Dense   ViT   ViT   ViT   ViT   ViT  FS 5.4 3.6 2.0 1.5 1.7 1.9 4.2 4.0 3.3 2.8 2.2  PR 7.6 9.2 9.6 1.6 2.0 2.4 9.2 9.3 9.2 9.6 9.6,The question asks for the maximum emissions saving achievable with the Pause and Resume (PR) optimization on the 6B transformer.  The table in Doc [dodge2022] lists the percent emissions reductions for PR; the highest value for the 6B transformer is 9.6 %.  Thus the maximum potential saving is 9.6 %.
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 gCO2eq,0.32,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: g for ). The difference is much more drastic if comparing BERT-based models for tasks such as text classification with the larger multi-purpose models: for instance  emits just 0.32g of  per 1,000 queries, compared to 2.66g for  and 4.67g for .","The 2024 study states that the BERT-based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 g CO2eq per 1,000 text classification queries, which directly answers the question."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a statement that adding compute resources to accelerate the MoE layers during fine‑tuning can increase costs, so the claim cannot be confirmed or refuted with confidence."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain an explicit execution time for a sparse Mixtral model fine‑tuned on an NVIDIA A40‑48GB with batch size 10, so I cannot answer the question based on the given context."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.',"The question asserts that GPU-level power consumption monitoring is recommended as the preferred method. The cited passage from Doc [ebert2024] explicitly states that GPU-level monitoring is not recommended, so the statement is false."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not mention the JetMoE-8B model or its GSM8k benchmark score, so I cannot answer the question based on the available context."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: 'finding that training accounted for only half of the model's overall emissions',"The 2023 article on BLOOM’s carbon footprint, referenced in Doc [luccioni2025b], states that training accounted for only half of the model’s overall emissions, which corresponds to 50%."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","2,738 days",2738,days,is_blank,is_blank,Doc [morrison2025#0007]: '...or the amount of water consumed by one average person in the United States in about 7.5 years.',The document equates the training water consumption to roughly 7.5 years of an average U.S. person’s water usage. Converting 7.5 years to days (7.5 × 365 ≈ 2738) gives the equivalent water usage in days.
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,is_blank,is_blank,Doc [dodge2022#0065]: '…totaling 8.3 metric tons per year.',"The question asks for the estimated CO2 emission for one year of average US home energy use. Doc [dodge2022#0065] explicitly states that this is 8.3 metric tons per year, so that is the answer."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a numeric estimate for the total fine-tuning cost of Mixtral on GSM8K with sparse MoE on an NVIDIA A40-48GB GPU.
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",8 samples,8,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse. -D  -S  -D  -S    2  8  6  20    1  3  2  8',"The table in Doc [xia2024] lists the maximum batch sizes that fit into an NVIDIA A40‑48 GB for each model and sparsity. For the sparse Mixtral model, the corresponding value is 8, indicating the batch size of the longest‑running MoE layer."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10-50 requests,"[10, 50]",requests,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '10–50 queries on GPT-3 consumes around half a liter of water.',"The passage from Doc [luccioni2025a] states that 10–50 queries on GPT‑3 consume about 0.5 L of water, which is equivalent to a 500 ml bottle. Thus, the number of user requests needed to consume a 500 ml bottle ranges from 10 to 50."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024#0016]: '7B   1  64  1  64' and Doc [samsi2024]: 'The 7B model was run on a single GPU',"The table in Doc [samsi2024#0016] shows that the 7B model requires 1 V100 GPU for inference, and Doc [samsi2024] confirms that the 7B model was run on a single GPU, indicating the bare minimum is one NVIDIA V100 32GB GPU."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,836,000 tokens",1836000,tokens,kim2025;jegham2025,https://arxiv.org/pdf/2504.11816;https://arxiv.org/pdf/2505.09598,"Doc [kim2025]: ut tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.
Doc [jegham2025]: short-form (100 input, 300 output token","The online inference workload evaluation in Doc [kim2025] describes 3000 requests, each with 100 input tokens (inferred from Doc [jegham2025]) and 512 output tokens. Multiplying 3000 by (100+512) yields 1,836,000 total tokens."
