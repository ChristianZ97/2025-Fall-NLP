id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 tCO2e,1.2,tCO2e,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'to help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2 e (Table 2).'","The question asks for the approximate CO2e of a single passenger round trip from San Francisco to New York. Doc [patterson2021] explicitly states that this trip emits about 1.2 tCO2e, so the answer is 1.2 tCO2e."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021;luccioni2023,https://arxiv.org/pdf/2104.10350;https://arxiv.org/pdf/2302.08476,"Doc [patterson2021]: 'It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.'
Doc [luccioni2023]: 'over 3.5 million hours (14.8 days with 10,000 GPUs)'","The question asks for the training duration of GPT‑3 under the specified configuration. Doc [patterson2021] explicitly states that training with 10,000 V100 GPUs at 24.6 TFLOPS/sec takes approximately 14.8 days, and Doc [luccioni2023] confirms the same value. Thus the answer is 14.8 days."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,Doc [morrison2025#0138]: ... Llama 3.2 1B 8 0.036 ...,"The table in Doc [morrison2025#0138] lists the GPU Power Usage for Llama 3.2 1B at 8 requests per second as 0.036 kWh, which is the estimated value requested."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The question asks for the percentage of time saved. Doc [li2025a] explicitly states a 72% time-saving when using the growth strategy for the 101B model, so 72% is the answer."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750-fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'This is a more than 6,750 fold improvement in processor clock speed...'","The question asks for the fold change in processor clock speed from the Intel 4004 (740 kHz) to typical 2021 microprocessors (5,000,000 kHz). Doc [wu2021b] explicitly states a more than 6,750‑fold improvement, which directly answers the question."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.'","The document states that the avoided emissions are equivalent to taking more than 13,900 cars off the road, so 13,900 cars is the estimated number requested."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents include information about premature deaths from scope‑2 pollutants in 2030 for U.S. data centers, so the answer cannot be determined with confidence."
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020]'.","The question asks for the average increase in U.S. data center electricity consumption between 2010 and 2014. Doc [wu2021b] explicitly states that the increase was about 4%, so that is the answer."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg CO2eq,463,kg CO2eq,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.'","The question asks for the estimated embodied carbon emissions per GPU. Doc [morrison2025] explicitly states that the embodied emissions are 463 kg CO2eq per GPU, so this value is used as the answer."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24 data centers,24,data centers,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.'","The document states that in 2023, AWS increased the number of data centers using recycled water for cooling to 24, so the answer is 24 data centers."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",44.41,44.41,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 101B 26.54B 44.41 39.76 67.88 28.54 41.47,"The question asks for the final average performance score of FLM‑101B on the Open LLM Leaderboard. In Doc [li2025a], Table 6 lists the average score for the 101B model as 44.41, which is the value to report."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22% of e-waste,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled,'","The question asks for the percentage of e-waste formally collected and recycled according to the UN’s Global E‑Waste Monitor 2024. Doc [luccioni2025a] states this value as about 22%, so the answer is 22%."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: 'This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/...'","The question asks for the estimated duration of a full training run. Doc [dodge2022] explicitly states that a full run would take 60 days, so that is the answer."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014 year,2014,year,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014,'","The question asks for the launch year of the One Hundred Year Study on Artificial Intelligence. Doc [stone2022] explicitly states it was launched in the fall of 2014, so the answer is 2014 years."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'The total permitted site-level annual emission limits are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.'","The question asks for the total permitted annual NOx limits for data center backup generators in northern Virginia between 2023 and 2024. Doc [han2024] explicitly states the total permitted site-level annual emission limits as approximately 13,000 tons of NOx, which directly answers the question."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts per MoE layer,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'We set the same number of experts to 8 and top-k to 2 for every layer.',"The question asks how many experts are in each MoE layer of JetMoE-8B. Doc [shen2024] states that the same number of experts is set to 8 for every layer, so the answer is 8 experts per layer."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400×,400,is_blank,cottier2024;li2025a,https://arxiv.org/pdf/2405.21015;https://arxiv.org/pdf/2309.03852,"Doc [cottier2024]: 'GPT-4 has the largest amortized hardware and energy cost, at $40M.' Doc [li2025a]: 'FLM-101B ... trained from scratch within a $100,000 budget.'","The question asks for the ratio of GPT‑4’s amortized training cost to FLM‑101B’s training budget. Doc [cottier2024] gives the amortized cost as $40 M, and Doc [li2025a] provides FLM‑101B’s budget as $100 k. Dividing 40 M by 100 k yields 400, so GPT‑4’s cost is 400 times higher."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3×,3,is_blank,is_blank,is_blank,"Doc [wu2021a#0075]: 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.'","The question asks for the factor by which the carbon footprint decreases when GPU utilization is raised to 80%. Doc [wu2021a#0075] explicitly states that this increase leads to a 3× reduction, so 3 is the correct factor."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,is_blank,is_blank,"Doc [luccioni2025c#0029]: h GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The question asks for the model with the highest GPU energy consumption for 1,000 queries. The quoted passage from Doc [luccioni2025c#0029] states that the maximum energy usage is over 3,426 Wh for Command‑R Plus, making it the highest among the listed models in the appendix."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents provide pre‑training energy for BLOOMz variants but do not specify the energy consumption for the original BLOOM model. Therefore the requested value cannot be determined from the given context.
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).',"The question asks for the training cost of the English portion of FLM-101B. Doc [li2025a] explicitly states that 28.22 zettaFLOPs were required for English, so that is the answer."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,models,is_blank,is_blank,"Doc [luccioni2024#0029]: 'To be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or multi-task models,'","The question asks how many models were sampled and analyzed. The quoted passage from Doc [luccioni2024#0029] explicitly states that 88 models were sampled, indicating that the study analyzed 88 different machine learning models."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.1%,0.1,percent,is_blank,is_blank,Doc [patterson2021#0063]: 'The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.',"The question asks for the percentage of the Switch Transformer’s 1500 B parameters that are activated per token. Doc [patterson2021#0063] explicitly states that 0.1% of the 1500 B parameters are activated, so that is the answer."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The question asks for the total wall‑clock time required to train the FLM‑101B model using a growth strategy. Doc [li2025a] explicitly states that the total time cost for training FLM‑101B under the growth schedule is 21.54 days, so that is the answer."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: 'on to train this model to completion would be approximately (60/8)*13.8 = 103.5 MWh, or 103,500 kWh'","The question asks for the full training energy of a 6.1 billion‑parameter transformer. Doc [dodge2022] provides the calculation (60/8)*13.8 MWh = 103.5 MWh for a full run, giving the required estimate."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a;jegham2025,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2505.09598,"Doc [luccioni2025a]: 'the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.' Doc [jegham2025]: 'aligned with the Jevons Paradox [76], where increased efficiency drives systemic demand.'",The question asks for the economic principle underlying the paper’s argument. Both Doc [luccioni2025a] and Doc [jegham2025] explicitly cite Jevons’ Paradox as the principle explaining why efficiency gains can lead to higher overall consumption and thus not guarantee net environmental benefits.
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x,2.2,x,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'n scale: we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.'","The question asks for how many times larger the total compute is compared to the final training run. Doc [cottier2024] provides the median ratio of 2.2x, which directly answers the question."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 – 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The question asks for the projected water withdrawal in 2027. Doc [li2025b] states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal, so that range is the answer."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,is_blank,is_blank,"Doc [luccioni2025c#0058]: 'AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks', Doc [luccioni2025c#0013]: 'AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models'.","The question asks for the name of the collaborative project that creates a standardized method for comparing inference efficiency. The passages from Doc [luccioni2025c#0058] and Doc [luccioni2025c#0013] explicitly identify that project as ""AI Energy Score""."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: munity has paid relatively little attention to computational efﬁciency. In fact, as Figure 1 illustrates, the computational cost of research is increasing exponentially, at a pace that far exceeds Moore’s Law [28]. Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The question asks whether Red AI is on the decline. The excerpt from Doc [schwartz2019] explicitly states that Red AI is on the rise, which contradicts the claim, so the correct answer is False."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",False,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a#0094]: 'Fig. 10. A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.'","The question asks whether the majority of experimentation workflows use GPUs at over 80% capacity. The cited passage states that the majority use only 30-50% capacity, so the statement is false."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.'","The context indicates a 300,000x increase in compute over six years, not a 200,000x increase, so the statement is false."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22 regions,22,regions,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy so'","The question asks for the number of AWS data center regions with 100% renewable‑matched electricity in 2023. Doc [amazon2023] explicitly states 22 regions, so that is the answer."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,experts,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top‑k to 2 for every layer.'
Doc [shen2024#0034]: 'Top-k 2'","The documents state that JetMoE‑8B uses a top‑k of 2 for each MoE layer, meaning two experts are activated per token in every layer."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4% drop,6.4,%,is_blank,is_blank,"Doc [wu2021b#0060]: 'the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021]'.","The question asks for the reported drop in global carbon emissions in 2020. Doc [wu2021b#0060] explicitly states that emissions dropped by 6.4%, so the answer is 6.4%."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,69 years,69,years,is_blank,is_blank,Doc [stone2022#0449]: The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.,"The question asks for the age of AI in 2025. Doc [stone2022#0449] states the field was born in 1956, so 2025 minus 1956 equals 69 years. Hence the approximate age is 69 years."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific numeric value for the longest MoE kernel execution time in microseconds for a dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU.
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025;li2025b;patterson2021,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2104.10350,"Doc [jegham2025]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity...' Doc [li2025b]: 'estimated training energy of 1287 MWh' Doc [patterson2021]: 'its energy consumption is 1287 MWh.'","The question asks for the estimated training electricity consumption of GPT-3. All three documents report the same figure of 1,287 MWh, so the answer is 1,287 MWh."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not contain a specific numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU, so the answer cannot be determined with confidence."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not give the maximum batch size for BlackMamba sparse fine‑tuning on the GSM8K dataset with an NVIDIA A40 GPU, so the answer cannot be determined from the context."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'The average data center PUE in 2023 was 1.58 globally',"The question asks for the global average PUE of AI‑dedicated data centers in 2023. Doc [ebert2024] states that the average data center PUE in 2023 was 1.58 globally, which is the value provided in the context."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B parameters,2,parameters,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The question asks for the number of parameters activated per token during inference. Doc [shen2024] explicitly states that JetMoE-8B activates 2B of its 8B total parameters for each input token, so the answer is 2 billion parameters."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023#0300]: 'hold 1.3 GW of storage capacity, up from 445 MW in 2022.'","The question asks for Amazon’s energy storage capacity in 2023. All cited documents state that Amazon held 1.3 GW of storage capacity that year, so 1.3 GW is the correct answer."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",44%,44,%,chung2025,https://arxiv.org/pdf/2505.06371,Doc [chung2025]: '...reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.',"The question asks for the percentage decrease in energy use when targeting a TPOT of 100 ms versus minimizing latency. Doc [chung2025] states that on the Pareto frontier (which includes the 100 ms target) energy consumption per generation is reduced by 44% compared to the latency‑minimizing configuration, so the answer is 44 %."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"14,380 electric delivery vans",14380,electric delivery vans,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Last Mile Electric Delivery Vehicles by Region 2022 2023
U.S. 2,600 11,800
Europe 1,220 3,000+
India 3,800 7,200+' and Doc [amazon2023]: 'Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.'","The table gives van counts for 2022 and 2023 by region. Subtracting the 2022 numbers from the 2023 numbers gives added vans: 11,800-2,600=9,200 for the U.S., 3,000-1,220=1,780 for Europe, and 7,200-3,800=3,400 for India. Summing these additions yields 14,380 electric delivery vans added across 2022 and 2023."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give a specific GHG emission figure for pre‑training the Llama 7B model.
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear statement identifying the specific hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification.
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",pre-training of large language models,pre-training of large language models,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: 'the “five cars” number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large... the original article reports AI training workloads emitting as little as 26 pounds CO2e.'; Doc [li2025a#0050]: 'carbon footprints originated from the pre-training process.',"The question asks for the AI process underlying the five cars estimate. Doc [luccioni2025c] shows the estimate is tied to training AI models, and Doc [li2025a#0050] identifies pre‑training as the specific training phase whose emissions were measured. Thus the estimate is based on the pre‑training of large language models."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the percentage of AI inference workloads in Asia powered by coal in 2023.
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,flights,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: ... the cumulative emissions from approximately 272 transatlantic flights between Boston and London.,"The question asks how many transatlantic flights the projected annual emissions of GPT-4o inference correspond to. Doc [jegham2025] states that these emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London, so 272 is the answer."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.'","The question asks for the percentage reduction in RM2 model size after 32‑bit to 16‑bit quantization. Doc [wu2021a] explicitly states a 15% reduction, so 15% is the correct answer."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,Doc [stone2022]: The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.,"The passage from Doc [stone2022] explicitly states that the AI field was christened in 1956, answering the question about the official christening date."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].'","The question asks whether the claim that roughly 770 million people lack stable electricity is correct. Doc [wu2021b] explicitly states that approximately 770 million people do not have access to a stable supply of electricity, which matches the claim, so the answer is True."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'sions arising from energy sources used to power model training and deployment, including servers and data center cooling.'","The question asserts that operational impacts exclude GHG emissions from servers and data center cooling, but Doc [morrison2025] states that operational environmental impacts include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore the statement is false."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: The optimizer stage in BlackMamba fine‑tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine‑tuning with batch size = 1).,"The question asks for the percentage of the running time spent in the optimizer stage during BlackMamba sparse fine‑tuning on an NVIDIA A40‑48GB GPU with batch size 1. Doc [xia2024] explicitly states that this stage accounts for up to 53% of the total runtime, which directly answers the query."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: 'The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s exp.',"The documents state that the 5‑10% reduction estimate lacks detailed, publicly available calculations and sound scientific grounding, so the claim is not supported as stated."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",True,1,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy'","The question asserts that sparsely activated DNNs use less than one‑tenth the energy of large dense DNNs while keeping accuracy. Doc [patterson2021] explicitly states that large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy, directly supporting the claim."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000 USD",25000,USD,is_blank,is_blank,"Doc [schwartz2019#0023]: 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.'","The question asks for the estimated cost of training Grover on 256 TPU chips for two weeks. Doc [schwartz2019#0023] explicitly states that this training cost was $25,000, so the answer is $25,000 USD."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: The average data center PUE in 2023 was 1.58 globally[74],"The question asks for the average global data center PUE in 2023. Doc [ebert2024] explicitly states that the average PUE was 1.58, so that is the value to report."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,is_blank,is_blank,"Doc [wu2021a#0111]: 'Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.'","The question asks for the manufacturing share of a client device’s total carbon footprint. Doc [wu2021a#0111] explicitly states that manufacturing accounts for 74% of the total footprint, so 74% is the correct answer."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules per token,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.'","The question requests the energy per token at a maximum generation length of 512 tokens. The cited passage from Doc [samsi2024] states that for length 512 the energy per output token is about 3–4 Joules, which directly answers the question."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,members,stone2022,https://arxiv.org/pdf/2211.06318,Doc [stone2022]: 'The seventeen-member Study Panel',"The question asks for the number of members in the inaugural 2015 Study Panel. Doc [stone2022] states that the panel was a seventeen-member Study Panel, giving the required count of 17 members."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Doc [chung2025#0062]: 'Generally, models with more parameters consume more energy, but this is not always the case.'","The question asserts that larger parameter counts always increase inference energy. Doc [chung2025#0062] explicitly states that this is not always the case, so the claim is false."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.'","The question asks whether the Study Panel is concerned that AI poses an imminent threat. The cited passage states the panel found no cause for concern, so the statement is false."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain the 0.083 kWh per 1,000 queries figure required to calculate the daily energy consumption."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023, so the answer cannot be determined with confidence."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the CO2 emissions from OpenAI's API requests in January 2024, so I cannot answer based on the given context."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,1 L/kWh,1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'On average, depending on the weather conditions and operational settings, data centers can evaporate approximately 1 – 9 liters per kWh of server energy: 1 L/kWh for Google’s annualized global on-site water efficiency [4] and 9 L/kWh for a large commercial data center during the summer in Arizona [16].'","The question asks for the average water use effectiveness of Google’s AI‑dedicated data centers.  The context states that Google’s annualized global on‑site water efficiency is 1 L/kWh, so that is the required average WUE."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,0.443 Wh,0.443,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: t efficient, requiring only 0.443 Wh for long prompts (approximately 7,000 words of input and 1,000 words of output)","The question asks for the energy consumption of the o3 (GPT‑4o) model for a long prompt. Doc [jegham2025] states that the model requires only 0.443 Wh for long prompts, so that is the answer."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the distance from Earth to the Sun, so I cannot answer this question based on the available context."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The question asks for the R&D staff cost share for Gemini Ultra including equity. Doc [cottier2024] explicitly states that this shares 49% of the total amortized cost, so 49% is the correct answer."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9×,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'The explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.',"The question asks for the factor by which AI training infrastructure capacity grew at Facebook over the 1.5‑year period. Doc [wu2021a] states that this growth was 2.9×, so that is the factor reported."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e, which is over 40× the 'five cars' estimate",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the","The context states that Meta’s Llama 3 family emitted 11,390 tCO2e during pre‑training, and that this figure is over 40 times the ""five cars"" baseline estimate, directly answering both parts of the question."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous Batching,Continuous Batching,is_blank,fernandez2025,https://arxiv.org/pdf/2504.17674,"Doc [fernandez2025#0045]: 'Continuous Batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time.'","The question asks for the batching strategy that reduces idle GPU time by replacing completed requests with new ones. Doc [fernandez2025#0045] explicitly names this approach as ""Continuous Batching"" and explains that it dynamically replaces completed requests, thereby reducing idle GPU time."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'The entire alignment process takes 60 H100 GPU hours.',"The question asks for the total GPU hours for the alignment process, which Doc [shen2024] states is 60 H100 GPU hours, covering both dSFT and dDPO fine-tuning."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,is_blank,is_blank,Doc [patterson2021#0043]: 'The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO 2 e/KWh [USE21].',"The question asks for the gross carbon intensity of energy for the U.S. average mix in 2021. Doc [patterson2021#0043] directly states that intensity as 0.429 kg CO2e per kWh, so that value is reported as the answer."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency""', which involves integrating socio-technical aspects in the description and understanding of AI systems","The question asks for the term proposed for expanding AI transparency to include socio-technical and environmental aspects. Doc [luccioni2025b] explicitly names this term as ""social transparency"", so that is the answer."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: 'We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)',"The passage from Doc [luccioni2025b] explicitly states that researchers do not believe a universal, one‑size‑fits‑all approach to AI ethics and sustainability can be developed, so the correct answer is False."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 gCO2eq per 1,000 inferences",1594,gCO2eq,is_blank,is_blank,"Doc [luccioni2024#0052]: 'the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences'","The question asks for the CO2eq emissions of stable-diffusion-xl-base-1.0 per 1,000 inferences. Doc [luccioni2024#0052] explicitly states 1,594 grams of CO2eq, so that is the answer."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,chen2024,https://arxiv.org/pdf/2405.01814,"Doc [chen2024]: 'we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.'","The question asks for the name of the inference system in the Chen et al. paper that uses model-attention disaggregation. Doc [chen2024] explicitly states that the system is called Lamina, so that is the answer."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,Doc [erben2023]: 'Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.',"The question asks for the decentralized PyTorch-based framework used for distributed spot instance training. Doc [erben2023] explicitly names Hivemind as a PyTorch-based framework for collaborative, decentralized training, and the same document notes that the experiments used Hivemind. Therefore, Hivemind is the correct answer."
q080,True or False: The AlphaGo program defeated the human Go champion.,TRUE,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to ...'","The question asks whether AlphaGo defeated a human Go champion. Doc [stone2022] explicitly states that AlphaGo beat the human Go champion in a five-game match, so the correct answer is TRUE."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,is_blank,is_blank,"Doc [luccioni2025c#0029]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).'","The question asks for the range of GPU energy usage for 1,000 inference queries. Doc [luccioni2025c#0029] explicitly states that the range is from 0.06 Wh to 3,426 Wh, so that is the answer."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.'","The question asks for the percentage price difference between Max-Performance and InferSave at 100 TPS offline. Doc [kim2025] states that Max‑Performance’s selected instance costs $2.699 vs. InferSave’s $2.13, an increase of about 26.7%, which directly answers the question."
q094,What is the total number of parameters in the JetMoE-8B model?,8B parameters,8,billion parameters,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'JetMoE-8B has 8B parameters'. Doc [shen2024]: '# Total Params 8B',"The question asks for the total number of parameters in JetMoE-8B. Both the text and the evaluation table in Doc [shen2024] state that the model has 8 billion parameters, so the answer is 8B parameters."
q093,How many parameters does the largest T5 model have?,11B parameters,11000000000,parameters,is_blank,is_blank,"Doc [patterson2021#0056]: 'The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO 2 e.'","The question asks for the parameter count of the largest T5 model. Doc [patterson2021#0056] explicitly states that the largest T5 has 11B parameters, so that is the answer."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about classification experiments on German public administration texts or the performance of sentence embedding models on that dataset.
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: '101 4 4 12 192 2160 165 52.88%',"The question asks for the FLOPs utilization percentage in the final (101B) growth stage of FLM‑101B training. The table in Doc [li2025a] lists the utilization for each stage and shows 52.88% for the 101B stage, so that is the answer."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810×,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.',"The question asks for the factor reduction relative to a CPU baseline. Doc [wu2021a] explicitly states that full‑stack optimization—including platform‑level caching, GPU acceleration, and algorithmic changes—can reduce the operational carbon footprint by 810×, so 810 is the answer."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"Doc [li2025b]: 'Importantly, the company’s data center water consumption increased by ∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4], and another technology company’s'","The question asks for the percentage increase in Google’s data center water consumption from 2021 to 2022. Doc [li2025b] explicitly states an increase of about 20% for that period, which is the value used in the answer."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Doc [rubei2025]: 'Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.',"The question asks whether custom tags can reduce energy consumption for zero‑shot, one‑shot, and few‑shot source code completion. The cited passage from Doc [rubei2025] explicitly states that custom tags reduce energy consumption across all three techniques, so the correct answer is True."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The question asks for the average share of amortized hardware and energy cost attributed to AI accelerator chips. Doc [cottier2024] explicitly states that this share is 44%, so that is the answer."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5B liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: '3.5B Liters of water returned to communities from replenishment projects in 2023.'; Doc [amazon2023#0461]: 'AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.',"Both cited documents state that Amazon’s replenishment projects returned 3.5 billion liters of water to communities in 2023, which is expressed as 3.5B liters."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,gCO2eq/kWh,gCO2eq/kWh,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"Doc [luccioni2023]: 'The unit of measurement typically used for quantifying and comparing carbon emissions is CO2 equivalents. This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of CO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh) 1.'","The question asks for the emissions metric defined as CO₂ emissions per unit of electricity consumed. The context explicitly defines this metric as grams of CO2 emitted per kilowatt hour of electricity, i.e., gCO2eq/kWh, so that is the name of the metric."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: training costs. We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M.,"The question asks for the estimated amortized training cost of GPT‑4. The cited passage from Doc [cottier2024] states that GPT‑4’s amortized cost is $40M, which directly answers the question."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3.7,million GPUs,is_blank,is_blank,Doc [luccioni2025a#0081]: NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023),"The question asks how many GPUs NVIDIA shipped in 2024. Doc [luccioni2025a#0081] states the exact number: 3.7 million GPUs, so that is the answer."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]'","The passage from Doc [luccioni2025b] explicitly names the Finnish project as ETAIROS, which is the acronym requested."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,"Doc [wu2021a]: 'Achieving a Power Usage Effectiveness (PUE) of about 1.10,'; Doc [wu2021b]: 'the PUE of Facebook datacenters is 1.10 (2020)'","Both Doc [wu2021a] and Doc [wu2021b] state that Facebook’s data centers achieve a PUE of 1.10, so the answer is 1.10."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the energy consumption of the DS Llama 70B model on the FKTG dataset, so the answer cannot be determined from the given context."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device'","The question asks for the number of physical print books that equal the CO2 emitted by one Amazon Kindle. Doc [luccioni2025a] explicitly states that 115 books produce the same amount of CO2 as one Kindle, so 115 is the answer."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024#0003]: 'per-household health burden could be 200x more than that in less-impacted communities.',"The question asks for the factor by which the per-household health burden is higher in the most affected, economically-disadvantaged communities. Doc [han2024#0003] explicitly states that it could be 200x more, which matches the other passages and provides the required factor."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 µg/m^3,9,µg/m^3,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3,'","The question asks for the EPA’s tightened primary standard for annual average PM2.5. Doc [han2024] explicitly states that the limit is 9µg/m^3, providing the required value and unit."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,Doc [luccioni2024]: 'image generation 2.907 3.31',"The question asks for the average energy consumption per 1,000 image generation inferences. Table 2 in the 2024 study lists the mean energy for image generation as 2.907 kWh, so that is the answer."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,30M,30,M,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.',"The question asks for the estimated amortized training cost of Google’s Gemini Ultra. The passage in Doc [cottier2024] explicitly states that Gemini Ultra’s amortized training cost was $30M, so that is the answer."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",rebound effects,rebound effects,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'rebound eﬀe cts, in which improved eﬃciency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resour ces'","The question asks for the phenomenon where improved efficiency leads to higher usage and resource consumption. Doc [luccioni2025b] explicitly defines rebound effects as this phenomenon, and Doc [luccioni2025b#0125] reiterates the same idea, confirming ""rebound effects"" as the correct answer."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.64,0.64,is_blank,is_blank,is_blank,"Doc [erben2023#0088]: 'the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs (Figure 7a).' Doc [erben2023#0091]: 'NLP 35%) as with C-4 (CV 9%, NLP 36%) to A-4.'",The documents state that NLP throughput is 36% slower in the four‑continent setup relative to the local experiment. A 36% drop means 64% (0.64) of the local throughput is achieved.
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.75,0.75,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: ... Mistral-small 0.020 ... After Optimization ... Mistral-small 0.015,"The table in Doc [khan2025] shows Mistral-small’s emissions before optimization as 0.020 kg and after optimization as 0.015 kg. Dividing the after value by the before value gives a multiplier of 0.015/0.020 = 0.75, indicating emissions were 75% of the original amount."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any explicit mention of the total number of parameters in the large language model analyzed by Dodge et al. 2022.
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a numeric energy consumption value for a Meena training run, so the number of runs required to match GPT-3’s energy cannot be determined with confidence."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9)'. Doc [ebert2024]: 'these provisions lack sufficient emphasis on environmental factors... no detailed reporting on mitigation efforts concerning environmental risks is currently required.',"The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, but it does not explicitly mandate that those assessments include environmental risks. The quoted passages indicate the Act mandates risk assessment and mitigation but note that environmental factors are not explicitly required to be included."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: Training energy (kWh) 51,686 25,634 17,052 10,505 Finetuning energy (kWh) 7,571 3,242 1,081 543","The Power Hungry Processing study (Doc [luccioni2024]) lists the training energy for BLOOMz-7B as 51,686 kWh and finetuning energy as 7,571 kWh. Adding these gives a combined cost of 59,257 kWh."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.
q125,What is the total number of parameters in the final FLM-101B model?,101B parameters,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Specifically, we train three models, with 16B, 51B, and 101B parameters, respectively, in a sequential manner.'","The question asks for the number of parameters in the final FLM-101B model. Doc [li2025a] explicitly states that the final model has 101B parameters, so the answer is 101B."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84% of LLM usage,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed.'","The question asks for the percentage of token usage through models that did not disclose environmental impact. Doc [luccioni2025c] explicitly states that 84% of token usage was through models with no disclosure, so the answer is 84%."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents provide a specific freshwater consumption figure for Meta’s Llama 3 inference serving clusters in 2024, so the answer cannot be determined from the given context."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: f model experimentation and evaluation, we used a total of 754.66 kWh of energy","The question asks for the total energy used for model experimentation and evaluation in the 2024 study. Doc [luccioni2024] states that ""we used a total of 754.66 kWh of energy"" for that purpose, which directly provides the requested value."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592'","The question asks for the number of inferences needed for deployment energy to match the initial training and fine‑tuning energy of BLOOMz‑7B. Table 5 in Doc [luccioni2024] lists the cost parity for BLOOMz‑7B as 592,570,000 inferences, which directly answers the question."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Marshall County,Marshall County,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: WV Marshall 1083.8(831.2, 1336.5) 0.77","The table of projected per‑household health costs for 2030 lists West Virginia counties, and Marshall County has the highest value (1083.8), indicating it is projected to have the greatest per‑household health cost."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a specific numeric value for the total carbon emissions avoided by pruning and quantizing large language models in 2023.
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 NVIDIA A100 80GB GPU,1,is_blank,is_blank,is_blank,Doc [samsi2024#0045]: '13B 2 64 1 64' indicates that for the LLaMA-13B model the bare minimum hardware on A100 80GB GPUs is 1 GPU with a maximum batch size of 64.,"The question asks for the bare minimum number of A100 80GB GPUs needed for LLaMA-13B inference without compression or quantization. Doc [samsi2024#0045] lists the bare minimum hardware counts, showing that only 1 A100 80GB GPU is required for the 13B variant, so that is the answer."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention a dataset used for German nuclear waste site objection texts, so the answer cannot be determined from the context."
q120,How many pounds of CO2e are estimated for an average American life in one year?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a statement giving the pounds of CO2e for an average American life in one year, so the answer cannot be determined from the context."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the price per hour for an NVIDIA H20 as reported by Chen et al. (2025), so I cannot answer this question based on the available context."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024]: 'This is equivalent to approximately 44% of the data centers’ total electricity c',"The question asks for the percentage of total electricity cost that the public health cost equates to in 2023. Doc [han2024] states that this percentage is approximately 44%, so that is the answer."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,~3 passengers,3,passengers,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: e   of   84.5%   seat   occupancy,   yielding   1.2t   of   CO 2 e   per   passenger   round   trip.   Thus,   the   CO 2 e   equivalent   of   NAS   is   ~3   passengers   taking   a   round   trip   between   San   Francisco   and   New   York.","The passage states that the NAS emits 3.2 tCO2e and that a single passenger round‑trip SF‑NY emits about 1.2 tCO2e, so dividing 3.2 by 1.2 gives roughly 3 passengers."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24% cost saving,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: 'Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.',"The question asks for the cost savings achieved when mixing A100 and A10G GPUs compared to an A100-only strategy. Doc [griggs2024] explicitly states a 24% cost saving, which directly answers the question."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA A100 80GB GPU,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024#0045]: 'Model Size 7B 1 64' and Doc [samsi2024]: 'The 7B model was run on a single GPU.',"The question asks for the bare minimum number of NVIDIA A100 80GB GPUs needed for LLaMA‑7B inference without compression or quantization. The table in Doc [samsi2024#0045] lists 1 A100 for the 7B model, and Doc [samsi2024] confirms the 7B model was run on a single GPU. Therefore the minimum required is one A100 80GB GPU."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",122%,122,%,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: e public health cost of training a large AI model in selected U.S. data centers.
Location Electricity Price (¢/kWh) Electricity (million$) Health Cost (million$) % of Electricity Cost Emission(Metric Ton) PM2.5(LA-NYC) NOx(LA-NYC) SO2
Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34","The table for Altoona, IA lists the health cost as 2.51 million$ and indicates that this is 122% of the electricity cost. Thus the health cost was 122% of the electricity cost."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'In fact, most carbon foot print analyses gather the information manually by writing to authors.'","The question claims that most analyses gather data automatically, but Doc [luccioni2025b] explicitly states they gather manually by contacting authors, so the statement is false."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",tCO2,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: 'we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).',"The question asks for the estimated range of CO2 emissions for a complete training run of a 6.1 billion‑parameter transformer. Doc [dodge2022] explicitly states that a full run would emit 21 to 78 metric tons of CO2, providing the required range."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.',"The question asks whether sustainable deployment techniques (specifically quantization) demonstrated up to a 45% reduction in carbon emissions. The cited passage from Doc [khan2025] explicitly states that these techniques can reduce emissions by up to 45% after quantization, so the statement is true."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,answers,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers'","The question asks how many answers were obtained after contacting more than 500 authors. Doc [luccioni2025b] explicitly states that 95 answers were collected, which directly answers the question."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"Doc [erben2023]: 'We introduce the granularity metric, the ratio of calculation to communication time'",The question asks for the metric used to assess the calculation-to-communication time ratio in geo‑distributed training; Doc [erben2023] explicitly states that this metric is called the granularity metric.
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36 projects,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: United Kingdom 36 901,"The table of projects announced as of January 2024 lists the United Kingdom with 36 projects, which directly answers the question."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,li2025b,https://arxiv.org/pdf/2304.03271,Doc [li2025b]: Apple reports that its supply chain accounts for 99% of its total water footprint [23].,"The question asks for the percentage of Apple's water footprint that comes from its supply chain. Doc [li2025b] states that the supply chain accounts for 99%, so the answer is 99%."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25T tokens,1250000000000.0,tokens,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: JetMoE-8B is trained on 1.25T tokens of primarily English data,"The question asks for the number of tokens used for pre-training. Doc [shen2024] explicitly states that JetMoE-8B was trained on 1.25T tokens, so the answer is 1.25 trillion tokens."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses.","The question asks for the term describing freshwater extracted from ground or surface sources. Doc [li2025b] defines this term explicitly as ""Water withdrawal,"" so that is the correct answer."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'","The passage from Doc [stone2022] explicitly states that the Standing Committee forms a Study Panel every five years, so the answer is ‘Every five years’ with a numeric value of 5 and unit years."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","1,035,930,000 inferences",1035930000,inferences,dodge2022;luccioni2024,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2311.16863,"Doc [dodge2022#0054]: 'we estimate a full training run would consume approximately 103,593 kWh.'; Doc [luccioni2024]: 'Inference energy (kWh) 1.0 × 10−4' for BLOOMz-7B.","The 6.1B model requires about 103,593 kWh to train fully (Doc [dodge2022#0054]). BLOOMz-7B consumes 1.0×10⁻⁴ kWh per inference (Doc [luccioni2024]). Dividing 103,593 by 1.0×10⁻⁴ yields roughly 1.03593×10⁹ inferences, which is the number needed to match the training energy."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].'","The question asks for the average number of connected devices per U.S. household in 2021. Doc [wu2021b] explicitly states that the average household has 25 connected devices, so that is the reported value."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE project’s training budget or total GPU hours, so I cannot compute a cost per H100 GPU-hour from the available context."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011'","The passage explicitly says Watson beat human contenders, so the claim that it did NOT beat them is false."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The question asks for the maximum cost reduction percentage in conversational chat settings reported by Griggs et al. 2024. Doc [griggs2024] explicitly states a 77% reduction, which directly answers the question."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '... could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year ...',"The coalition of Microsoft employees reported that a single deal with Exxon Mobil could increase emissions by 640 percent relative to Microsoft’s yearly removal targets. 640 percent equals 6.4 times more emissions, which is the answer requested."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: JetMoE-8B-chat 6.681,"The question asks for the MT-Bench score of JetMoE-8B-Chat after alignment. Doc [shen2024] lists the MT-Bench scores and shows JetMoE-8B-chat as 6.681, which is higher than Llama-2-13b-chat’s 6.650, confirming the model surpassed the Llama-2-13b-Chat. The score 6.681 is therefore the answer."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10–50 medium-length completions,"[10, 50]",completions,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.'","The question asks how many medium‑length GPT‑3 completions can be produced using the water from one 500 mL bottle. Doc [li2025b] explicitly states that a 500 mL bottle of water supports roughly 10 – 50 medium‑length responses, so the answer is a range of 10 to 50 completions."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,"Doc [luccioni2025a]: 'one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water' and Doc [li2025b]: 'GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses'","The question asks for the number of GPT‑3 queries that use about half a liter of water. Both documents state that 10 to 50 queries consume roughly 0.5 L, so the answer is the range 10–50 queries."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8 to 3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)'","The question asks for the range of energy consumption in MWh for pre-training an LLM. Doc [luccioni2025c] explicitly states that the energy required spans from 0.8 MWh to 3,500 MWh, providing the requested range."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4 A100 80GB GPUs,4,is_blank,samsi2024;rubei2025,https://arxiv.org/pdf/2310.03003;https://arxiv.org/pdf/2501.05899,Doc [samsi2024]: '8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.',"The question asks for the bare minimum number of A100 80GB GPUs needed. Both Doc [samsi2024] and Doc [rubei2025] state that 4 A100 GPUs are required, so the answer is 4."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80–90%,"[80, 90]",%,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].'","The question asks for NVIDIA’s estimate of inference processing as a percentage of the ML workload in 2019. Doc [patterson2021] explicitly states that NVIDIA estimated 80–90% of the ML workload is inference, which directly answers the question."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg CO2eq,178.97,kg CO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.'","The question asks for the total CO2 equivalent emissions for the entire study. Doc [luccioni2024] explicitly states that the emissions totaled 178.97 kg of CO₂eq, so this value is used as the answer."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,han2024;luccioni2025c,https://arxiv.org/pdf/2412.06288;https://arxiv.org/pdf/2506.15572,"Doc [han2024]: 'training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.' ; Doc [luccioni2025c]: 'training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.'","Both Doc [han2024] and Doc [luccioni2025c] explicitly state that training a Llama-3.1 scale model produces air pollutants equivalent to more than 10,000 LA-NYC round trips, so the answer is 10,000 round trips."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a numeric value for the water used for cooling during OpenAI’s GPT‑4 training run.
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.516,7.516,$/h,is_blank,is_blank,Doc [griggs2024#0064]: 'resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100.',"The question asks for the normalized on‑demand hourly price of an H100 GPU in Griggs et al. (2024). The passage from Doc [griggs2024#0064] explicitly states that the normalized price is $7.516 per hour, so that is the answer."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",52.9%,52.9,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: '46.9%52.9%',"The table in Doc [amazon2023] for Amazon Workforce (All Levels) lists the percentage of men and women in the U.S. workforce. The second value, 52.9%, represents the proportion of employees who identified as men in 2023."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","1,000× larger",1000,×,is_blank,is_blank,"Doc [wu2021a#0013]: 'with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.'","The question asks how many times larger a GPT‑3‑based model must be to raise BLEU from 5 to 40. Doc [wu2021a#0013] states that this requires a 1,000× larger model, so the answer is 1,000× larger."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,chung2025,https://arxiv.org/pdf/2505.06371,Doc [chung2025]: 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.',"The question asks whether using TDP to estimate GPU energy consumption is reliable and accurate. Doc [chung2025] explicitly states that TDP‑based estimates are typically an overestimation and thus not accurate, which contradicts the claim of reliability and accuracy."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emissions value (626,155 lbs) for training and NAS, but they do not provide the emissions‑to‑driving‑distance ratio needed to compute the equivalent miles. Therefore the requested driving distance cannot be determined from the given context."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025#0092]: 'GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.' 
Doc [jegham2025]: 'GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.'","The question asks whether GPT-4o mini uses less energy per query than GPT-4o. The cited passages show GPT-4o mini consumes more energy (20% more on long queries and 3.098 Wh vs 2.875 Wh), so the statement is false."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",14.9 queries/sec,14.9,qps,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 14.9 Dense(bsz=1) Dense(bsz=6) Sparse(bsz=1) Sparse(bsz=6) Sparse(bsz=20),"The figure caption indicates Mixtral-CS throughput, and the quoted line lists 14.9 as the throughput for Dense(bsz=1), which is the ground‑truth throughput for dense Mixtral-CS‑A100‑40GB at batch size 1."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$7.22 per hour,7.22,$/hour,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.'","The document states a monthly cost of over $5,200 for the two A100 GPUs. Dividing $5,200 by 30 days × 24 hours (720 hours) gives about $7.22 per hour, which is the estimated hourly cost to run the model."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a numeric total execution time for a sparse BlackMamba model fine‑tuned on an NVIDIA A40-48GB with batch size 84, so the answer cannot be determined from the supplied context."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,H100 GPU hours,is_blank,is_blank,"Doc [shen2024#0001]: 'using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.' Doc [shen2024#0008]: 'using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.'","Both documents explicitly state that the JetMoE-8B model was trained using 30,000 H100 GPU hours, which directly answers the question."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.'","The question asks for the cost threshold that the largest training runs will exceed by 2027. Doc [cottier2024] explicitly states that such runs will cost more than a billion dollars by that year, so the answer is one billion dollars."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.'","The paper notes that after 2022 the trend reversed, with disclosures decreasing, so the statement that it continued to increase is false."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.'","The question asks for the GPU hours needed to train FAIR’s RoBERTa on 160GB of text. Doc [schwartz2019] explicitly states that this training required around 25,000 GPU hours, which is used as the answer."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 V100 GPUs,8,V100 GPUs,rubei2025;samsi2024,https://arxiv.org/pdf/2501.05899;https://arxiv.org/pdf/2310.03003,Doc [rubei2025]: '8 V100 GPUs each with 32 GB of RAM ... are required for any meaningful inferences with the 65B LLaMA model.',Both Doc [rubei2025] and Doc [samsi2024] state that the bare minimum configuration for LLaMA‑65B inference without compression or quantization is 8 NVIDIA V100 32GB GPUs.
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.,The document states that 24 servers each contain 8 A800 GPUs; multiplying 24 by 8 gives 192 GPUs in total for training FLM-101B.
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a numeric value for AlexNet’s top‑1 accuracy on ImageNet, so I cannot answer this question based on the available context."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emissions estimate for NAS (284 metric tons or 626,155 pounds) but do not give a comparison to average American lifetimes, so I cannot answer the second part of the question."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].,"The question asks for GPT‑3’s total floating point operations. Doc [patterson2021] explicitly states that OpenAI published the figure as 3.14E+23, so that is the value reported."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents specify the amount of water consumed per ChatGPT user session in 2023, so the answer cannot be determined from the given context."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific numeric factor for the increase in energy consumption of the Llama 3.1 70B model when deployed on two nodes versus one.
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.'","The question asks for the PUE value. Doc [patterson2021] explicitly states that the PUE for the Iowa datacenter during the Evolved Transformer run was 1.11, so that value is returned as the answer."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Yelp sentiment analysis benchmarks or compare traditional models to large language models, so the answer cannot be determined from the context."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,U.S. homes,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset.'","The question asks how many U.S. homes the annual electricity use of 700 million daily GPT‑4o queries would match. Doc [jegham2025] explicitly states this equals 35,000 U.S. homes, so that is the answer."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons of CO2e",47400,tCO2e,is_blank,is_blank,"Doc [amazon2023#0170]: 'avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.'","The question asks for the annual CO2e avoided by Amazon’s on‑site solar energy systems. Doc [amazon2023#0170] explicitly states that these systems avoid roughly 47,400 metric tons of CO2e each year, which is the figure used in the answer."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the sam","The question asks for the reported percentage increase in global water consumption. Doc [luccioni2025a] explicitly states Microsoft reported a 34% increase, so that is the answer."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,Megatron-LM,Megatron-LM,is_blank,fernandez2025,https://arxiv.org/pdf/2504.17674,Doc [fernandez2025]: 'Efficient large-scale language model training on gpu clusters using megatron-lm.',"The question asks for the framework used to deploy large language models across multiple GPUs and nodes. Doc [fernandez2025] explicitly states that Megatron-LM was used for efficient large‑scale language model training on GPU clusters, indicating that it serves as the deployment framework across GPUs and nodes."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'yielding a total of approximately 772 billion GPT-4o queries in 2025',"The question asks for the estimated total GPT-4o queries in 2025. Doc [jegham2025] explicitly states that the analysis projects approximately 772 billion queries for that year, so the answer is 772 billion queries."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the number of AI training runs conducted globally on renewable-only power in 2022, so I cannot answer this question based on the available context."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.'","The question asks for the KV Cache size at batch size 32 for the OPT-2.7B model. Doc [kim2025] explicitly states that the cache expands to 5.312GB at that batch size, so that is the answer."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29%–49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.'","The question asks for the percentage range of R&D staff costs (including equity) relative to the total amortized cost for the four models studied by Cottier et al. The cited passage from Doc [cottier2024] gives the exact range of 29% to 49%, which is used as the answer."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'The US national datacenter average in 2018 was 1.58, which is the value [Str19] used ; In 2020, it was 1.59 .'","The question asks for the US national datacenter average PUE in 2020. Doc [patterson2021] explicitly states that in 2020 the average was 1.59, so that is the answer."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024#0089]: 'at the time of writing this article (November 2023), BLOOMz-7B has been downloaded 606,096 times' ; Doc [luccioni2024]: 'Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5'","Doc [luccioni2024] gives the per‑inference energy for BLOOMz‑7B as 1.0×10⁻⁴ kWh. One million inferences then use 100 kWh. Doc [luccioni2024#0089] states 606,096 downloads, so total energy is 606,096 × 100 kWh = 60,609,600 kWh, which equals 60,609.6 MWh."
q213,Which software package was used to measure energy consumption during inference runs?,Code Carbon,Code Carbon,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [luccioni2024#0034]: 'used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference' and Doc [morrison2025]: 'measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking'.",The question asks for the software package used to measure energy consumption during inference runs. Both Doc [luccioni2024#0034] and Doc [morrison2025] explicitly state that the Code Carbon package was employed for this purpose.
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,Doc [kim2025#0001]: 'Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.',"The question asks for the name of the function that improves instance selection accuracy by calibrating theoretical and actual GPU performance. Doc [kim2025#0001] explicitly names this function as the Compute Time Calibration Function (CTCF), so that is the answer."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: '3) Open‑source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].'","The question asks whether open‑source general‑purpose AI models are exempt from reporting energy consumption unless they pose systemic risk. Doc [ebert2024] states that such models are largely excluded from transparency requirements unless they present a systemic risk, which directly supports the statement. Hence the answer is TRUE."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",almost 30%,30,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]'.","The question asks for the percentage of PPAs in 2020 held by Amazon, Microsoft, Meta, and Google. Doc [luccioni2025a] explicitly states that they accounted for almost 30% of all corporate PPAs, so the answer is 30%."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",15-77%,"[15, 77]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: Short-context Dataset (Arena). In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO)","The question asks for the cost‑reduction range for short‑context workloads on the Arena dataset with a 120ms SLO. Doc [griggs2024] explicitly states that Mélange achieves a 15‑77% cost reduction in this scenario, so that is the reported range."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,TRUE,1,is_blank,is_blank,is_blank,"Doc [samsi2024#0060]: 'Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.'","The context states that increasing the number of GPU shards tends to increase the energy cost per inference response, so the statement is True."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide a specific numeric value for the net carbon emissions of FLM-101B’s pre‑training, so the answer cannot be determined confidently."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: . For each article, we noted whether it mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by acknowledging uncertainty or suggesting that such statistics should be viewed critically. Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it con","The question asks for the percentage of 100 news articles that cited the contested 3 Wh/10‑times‑Google‑search estimate. The passage from Doc [luccioni2025c] explicitly states that 53% of articles cite that figure, providing the required answer."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: '3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].'","The question asks whether open‑source general‑purpose AI models must report energy consumption. Doc [ebert2024] explicitly states that OS GPAI models are largely excluded from transparency requirements, meaning they are not required to report energy consumption to authorities. Hence the statement is false."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.0022 kL,0.0022,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.'","The passage states that mining the rare‑earth content for a single H100 consumes 2.2 liters of water. Converting 2.2 liters to kiloliters gives 0.0022 kL, which is the required answer."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'Table 1 shows the public health cost of U.S. data centers from 2019 to 2023 as a reference. Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.'","The question asks for the total public health cost in 2023. Doc [han2024] directly states that figure as about $6.7 billion, which is the value to report."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 2
TruthfulQA 38.8 36.1 33.1 41.7
WinoGrande 74.0 73.7 66.3 70.2
GSM8k 14.5 17.3 16.9 27.8
OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0
MBPP (Pass@1) 20.8 34.0 28.0 34.2
HumanEval (Pass@1) 12.8 25.0 24.4 14.6
All Avg. 45.5 47.3 43.2 47.6
Table 3: OpenLLM leaderboard and code benchmarks results from four different models.","The table lists the OpenLLM Leaderboard averages for four models. The last value, 53.0, corresponds to JetMoE-8B’s average score, so the final average score for JetMoE-8B is 53.0."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.'","The question asks whether public health costs of AI are evenly distributed; the cited passage states they are highly unevenly distributed, so the correct answer is FALSE."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Doc [khan2025]: 'We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.'","The question asks which open-source tool was used for 4‑bit quantization and local deployment. Doc [khan2025] explicitly states that quantization was applied through Ollama, making it the correct answer."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024',"The question asks which senator introduced the AI Environmental Impacts Act bill in February 2024. Doc [ebert2024] explicitly states that Senator Edward J. Markey introduced the bill on 1 Feb 2024, providing the required answer."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Backblaze B2,Backblaze B2,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"Doc [erben2023]: 'we chose an independent S3 storage provider, Backblaze (B2) [4].'","The question asks which storage service was used for sharding and streaming datasets on spot VMs. Doc [erben2023] states that the authors chose Backblaze B2, an independent S3-compatible storage provider, for this purpose, so the answer is Backblaze B2."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].'","The question asks whether GPU theoretical performance per watt doubles every 3-4 years according to 2019 product data. Doc [wu2021b] states exactly that GPU theoretical performance per watt doubles every 3-4 years, citing Sun et al., 2019, matching the premise, so the correct answer is True."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the price per hour for an NVIDIA H100 according to Chen et al. (2025).
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,3.7 years,3.7,years,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.'","The question asks for the estimated average GPU lifetime before retirement in AI data centers. Doc [cottier2024] explicitly states that the expected hardware lifetime would be 3.7 years, so that is the value provided."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents explicitly state that the relationship between runtime and energy consumption during LLM inference is nearly linear, so I cannot confidently answer the question based on the available context."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 NVIDIA V100 32GB GPUs,2,GPUs,is_blank,is_blank,Doc [samsi2024#0045]: '13B 2 64' in the table of bare minimum hardware requirements for V100 32GB GPUs,"The table in Doc [samsi2024#0045] lists the bare minimum hardware needed for each LLaMA variant; for the 13B model it shows a requirement of 2 V100 GPUs, so the minimum number of V100 32GB GPUs needed is 2."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy— a goal that Amazon, including AWS, achieved in 2023.'","The question asks for the typical percent reduction in carbon footprint when moving workloads to AWS in North America. Doc [amazon2023] explicitly states that AWS can lower customers’ workload carbon footprints by up to 96% in that region, which directly answers the question."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: Model Cost (zettaFLOPs) ... FLM-101B 28.22 ...,"The cost table in Doc [li2025a] lists the estimated floating‑point operations for training the 101B model as 28.22 zettaFLOPs, which corresponds to the total computational work of the final 101B training stage."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, over 4x the five cars estimate",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the ""five cars"" number.'","The context states that Gemma’s pre‑training emitted 1247.61 tCO2e and that this figure is over four times larger than the five‑cars estimate, so the answer reflects both the value and the comparison."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.'","The question asks for the U.S. national average water consumption for electricity generation. Doc [li2025b] explicitly states it is estimated at 3.1 L/kWh, so that is the answer."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: '... cost of $3460.',"The question asks for the net cost of fine‑tuning a sparse Mixtral model with 2 million queries on an NVIDIA H100 GPU. Both Doc [xia2024] and Doc [xia2024#0093] state that the predicted cost is $3460, so that is the answer."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,is_blank,is_blank,Doc [strubell2019#0022]: Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).,"The question asks for the training duration of ELMo on 3 NVIDIA GTX 1080 Ti GPUs. The only explicit training time provided in the context is 2 weeks (336 hours) for ELMo trained on 3 NVIDIA GTX 1080 GPUs, which matches the hardware configuration requested. Therefore the answer is 336 hours."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,The energy consumption of the o3 (GPT‑4o) model for a long prompt is about 3.5 times higher than that of GPT‑4.1 nano.,3.48,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'GPT‑4o consumes around 2.875 Wh' and Doc [jegham2025]: 'GPT‑4.1 nano remains among the most efficient proprietary models at 0.827 Wh',"The question asks for the factor by which the o3 model’s energy usage exceeds GPT‑4.1 nano’s for a long prompt. Doc [jegham2025] gives the long‑prompt energy of GPT‑4o as 2.875 Wh and GPT‑4.1 nano as 0.827 Wh. Dividing 2.875 by 0.827 yields approximately 3.5, so the o3 model consumes about 3.5 times more energy."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: We conduct training on a cluster containing 12 nodes and 96 H100s.,"The question asks for the total number of H100 GPUs used in JetMoE-8B training. Doc [shen2024] states that the cluster had 12 nodes and 96 H100s, so the total GPUs is 96."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50–70% of the total provisioned power,"[50, 70]",%,chung2025,https://arxiv.org/pdf/2505.06371,"Doc [chung2025]: 'GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].'","The question asks for the percentage of total provisioned power that GPUs use. Doc [chung2025] explicitly states GPUs account for 50–70% of that power, so the answer is that range."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10 PUE,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,Doc [dodge2022]: 'Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021',"The question asks for Google’s hyperscale data center PUE in 2021. Both Doc [dodge2022] and Doc [wu2021b] report that Google’s PUE was 1.10 for the 12 months ending in Q1 2021, so 1.10 is the correct answer."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025#0084]: 'When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W'","The question asks for the average GPU power during the first 300 logging steps of OLMo 2 7B training while actively training. In Doc [morrison2025#0084] the text explicitly states that the average GPU power is over 600 W, which directly answers the question."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)',"The question asks for the energy consumption of a single short GPT-4o query. Doc [jegham2025] explicitly states that such a query consumes 0.42 Wh, so that value is reported as the answer."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",about 280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.'","The question asks for the percentage difference in cost between the Max-Performance instance and InferSave’s top choice for 400 TPS online workload. The context explicitly states that the Max-Performance g6e.xlarge costs $2.699, which is about 280% more expensive than InferSave’s top choice costing $0.71, so the answer is ~280%."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a statement or estimate of the CO2e per person per year globally, so the question cannot be answered with confidence from the available context."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,is_blank,is_blank,Doc [patterson2021#0059]: Processor Average (Watts) ... TPU v2 221 ... V100 GPU 325,"The table in Doc [patterson2021#0059] gives the average system power per processor as 221 Watts for TPU v2 and 325 Watts for V100. Subtracting 221 from 325 yields a difference of 104 Watts, which is the answer."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).','In addition, we propose reporting the ﬁnancial cost or “price tag” of developing, training, and running models to provide baselines for the investigation of increasingly efﬁcient methods.'","The question asks whether Green AI includes providing the financial cost of finding, training, and running models. Doc [schwartz2019] explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice, and also proposes reporting the financial cost of developing, training, and running models. Therefore the statement is true."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20×,20,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021.,"The document explicitly states that Facebook’s recommendation and ranking model sizes grew 20× over the period 2019 to 2021, providing a direct numeric answer."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: ning the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The question asks for the amount of clean freshwater directly evaporated by training GPT‑3 in Microsoft’s U.S. data centers. Doc [li2025b] explicitly states that it can directly evaporate 700,000 liters of clean freshwater, providing the required value."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,Approximately 1.25× speedup in inference throughput for LLaMA-13B on A100 vs V100,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The question asks for the speedup in inference throughput for LLaMA‑13B when using A100 compared to V100. Doc [samsi2024] explicitly states a 1.25‑times increase in throughput for 13B, so the answer is 1.25×."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256.'","The context window size for FLM-101B is explicitly stated as 2,048 tokens in Doc [li2025a]."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,is_blank,is_blank,"Doc [jegham2025#0148]: 'OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825)…'","The question asks which model ranked highest in the DEA eco‑efficiency analysis. Doc [jegham2025#0148] states that o3‑mini achieved the highest cross‑efficiency score (0.884), so it is the top‑ranked model."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'AI’s expanding operational footprint also contributes to electronic waste (e‑waste), which is now the fastest‑growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'","The question asks for the total amount of electronic waste worldwide in 2022. Doc [luccioni2025a] explicitly states that e‑waste reached 62 million tonnes in that year, so 62 million tonnes is the answer."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85–134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,Doc [li2025b#0008]: 'a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027',"The question asks for the projected electricity consumption range for global AI in 2027. Doc [li2025b#0008] explicitly states a range of 85–134 TWh, which is the value used in the answer."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million packages,150,million packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: ""Europe • We delivered 150 million packages via EVs.""","The question asks for the number of packages delivered via EVs in Europe in 2023. Doc [amazon2023] explicitly states that 150 million packages were delivered via EVs in Europe, so that is the answer."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61–76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The passage explicitly states that when equity is excluded, computing hardware accounts for 61–76% of the total amortized cost for the four key models, answering the question directly."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents explicitly state which GPU architecture is most energy-efficient for models generating only a single classification token, so I cannot answer this question with confidence."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: 'For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;'","The question asks for the maximum potential CO2 emissions reduction for DenseNet 201 in West US under Flexible Start. Doc [dodge2022] explicitly states that the reduction can be up to 80% in West US, so 80% is the answer."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific numeric value for the total execution time of a sparse Mixtral model with batch size 1 on an NVIDIA A40 GPU.
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954 pt (2).","The question asks for the average CO2 per kWh in pounds as reported by the EPA. Doc [strubell2019] states that the EPA average is 0.954 pounds per kWh, so that is the answer."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Yelp sentiment analysis benchmarks or compare traditional models’ accuracy to large language models, so the answer cannot be determined from the context."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. ... The workload evaluates a total of 3000 requests.'","The online inference workload uses 128 input tokens and 512 output tokens per request, giving 640 tokens per request. With 3000 requests, the total tokens processed are 640 × 3000 = 1,920,000 tokens."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,is_blank,is_blank,Doc [schwartz2019#0060]: 'Amazon’s AWS only covered fifty percent of its power usage with renewable energy.',"The question asks for the percentage of AWS power usage covered by renewable energy in 2018. The only statement providing a percent in the context is from Doc [schwartz2019#0060], which states that AWS covered fifty percent of its power usage with renewable energy, which matches the requested year."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,True,1,is_blank,chung2025,https://arxiv.org/pdf/2505.06371,"Doc [chung2025]: 'It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations per byte of memory loaded is low...'
Doc [chung2025]: 'Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.'","The documents state that LLMs draw much less power than diffusion models and that this is due to LLM decoding being less compute‑intensive and limited by VRAM bandwidth, confirming the statement is true."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b#0053]: 'develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].' Doc [wu2021b]: 'Sustainability targets and the associated regulations will continue to increase, and hardware must be manufactured with less planetary impact, use less energy while in operation, and produce less e-waste at the end of life [Orcuttarchive, 2015, Chang et al., 2017].'","The statement is true because Doc [wu2021b#0053] states that smartphones currently average less than 3 years, and Doc [wu2021b] notes that such short lifetimes contribute to e-waste concerns."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",over 1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,Doc [luccioni2024]: '...can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.',"The 2024 study reports that image generation consumes 2.9 kWh per 1,000 inferences while text classification consumes 0.002 kWh. The text explicitly states that this difference is a factor of over 1450, which directly answers the question."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,False,0,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"Doc [erben2023]: 'CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)'","The context reports near‑linear per‑GPU speedup for CV only in the transatlantic experiments, not in intra‑zone scaling with T4 GPUs. Since the statement refers to intra‑zone scaling, the claim is not supported and is therefore false."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024;luccioni2025c,https://arxiv.org/pdf/2410.06681;https://arxiv.org/pdf/2506.15572,"Doc [ebert2024]: '5) The AI Act fails to address the greenhouse gas (GHG) emis‑sions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].'","The question asks whether the AI Act mandates disclosure of GHG emissions for AI applications such as oil and gas exploration. The cited passages from Doc [ebert2024] and Doc [luccioni2025c] state that the AI Act fails to address these emissions, indicating no such mandate. Therefore the answer is False."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244 projects,244,projects,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 787 United Arab Emirates 1 3 United Kingdom 36 901 United States 244 17,706 Total 513 27,993","The table in Doc [amazon2023] lists 244 projects in the United States as of January 2024, which directly answers the question."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the amount of fiber optic cable installed globally for AI workloads in 2023, so the answer cannot be determined from the given context."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: 'The GPU alone accounts for 74% of the total energy consumption',"The question asks for the percentage of total electricity consumption attributed to the GPU during a BERT‑base training experiment. Doc [dodge2022] explicitly states that the GPU alone accounts for 74% of the total energy consumption, so 74% is the correct answer."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,Cumulative server level,Cumulative server level,is_blank,is_blank,is_blank,"Doc [ebert2024#0003]: 'for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level.'","The question asks which measurement level the authors recommend for reporting AI energy consumption. Doc [ebert2024#0003] explicitly recommends the cumulative server level, and several other documents corroborate this recommendation, justifying the answer."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,GPUs,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: 'For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.'","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, so the answer is 2 GPUs."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48% increase,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: 'Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to increases in data center energy consumption.',"The question asks for the percentage increase in GHG emissions reported in Google’s 2024 environmental report. Doc [luccioni2025a] explicitly states a 48% increase, so that is the answer."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: n Recall F1 Accuracy CO2 (kg) Before Optimization Baseline metrics for comparison Llama 3.2 0.55 0.45 0.44 0.45 0.012 Phi 3.2 0.97 0.82 0.88 0.82 0.012 Qwen 0.77 0.79 0.76 0.79 0.009 Mistral-small 0.70 0.67 0.65 0.67 0.020 Llava-Llama 3 0.58 0.50 0.48 0.50 0.014 After Optimization Metrics following quantization and local inference techniques Llama 3.2 0.57 0.48 0.47 0.48 0.005 Phi 3.2 1.00 0.84 0.91 0.84 0.007 Qwen 0.80 0.81 0.80 0.81 0.004 Mistral-small 0.73 0.70 0.69 0.70 0.015 Llava-Llama 3 0.61 0.54 0.5,"The table in Doc [khan2025] shows that for every model listed, both F1 and accuracy increased from the before‑optimization row to the after‑optimization row, indicating that in the financial sentiment case study these metrics always improved."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention McKinsey projections for 2030 data center electricity consumption.
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping consumes less energy,Swapping,is_blank,chung2025,https://arxiv.org/pdf/2505.06371,"Doc [chung2025]: it can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","The question asks which preemption mechanism consumes less energy when overloaded. Doc [chung2025] explicitly states that Swapping consistently consumes less energy, so the answer is Swapping."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,about 70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'reducing inference computation by about 70% compared to Llama2-7B.',"The question asks for the percentage reduction in inference computation. Doc [shen2024] explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, so 70% is the answer."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800M,800000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The question requests the upfront hardware acquisition cost for GPT‑4. Doc [cottier2024] explicitly states that the acquisition cost was estimated at $800M, which directly answers the question."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the environmental sustainability of AI approaches themselves.'","The passage from Doc [luccioni2025b] states that Sustainable AI was proposed to encompass both using AI in climate-positive applications and improving AI’s own environmental sustainability, not just the former. Therefore the statement is false."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5% reduction,28.5,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).',"The question asks for the total operational energy footprint reduction at Facebook over 2019‑2021. Doc [wu2021a] explicitly states a 28.5% reduction, which is the value used in the answer."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'the seminal 2019 article by Strubell et al. which quantified the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].'","The question asks for the carbon footprint in lbs CO2e of training BERT from the seminal 2019 study. Doc [luccioni2025b] explicitly states 626,155 pounds of emissions, so that is the answer."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",True,1,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"Doc [erben2023]: rity tasks like CV, even distributing VMs over four continents only slows down performance by 7%","The passage from Doc [erben2023] explicitly states that intercontinental training for CV models slows performance by only 7% compared to local training, confirming the statement as true."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 'the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.'","The question seeks the ground truth maximum batch size for fine-tuning Mixtral on an NVIDIA A100-40GB GPU. The cited passage from Doc [xia2024] states that the maximum batch size for Mixtral on the A100‑40GB GPU is 28 samples, making 28 the correct answer."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 gCO2eq,0.32,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries'","The question asks for the emissions of bert-base-multilingual-uncased-sentiment per 1,000 queries. Doc [luccioni2024] states the value is 0.32g CO2eq, so that is the answer. "
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,li2025b;jegham2025;patterson2021,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2104.10350,"Doc [li2025b]: 'GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].'\nDoc [jegham2025]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity...'.\nDoc [patterson2021]: 'its energy consumption is 1287 MWh.'","The question asks for the estimated training energy of GPT‑3 in MWh. All cited documents report 1287 MWh, so the answer is 1287 MWh."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.',"The passage explicitly states that the MoE layer is the prime target for optimization in LLM fine‑tuning, so the statement is true."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide any information about AlphaFold servers' freshwater consumption in 2023.
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.'","The question asks for the year in which direct environmental disclosures peaked. Doc [luccioni2025c] states that this peak occurred in 2022, so 2022 is the correct answer."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",26.1%,26.1,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: 16.6% 31.9%26.1%23.5%,"The question seeks the 2023 percentage of Amazon People Managers who identified as women. The table in Doc [amazon2023] lists 26.1% for 2023 in the Women column for People Managers, providing the required value."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any projection for the public health burden of U.S. data centers in 2030, so I cannot answer this question based on the available context."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",7–26 thousand grams,"[7, 26]",thousands of grams,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: '7k grams vs. 26k grams, for the most efficient vs. least efficient regions'.","The document reports the emissions for the most efficient region as 7k grams and for the least efficient as 26k grams, which corresponds to a range of 7 to 26 thousand grams."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2 samples,2,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20,"The document’s Table III lists the maximum batch sizes for each model‐dataset combination. For Mixtral in dense mode (Mixtral-D) on the CS dataset—which corresponds to Hellaswag—the table gives a batch size of 2. Thus, the maximum batch size supported is 2 samples."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the number of hectares occupied by new AI data centers globally in 2022, so the answer cannot be determined from the context."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,$32.7,32.7,$,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE TABLE IV shows A40 48GB cost $32.7,"The question asks for the total cost on an NVIDIA A40-48GB GPU. Table IV in Doc [xia2024] lists the total cost for that GPU as $32.7, which directly answers the query."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific numeric value for the total energy consumption of training the FLM-101B model.
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.',"The question asks if GPU-level power monitoring is recommended as the preferred method. Doc [ebert2024] explicitly states it is not recommended for overall energy measurements, so the correct answer is FALSE."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a numeric total execution time for a sparse Mixtral model fine‑tuned on an NVIDIA A40‑48GB with batch size 10.
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.',"The passage from Doc [xia2024] states that adding compute resources reduces cost, so the claim that it can increase costs is false."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons CO2,8.3,tCO2,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: 'one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)'","The question asks for the CO2 emission for one year of average US home energy use. Doc [dodge2022] explicitly states that this is estimated at 8.3 metric tons CO2 per year, which directly answers the question."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA V100 32GB GPU,1,GPU,is_blank,is_blank,Doc [samsi2024#0045]: 'Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 ...',"The table in Doc [samsi2024#0045] lists the bare minimum hardware for each LLaMA variant; for the 7B model it shows a count of 1 V100 32GB GPU, indicating that a single V100 is sufficient for inference without compression or quantization."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: GSM8k 14.5 17.3 16.9 27.8,"The table from Doc [shen2024] lists GSM8k scores for the four models; the highest value, 27.8, corresponds to JetMoE-8B, matching the statement that JetMoE-8B outperforms the others on most tasks."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'training accounted for only half of the model’s overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.'","The 2023 article is cited in Doc [luccioni2025b], which states that training accounted for only half of the BLOOM model’s overall emissions, i.e., 50%."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",28 samples,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: ""1200 5 10 15 20 25 30 35 40 A100-40GB A100-80GB A40 H100 bsz=28 bsz=35 Projected GPU capacity Ground Truth Projection Max batch size GPU DRAM capacity Fig. 13.""","The question asks for the batch size of the longest-running MoE layer for a sparse Mixtral model fine‑tuned on an NVIDIA A40‑48 GB GPU. In the figure caption for Fig. 13, the A40 line lists the maximum batch size as bsz=28, which corresponds to the longest-running MoE layer under those conditions."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",19% emissions saving,19,%,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: P&R 19.0%,"The table in Doc [dodge2022] lists the emission reductions for each model under the Pause and Resume optimization. For the 6B parameter transformer, the maximum recorded saving is 19%, which is the highest value in that column."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",Approximately 16.7 requests,16.7,requests,li2025b,https://arxiv.org/pdf/2304.03271,Doc [li2025b]: 'Arizona 1.180 1.630 4.959 2.098 7.531 9.629 6.520 23.406 29.926 16.7',"The table in Doc [li2025b] lists the number of requests needed to consume a 500ml bottle of water for each location. For Arizona, the value is 16.7."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",6205 days,6205,days,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years.'","The question asks for the water usage equivalent, in days, for training the OLMo 60M model. The context states that training the series of models consumed water equivalent to about 17 years for an average U.S. person. Converting 17 years to days (17 × 365) yields 6205 days."
