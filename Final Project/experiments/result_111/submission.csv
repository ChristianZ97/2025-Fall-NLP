id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts mention the number of AWS data centers that began using recycled water for cooling in 2023, so the answer cannot be determined from the supplied documents."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts contain a specific CO₂e value for a single passenger round‑trip between San Francisco and New York, so the answer cannot be determined from the documents alone."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the Amazon Solar Farm Maryland-CPV Backbone or the equivalent number of cars that could be taken off the road. Therefore, the answer cannot be determined from the available documents."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold improvement",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The document states that moving from the Intel 4004’s 740 kHz to a typical 2021 microprocessor’s 5,000,000 kHz yields a >6,750‑fold increase, which directly answers the question."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,morrison2025,https://arxiv.org/pdf/2503.05804,"""NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as , or 3700 kg of COeq per 8x server node, equal 463 kg per GPU.""","The passage from Morrison 2025 explicitly states that the estimated embodied carbon emissions are 463 kg of CO₂-equivalent per GPU, based on a 3700‑kg per 8‑node estimate. This provides a clear numeric answer for the question."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied context or metadata contains a figure for the permitted annual emission limits of nitrogen oxides from data center backup generators in northern Virginia for the specified period. Therefore the answer cannot be determined from the provided documents.
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,luccioni2023,https://arxiv.org/pdf/2302.08476,"""total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs).""","The context from Doc [luccioni2023] explicitly states that GPT‑3’s training took 14.8 days when run on 10,000 V100 GPUs, which directly answers the question."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""",The passage states that the growth strategy saved 72% of the training time compared to a from‑scratch approach for the 101B model.
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context contains incomplete placeholders and does not provide a concrete numeric final average performance score for FLM-101B on the Open LLM Leaderboard.
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""the total energy consumption of the US data centers increased by about 4 from 2010-2014, compared with the estimated 24 increase from 2005-10 and nearly 90 increase from 2000-05.""",The quoted passage from Wu 2021b states the U.S. data center electricity consumption rose by roughly 4% between 2010 and 2014. This directly provides the average increase requested.
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the launch year of the One Hundred Year Study on Artificial Intelligence, so a confident answer cannot be derived from the documents."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,cottier2024;li2025a,https://arxiv.org/pdf/2405.21015;https://arxiv.org/pdf/2309.03852,"""OpenAI's GPT-4 at 40M""; ""FLM-101B, an open-sourced LLM that is successfully trained from scratch within a 100,000 budget.""","The amortized training cost of GPT-4 is 40 million dollars, while the total training budget for FLM-101B is 100,000 dollars. Dividing 40,000,000 by 100,000 yields a factor of 400, indicating GPT-4’s cost was 400 times greater than FLM-101B’s budget."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give a projected number of premature deaths for 2028 (approximately 1,300) but do not contain any explicit estimate for the year 2030. Therefore, the answer cannot be determined from the available information."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the JetMoE-8B architecture or the number of experts per MoE layer. Therefore, a confident answer cannot be given."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,The UN’s Global E-Waste Monitor 2024 showed that about 22 of e-waste has been shown to be formally collected and recycled.,"The context explicitly states that roughly 22% of e‑waste is formally collected and recycled, providing the numeric value and percentage needed for the answer."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"""This model was not trained to completion, but only until 13; a full training run would take 60 days.""","The quoted sentence from Dodge et al. (2022) explicitly states that a full training run of the 6.1 billion‑parameter model would take 60 days, which directly answers the question."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40-48 GB GPU. No sentence or table gives this exact figure, so the answer cannot be determined from the documents alone."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context excerpts contain placeholders where the numeric values for the English FLOP cost of FLM-101B should appear, but the actual figures are missing. Therefore the documents do not provide a clear, extractable answer."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""To be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or multi-task models, to allow comparisons both for differen""","The quoted passage from the Power Hungry Processing study states that 88 models were sampled and analyzed, directly answering the question."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)","The quoted passage lists the range of GPU energy consumption for 1,000 queries and identifies Command‑R Plus as the model with the highest consumption (over 3,426 Wh), indicating it has the highest GPU energy usage among those enumerated in the appendix of the 2025 study."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not contain a specific energy consumption value for pre‑training the BLOOM (176B) model, so a confident answer cannot be provided."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.005 kWh,0.005,kWh,is_blank,is_blank,"Table shows significant reductions in carbon emissions across all models, with some achieving up to 45 after optimization. These results demonstrate the eff… Llama 3.2  0.57  0.48  0.47  0.48  0.005","The table in Doc khan2025#0042 lists the estimated GPU energy consumption for each model; the last column for Llama 3.2 1B is 0.005 kWh, which is the total GPU power usage for the 2400‑prompt SGLang benchmarking scenario at 8 req/s."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",6.6 billion cubic meters,6.6,billion cubic meters,jegham2025,https://arxiv.org/pdf/2505.09598,"""AI-related withdrawals could reach 6.6 trillion liters annually by 2027.""","The document states 6.6 trillion liters; converting liters to cubic meters (1,000 liters = 1 m³) gives 6.6 billion cubic meters. Therefore the projected global AI demand is 6.6 billion cubic meters."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x (median),2.2,is_blank,is_blank,is_blank,"""Based on this, we sampled the factor from a log-normal distribution with a 90 CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run. ... we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x."", ""We estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x."",","The cited passages state that total compute for model development ranges from 1.2x to 4x the final training run, with a median ratio of 2.2x. Thus the median multiple is 2.2, which is the value reported by Cottier et al. (2025)."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a specific statement regarding the percentage of Switch Transformer parameters activated per token.
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time‑saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""","The passage explicitly states that the total wall‑clock time for training FLM‑101B with the growth strategy is 21.54 days, providing the numerical answer and unit."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,Approximately 103.6 MWh,103.6,MWh,is_blank,is_blank,"Doc [dodge2022#0049]: ""we estimate a full training run would consume approximately 103,593 kWh.""  Doc [dodge2022#0052]: ""full training run would take 60 days. Thus, we estimate the total energy consumption to train this model.""","The 103,593 kWh figure (≈103.6 MWh) directly states the estimated energy for a full run, and is derived from the partial run data in Doc [dodge2022#0052] scaled to 60 days, confirming the same estimate."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""AI Energy Score , a project aiming to establish a unified approach for comparing the inference efficiency of AI models.""","The quoted passage explicitly names the collaborative project as ""AI Energy Score"" and states its goal to provide a unified, standardized method for comparing inference efficiency across AI models."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity""","The provided context from Doc [jegham2025] explicitly states that GPT-3’s training consumed 1,287 MWh of electricity. This directly answers the question with the specified unit."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about the JetMoE-8B model or the number of experts selected per token in its layers.
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context discusses diminishing returns and the definition of Red AI, but does not state whether Red AI is on the decline. Therefore the documents do not contain enough information to answer the question confidently."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any explicit numeric value for the execution time (in microseconds) of the longest kernel of the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48 GB GPU. Therefore a confident answer cannot be derived from the supplied documents.
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""the global carbon emissions for 2020 dropped by 6.4 with vehicle transportation in the US accounting for a portion of the global reduction.""","The quoted sentence from Doc [wu2021b] states that global carbon emissions fell by 6.4 percent in 2020, which directly answers the question."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit information about Amazon’s energy storage capacity as of 2023, so a confident answer cannot be derived."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents specify how many AWS regions achieved 100% renewable energy matching in 2023, so the answer cannot be determined with confidence."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons Paradox,Jevons Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox, where increased efficiency drives systemic demand."" (jegham2025#0108)","The quote explicitly states that efficiency gains in AI lead to increased overall consumption due to the Jevons Paradox, which is the economic principle the paper cites to argue that technical efficiency may not yield net environmental benefits."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents contain a statement that Facebook’s model experimentation workflows use GPUs at over 80% capacity. Therefore the answer cannot be determined with confidence from the provided context.
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,Approximately 15 years,15,years,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the period starting in 2010 as this is the beginning of the modern deep learning era (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed, including all 754 models from 2010 to the first quarter of 2025.""","The passage specifies 2010 as the start of the modern deep learning era, and the context is up to the first quarter of 2025, yielding roughly 15 years of development."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""The amount of compute used to train deep learning models has increased 300,000x in 6 years."" 
""The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.""","The context documents report a 300,000‑fold increase in compute between 2012 and 2018, not the 200,000‑fold increase stated in the claim. Therefore the claim is false."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about JetMoE-8B or the number of its parameters activated per input token. Therefore, the answer cannot be determined with confidence from the documents."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS),Neural Architecture Search (NAS),is_blank,dodge2022,https://arxiv.org/pdf/2206.05229,"""training a Transformer model with Neural Architecture search, finding it to be comparable to the lifetime carbon emissions of five cars.""","The quote from Dodge 2022 explicitly states that the five‑cars estimate comes from training a Transformer model using Neural Architecture Search, which is an infrequently performed AI process."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,transatlantic flights,jegham2025,https://arxiv.org/pdf/2505.09598,"""cumulative emissions from approximately 272 transatlantic flights between Boston and London""",The Jevans et al. 2025 document explicitly states that GPT‑4o’s projected annual carbon emissions are comparable to the emissions from about 272 transatlantic flights. This figure directly answers the question.
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement or data linking GPU utilization at 80% to a specific factor by which the overall carbon footprint decreases for LM training. Therefore, a confident answer cannot be derived from the given context."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts give global average PUE for all data centers (e.g., 1.58 in 2023) but do not specify a separate value for AI‑dedicated data centers. Therefore the documents do not contain sufficient information to answer the question confidently."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.""","The quoted passage from Morrison 2025 explicitly states that operational environmental impacts include GHG emissions from servers and data center cooling, contradicting the claim that they are excluded."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement giving the percentage of AI inference workloads in Asia powered by coal in 2023, so the answer cannot be derived from the documents."
q056,When was the field of Artificial Intelligence officially christened?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context passages do not mention the date or event when the field of Artificial Intelligence was officially named or christened, so the answer cannot be derived from them. "
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a specific percentage indicating how much energy use decreases for Llama 3.1 8B when targeting an average TPOT of 100 ms versus minimizing latency. Without that explicit figure, a confident answer cannot be given."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Intel Xeon Platinum 8358,Intel Xeon Platinum 8358,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Intel Xeon Platinum 8358 503 240""","The experimental setup table lists Intel Xeon Platinum 8358 as one of the CPUs used for inference evaluations, indicating that this processor was reported for the energy‑efficient local inference experiments in the financial sentiment classification study."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit average WUE value for Google's AI‑dedicated data centers in 2024, so the answer cannot be determined with confidence."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,20 samples,20,samples,xia2024,https://arxiv.org/pdf/2408.04693,Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse. -D -S ‐D ‐S … 2 8 6 20 … 1 3 2 8,"The table in the xia2024 document lists the maximum batch sizes for different model–dataset combinations. For BlackMamba on the GSM8K dataset under a sparse fine‑tuning setup, the sparse column shows a maximum batch size of 20 samples, which is the largest value reported for that configuration on the NVIDIA A40 GPU with 48 GB memory."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules,"[3, 4]",J,is_blank,is_blank,"""For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.""","The cited sentence directly states that at a maximum generation length of 512 tokens, LLaMA‑65B consumes roughly 3–4 Joules per output token."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about Facebook's second representative recommendation model (RM2) or its model size reduction from 32‑bit to 16‑bit quantization. Therefore the answer cannot be derived from the supplied context.
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity.""","The quoted passage from Doc wu2021b explicitly states that about 770 million people lack stable electricity, confirming the statement is true."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit statement or quantitative comparison indicating that sparsely activated DNNs consume less than one tenth the energy of large dense DNNs while maintaining accuracy. Therefore, there is insufficient evidence to answer the question confidently."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit information about the number of Amazon electric delivery vans added in 2022 or 2023, so a confident answer cannot be derived."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",25000,USD,schwartz2019,https://arxiv.org/pdf/1907.10597,"""BERT‑large was trained on 64 TPU chips for 4 days. Grover was trained on 256 TPU chips for two weeks, at an estimated cost of 25,000.""","The provided excerpt from Schwartz 2019 directly states that Grover’s training on 256 TPU chips for two weeks cost an estimated 25,000 USD, which gives the required value."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,33.8 Wh,33.8,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""Conversely, a long, high-reasoning query reaches an average of 33.8 Wh, comparable to the upper bounds observed among the most energy‑intensive models analyzed in this study."",","The quoted sentence from Doc jegham2025 states that a long, high‑reasoning query—which refers to the o3 model in this context—consumes an average of 33.8 Wh. This directly answers the question about the energy used by o3 for a long prompt."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The reasoning behind the 5-10 reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience.""; ""The second, Google-commissioned BCG study provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale.""; ""Applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding in fact, many of the emissions reductions on a global scale require individual, societal and political shifts.""","The quoted passages from Doc luccioni2025c state that the 5‑10% reduction figure originates from BCG reports, yet the underlying calculations are not detailed, and there is no scientific grounding or publicly available methodology that demonstrates a clear, robust estimate. Hence the claim is not supported by clear, publicly available calculations and sound scientific grounding."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally and 1.6 in the EU.""","The ebert2024 passage explicitly states that the global average PUE for 2023 was 1.58, which is the value requested."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,The optimizer stage in fine-tuning takes a considerable portion of the running time (up to 53 when conducting sparse fine-tuning with batch size = 1).,The quoted sentence from the context explicitly states that the optimizer stage accounts for up to 53% of the running time for sparse fine-tuning with batch size = 1 on an NVIDIA A40‑48GB GPU.
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"""Gemini Ultra has the highest fraction of RD staff cost at 49""","The document states that Gemini Ultra’s RD staff costs, including equity, make up 49% of the total amortized development cost, which directly answers the question."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about the number of members of the inaugural 2015 Study Panel of the One Hundred Year Study on AI.
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages contain a specific percentage indicating how much of a client device’s total carbon footprint comes from manufacturing. Therefore, the answer cannot be determined with confidence from the provided documents."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context passages do not contain any mention of a Study Panel from the 100 Year Study on AI or any statement indicating that such a panel is concerned about AI being an imminent threat to humankind. Therefore the answer cannot be determined from the supplied documents.
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not contain any information about the distance between the Earth and the Sun, so a confident answer cannot be given."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the number of wind turbines that Microsoft directly contracted to power Azure AI clusters in 2023.
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,is_blank,is_blank,"Zero‑shot models in our analysis with their architecture type, model size (in number of parameters), average quantity of emissions (in g of ) and average energy usage (in kWh) for 1,000 inferences. 0.083","The table lists 0.083 kWh per 1,000 inferences for Flan‑T5‑xxl. For 1 billion queries, that is 1,000,000 × 0.083 kWh = 83,000 kWh, which equals 83 MWh. This calculation directly follows the given energy per 1,000‑query figure."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents provide the number of metric tons of CO2 emitted by OpenAI’s API requests for January 2024, so the answer cannot be determined from the evidence given."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Meta's Llama 3 family emitted 11,390 tCO₂e, which is over 40 times the five‑cars estimate.",11390,tCO₂e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Meta reports that their Llama 3 family of models emitted 11,390 tons COe or over 40x the five cars estimate.","The quoted passage directly states the pre‑training emissions as 11,390 tCO₂e and that this figure exceeds the five‑cars estimate by more than 40×, providing the required comparison."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,schwartz2019;luccioni2024,https://arxiv.org/pdf/1907.10597;https://arxiv.org/pdf/2311.16863,"The number of model parameters does not tell the whole story: AlexNet (first point in the graph) actually has more parameters than ResNet (second point), but dramatically less work, and also much lower accuracy.  
We do observe a relationship between model size and quantity of emissions produced during inference, with differing progressions for each modality; however, the task structure accounts for more of the variation than the model size does.","Both documents indicate that parameter count alone is not a definitive predictor of inference energy consumption, showing that architecture and task type can lead to lower energy use even for larger models."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of JetMoE-8B alignment, dSFT or dDPO fine‑tuning GPU hours, so the required value cannot be extracted with confidence."
q080,True or False: The AlphaGo program defeated the human Go champion.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context mentions AlphaGo and its training resources but does not state whether it defeated a human Go champion. Therefore, the truth value of the statement cannot be determined from the documents alone."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)""","The quoted sentence from luccioni2025c explicitly lists the lowest and highest GPU energy consumption for 1,000 inference queries, giving the required range."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""It is difficult to define universal, or even generalizable, guidelines."" and ""There is no one-size-fits-all solution for either ethics or sustainability.""",The excerpts from Doc [luccioni2025b] explicitly state that universal or generalizable guidelines for AI ethics and sustainability do not exist and that researchers do not believe a one‑size‑fits‑all approach can be developed.
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any information about classification experiments on German public administration texts, sentence embeddings, or the accuracy of specific models. Therefore, there is no evidence to support a confident answer."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include a specific numeric value for the gross carbon intensity of energy for the U.S. average mix in 2021, so the answer cannot be determined from the documents."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context provided does not contain a clear, explicit name for the LLM inference system described by Chen et al. that uses model-attention disaggregation, so a confident answer cannot be derived from the documents."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,social transparency,social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems.""","The quoted passage explicitly defines the proposed term as ""social transparency"", indicating the expansion of AI transparency to include socio‑technical aspects and societal/environmental footprints."
q093,How many parameters does the largest T5 model have?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts contain an explicit statement of the parameter count for the largest T5 model, so the answer cannot be determined with confidence from the supplied documents."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,dynamic batching,dynamic batching,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization while maintaining low latency.,"The context states that dynamic batching is used to optimize GPU utilization, which inherently reduces idle GPU time by replacing completed requests with new ones, matching the description in the question."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams of CO2eq per 1,000 inferences",1594,g CO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"""the most carbon-intensive image generation model () generates 1,594 grams of  for 1,000 inferences"",""""","The quoted passage from the 2024 study reports that the most carbon‑intensive image generation model emits 1,594 g CO2eq per 1,000 inferences, matching the value for stable‑diffusion‑xl‑base‑1.0 as stated in the question."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Megetron‑LM,Megetron‑LM,is_blank,is_blank,is_blank,"We also utilize the Megetron‑LM implementation of the distributed optimizer to further reduce GPU memory consumption, which evenly distributes the optimizer states across data parallel ranks.","The context explicitly states that the Megetron‑LM distributed optimizer was used, and it distributes optimizer states across data parallel ranks—i.e., a decentralized PyTorch‑based framework for distributed training across multiple clouds and continents."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google observed a 20 uptick in the same period.""","The context states that Google’s water consumption increased by a 20% uptick from 2021 to 2022, directly answering the percentage increase question."
q094,What is the total number of parameters in the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about the JetMoE-8B model or its parameter count.
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40 million,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""GPT-4 has the largest amortized hardware and energy cost, at 40M.""","The cited passage directly states that the amortized hardware and energy cost for GPT‑4 is 40 million dollars, which is the value requested."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about Amazon's replenishment projects, the amount of water returned to communities, or the year 2023."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",51.3%,51.3,%,li2025a,https://arxiv.org/pdf/2309.03852,"""The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3.""","The context states that in all stages of FLM-101B training, the single‑GPU throughput exceeds 160 teraFLOPs/sec and the utilization rate is at least 51.3 %. Since the final growth stage is part of these three stages, the achieved FLOPs utilization percentage for that stage is 51.3 %."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3700000,GPUs,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023)""","The 2025 paper states NVIDIA shipped 3.7 million GPUs in 2024, which corresponds to the data center GPUs reported for that year."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,"""Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts.""","The cited passage from rubei2025 indicates that employing specific tags in prompt engineering—applicable to source code completion tasks—lowers LLM energy consumption, supporting the statement that custom tags in one-shot, zero-shot, and few-shot setups can reduce energy usage."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,morrison2025;luccioni2023,https://arxiv.org/pdf/2503.05804;https://arxiv.org/pdf/2302.08476,"""CO_2e = P PUE CI"" (morrison2025) and ""The unit of measurement typically used for quantifying and comparing carbon emissions is . This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of  emitted per kilowatt hour of electricity generated ()"" (luccioni2023).","The equation from morrison2025 uses CI, which stands for Carbon Intensity. The luccioni2023 passage defines the metric as grams of CO₂ per kWh, the standard definition of Carbon Intensity. Thus the emissions metric is named ""Carbon Intensity""."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain a specific value or statement about the fraction of local throughput achieved for NLP when training was spread across four continents versus remaining local. Therefore, the answer cannot be determined with confidence from the supplied documents."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,cottier2024,https://arxiv.org/pdf/2405.21015,"""components of amortized hardware CapEx + energy in , we find that on average, 44 goes toward AI accelerator chips.""","The quoted sentence from the cottier2024 document states that, on average, 44% of the total amortized hardware and energy cost is attributed to AI accelerator chips. This directly answers the question with a clear percentage."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems","The context explicitly names the Finnish project as ""ETAIROS,"" providing the requested acronym."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",about 27%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"""-1st  g4dn.xlarge  100 169.17  2.13"" and ""Max-Perf., (w/o KV)  g6e.xlarge  0  1506.54  2.699""","The table for the 100 TPS offline workload shows InferSave chose g4dn.xlarge costing 2.13, while Max‑Performance chose g6e.xlarge costing 2.699. The cost difference is (2.699‑2.13)/2.13≈0.267 or about 27 % more expensive."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,the PUE of Facebook datacenters is 1.10 (2020),"The provided text from Doc wu2021b explicitly states that Facebook’s data centers had a PUE of 1.10 in 2020, which is the required value."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",0.55,0.55,is_blank,is_blank,is_blank,"""These methods can reduce energy consumption and carbon emissions by up to 45% post quantization,"" (Doc [khan2025#0001])",The cited passage reports that full‑stack optimizations such as quantization reduce carbon emissions by up to 45%.  A 45% reduction corresponds to a factor of 0.55 of the baseline (CPU server) carbon footprint.
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain information about the EPA’s primary standard for PM2.5, so a confident answer cannot be given."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""115 books would produce the same amount of CO as a single Amazon Kindle device.""","The life cycle assessment cited in Doc [luccioni2025a] directly states that 115 physical print books generate the same CO₂ emissions as one Amazon Kindle e‑reader, providing the required numeric comparison."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric estimate for the amortized training costs of Google's Gemini Ultra. Therefore, I cannot determine the answer with confidence."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement from the 2022 paper by Dodge et al. that specifies the total number of parameters of the analyzed large language model, so it cannot be answered with confidence."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about a model named ""DS Llama 70B"" or its inference energy consumption on the FKTG dataset, so the answer cannot be determined with confidence."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200,200,is_blank,is_blank,is_blank,The ratio of the highest to lowest county-level per-household health cost reaches approximately 200. / low-income counties that could experience approximately 200x per-household health costs than others.,"Both documents report that the per-household health burden from AI-related air pollutants can be about 200 times greater in the most affected, economically-disadvantaged communities compared to less-impacted communities."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,jegham2025;luccioni2025a;morrison2025,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2503.05804,"""aligned with the Jevons Paradox , where increased efficiency drives systemic demand."" (Doc [jegham2025])","The quoted passage explicitly names Jevons Paradox as the phenomenon where efficiency gains lead to higher overall resource consumption, matching the question’s description."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",Approximately 3.3 times.,3.3,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,We find a growth rate of 2.2x per year (90 CI: 1.9x to 2.6x).,"The provided document reports a per‑year growth rate of 2.2x for Facebook’s AI training infrastructure capacity.  Over the 1.5‑year period (Yr1‑Q1 to Yr2‑Q2) this compounds to 2.2^(1.5) ≈ 3.3, indicating the capacity increased by roughly a factor of 3.3."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks."" 
""However, these provisions lack sufficient emphasis on environmental factors. Although environmental protection is included in the Act's objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required.""","The excerpts confirm that the AI Act requires risk assessments for GPAI models with systemic risk, but it does not explicitly require that these assessments include environmental risks; instead, it notes a lack of emphasis on such factors. Therefore the statement is false."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,ebert2024,https://arxiv.org/pdf/2410.06681,"""the energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process""","The context states the fine‑tuning cost is 7,571 kWh and the training cost is 51,686 kWh. Adding these gives 59,257 kWh, which is the combined energy cost reported for the BLOOMz‑7B model."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.75,0.75,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Mistral-small  0.70  0.67  0.65  0.67  0.020
Mistral-small  0.73  0.70  0.69  0.70  0.015","The table lists Mistral‑small’s emissions before optimization as 0.020 and after optimization as 0.015. Dividing the after‑optimization value by the before‑optimization value gives a multiplier of 0.015 ÷ 0.020 ≈ 0.75, indicating emissions dropped to 75 % of the original."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy.""",The provided excerpts from the 2024 study state explicitly that the total energy consumed for all model experimentation and evaluation was 754.66 kWh.
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any mention of a dataset name that corresponds to German nuclear waste site objection texts classified in experiments. Therefore, there is insufficient information to answer the question confidently."
q125,What is the total number of parameters in the final FLM-101B model?,101 billion parameters,101000000000,parameters,li2025a,https://arxiv.org/pdf/2309.03852,"""We train three models, with 16B, 51B, and 101B parameters, respectively""","The context explicitly states the final FLM-101B model has 101B parameters, which corresponds to 101,000,000,000 parameters."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit numeric energy consumption values for either GPT-3 or Meena, so the comparison cannot be determined from the given documents."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific West Virginia county name identified as having the highest projected per-household health cost for 2030. No quote or table from the excerpts supports a definitive answer.
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any explicit or inferable information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.9 kWh,2.9,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"image generation, whose mean consumption is 2.9kWh.","The passage from the 2024 study states that the mean energy consumption for 1,000 image generation inferences is 2.9 kWh, which directly answers the question."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the total carbon emissions avoided by pruning and quantizing large language models in 2023, so a confident answer cannot be derived."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""84 of LLM usage is through models with no disclosure, 14 for indirectly disclosed models, and only 2 for models with direct disclosure.""","The quoted sentence from the May 2025 OpenRouter data states that 84 % of token usage came from models that did not disclose their environmental impact, giving the requested percentage."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,is_blank,is_blank,"If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO₂ (depe…","The cited passage from Dodge 2022 provides the estimated CO₂ emissions range for a full training run of a 6.1 billion‑parameter transformer, giving a lower bound of 21 mt and an upper bound of 78 mt."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific figure for the freshwater consumption, in liters, of Meta's Llama 3 inference serving clusters in 2024. No supporting evidence or numeric value is found in the excerpts given."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the number of inferences required for BLOOMz-7B to reach parity with its training and fine‑tuning energy cost. The closest references describe ranges (hundreds of millions to tens of billions) or per‑inference energy, but no exact figure is given. Therefore, I cannot provide a confident answer."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""13B 2 64 1 64"" (table of bare minimum hardware requirements for LLaMA models)","The table lists the bare minimum GPU count and batch size for each model. For the 13B model, the entry shows 1 A100 GPU is sufficient, indicating that one NVIDIA A100 80GB GPU is required for inference without compression or quantization."
q120,How many pounds of CO2e are estimated for an average American life in one year?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric estimate of pounds of CO2e for an average American life in one year, so the answer cannot be derived with confidence from these documents."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention of Chen et al. (2025) or the price per hour for an NVIDIA H20, so the answer cannot be determined from these documents."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a statement linking 3.2 tCO2e of the Evolved Transformer NAS to a specific number of passengers on a round‑trip San Francisco–New York flight, so the answer cannot be determined with confidence from the supplied documents."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,is_blank,is_blank,"""using 2 A100s and 1 A10G results in a 24 cost saving over A100-only and 31 over A10G""","Both cited documents state that a mix of 2 A100s and 1 A10G yields a 24‑unit cost saving compared to an A100‑only strategy, which the context interprets as a 24% savings."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""In fact, most carbon footprint analyses gather the information manually by writing to authors.""","The cited passage from luccioni2025b explicitly states that most carbon‑footprint analyses collect data manually by contacting authors, which directly contradicts the claim that most analyses gather information automatically. Hence the statement is false."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied passages contain information about the percentage of Amazon’s U.S. workforce identified as men in 2023, so the answer cannot be determined from the provided documents."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95,95,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers""","The passage states that after contacting more than 500 authors, the researchers obtained 95 responses, which directly answers the question."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the percentage of Apple's total water footprint that is attributable to its supply chain. Therefore, the answer cannot be derived with confidence from the given context."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit information about the JetMoE project’s training budget or total GPU hours, so a reliable estimate of cost per H100 GPU‑hour cannot be derived from these documents."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,"""7B   1  64  1  64""\n\nThe 7B model was run on a single GPU","The table in the document lists that the 7B variant requires 1 GPU to achieve a maximum batch size of 64, and the text explicitly states that the 7B model was run on a single GPU, confirming the bare minimum requirement is one NVIDIA A100 80GB GPU."
q149,How many tokens were used to pre-train the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about a model named JetMoE-8B or the number of tokens used to pre-train it.
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,is_blank,is_blank,"""This is equivalent to approximately 44 of the data centers' total electricity cost.""","The cited passage states that in 2023 the public health cost of data centers was equivalent to about 44% of their total electricity cost, using the average attribution method."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,is_blank,is_blank,"""Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45 post quantization""","The quoted sentence explicitly states that quantization reduces carbon emissions by up to 45%, confirming the true statement."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024, so there is insufficient information to answer the question."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not mention the Standing Committee of the One Hundred Year Study or its practice of forming Study Panels, so the documents do not provide enough information to answer the question."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.""","The cited passage from wu2021b explicitly states that in 2021 the average U.S. household had 25 connected devices, matching the requested categories."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any information about IBM Watson’s performance in the Jeopardy challenge, so the truth value of the statement cannot be determined from these documents."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""10–50 queries on GPT-3 consumes around half a liter of water.""","The passage from Doc [luccioni2025a] explicitly states that 10–50 queries to GPT-3 consume about half a liter of water, providing the required range."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8 to 3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)""","The cited passage from luccioni2025c explicitly states the minimum and maximum publicly reported energy consumption for pre-training large language models, giving a clear range of 0.8 to 3,500 MWh."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year""","The document states the deal could add 640 percent more emissions, which is equivalent to 6.4 times the yearly removal target."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any reference to a metric introduced for assessing the ratio of computation to communication time in distributed training across continents.
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain the total execution time for a sparse BlackMamba model fine-tuned on an NVIDIA A40‑48GB with a batch size of 84, so the answer cannot be confirmed from the documents."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the JetMoE-8B-Chat model, its MT‑Bench score after alignment, or the Llama‑2‑13b‑Chat score, so the answer cannot be determined from the documents."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",120%,120,%,is_blank,is_blank,"""total health cost can even exceed 120 of the electricity cost""","The document states that the public health cost can exceed 120% of the electricity cost for training a Llama‑3.1 model, which applies to the Altoona, Iowa location as part of the same analysis."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"""...we estimate a full training run would consume approximately 103,593 kWh."" (dodge2022#0049) and in the comparison table for BLOOMz models the number of inferences required to reach training parity for BLOOMz-7B is listed as 592,570,000.""","The full 6.1 B‑parameter training energy is reported as 103,593 kWh.  The table of BLOOMz models gives the number of inferences needed for BLOOMz‑7B to equal its training energy as 592,570,000, which is the requested estimate."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided passages contain a definition of the term that matches the description in the question.
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention of Griggs et al., Mélange, or a stated percentage reduction in deployment costs for conversational chat settings."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg CO2e,178.97,kg CO2e,luccioni2024,https://arxiv.org/pdf/2311.16863,"""emitted 178.97 kg of .""","Doc [luccioni2024] explicitly reports that the entire Power Hungry Processing study emitted 178.97 kg of CO₂ equivalent, providing the total emissions figure."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the GHG emissions (in tCO2e) associated with pre‑training the Llama 7B model, so a confident answer cannot be given."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""the 65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).""",The document states that the bare‑minimum hardware for LLaMA‑65B inference without compression or quantization requires 4 A100 80GB GPUs. This number is directly cited in the passage describing the hardware setup for the 65B model.
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4o consumes around 2.875Wh while GPT-4omini's consumption is slightly higher at 3.098Wh"" and ""GPT-4o mini, despite its smaller architecture, consumes approximately 20 more energy than GPT-4o."",","The quoted passages from Doc jegham2025 state that GPT‑4o mini uses more energy per query than GPT‑4o, contradicting the claim that it consumes less energy. Therefore the statement is false."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."" (luccioni2025c)
""training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car."" (han2024#0029)","Both documents state that training a Llama‑3.1–scale model emits air pollutants equal to over 10,000 round trips between Los Angeles and New York City, so the answer is more than 10,000 round trips."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific monthly on‑demand rental cost figure for serving Llama‑2‑70B at BF16 precision on 2 NVIDIA A100 GPUs, so the answer cannot be inferred with confidence."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit relationship between model size and BLEU score improvement from 5 to 40 for a GPT‑3 based translation task, so a confident answer cannot be derived."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,"More than 700,000 liters",700000,liters,jegham2025,https://arxiv.org/pdf/2505.09598,more than 700 kiloliters (kL) of water for cooling alone,"The context from Doc jegham2025 states that cooling during GPT‑4 training used more than 700 kL of water, which equals over 700,000 liters. This directly answers the question."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,FALSE,0,is_blank,luccioni2023;ebert2024,https://arxiv.org/pdf/2302.08476;https://arxiv.org/pdf/2410.06681,"""This upper bound is typically higher than the actual consumption, as GPU utilization rarely reaches its theoretical peak."" (ebert2024) and ""Nonetheless, the TDP-based approach is often used in practice when estimating the carbon emissions of AI model training and it remains a fair approximation of the actual energy consumption of many hardware models."" (luccioni2023)","Both documents state that TDP-based estimates overestimate actual GPU consumption and are only a rough approximation, indicating that TDP is not a reliable and accurate method for estimating GPU energy usage."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10 to 50 completions,"[10, 50]",is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""10–50 queries on GPT‑3 consumes around half a liter of water.""","The cited estimate says 0.5 L of water is used for 10–50 queries, so each query (a medium‑length completion) uses between 0.05 L and 0.01 L. Dividing the 0.5 L bottle volume by these per‑completion values yields a range of 10 to 50 completions that can be produced with that amount of water."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The direct release of environmental information peaked in 2022, with 10 of notable models that year releasing some degree of information. However, ... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.""","The 2025 paper reports that after 2022 the trend of direct environmental disclosures declined rather than continued to increase, indicating the statement is false."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific percentage of ML workload attributed to inference processing by NVIDIA in 2019.
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include a specific numeric throughput value for a dense Mixtral-CS-A100-40GB model at batch size 1, so the answer cannot be determined with confidence."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied excerpts from the Griggs et al. (2024) document do not contain a specific numeric value for the normalized on-demand hourly price of an H100 GPU, so a confident answer cannot be derived from the available information."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","Approximately 730,000 miles",730000,miles,luccioni2024,https://arxiv.org/pdf/2311.16863,"""1,594 grams of CO2 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle""","The context states that 1,594 g (≈3.5 lb) of CO2 equals 4.1 miles, giving a ratio of about 1.17 miles per pound. Multiplying the 626,155 lb emitted during Transformer training+NAS by this ratio yields roughly 730,000 miles of driving distance."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific figure for the H100 GPU hours consumed during the pre‑training of the JetMoE‑8B model, nor does it identify JetMoE‑8B explicitly. Therefore the answer cannot be determined from the supplied documents."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",one billion dollars,1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"""the amortized cost of frontier training runs will exceed one billion dollars by 2027.""","The cited sentence directly states that by 2027, the largest training runs will surpass the one‑billion‑dollar threshold, providing the numeric threshold requested."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"""FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.""","The quoted passage from Doc [schwartz2019] explicitly states that training FAIR’s RoBERTa on 160 GB of text required about 25,000 GPU hours."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied contexts provide a quantitative figure for the CO2e avoided by Amazon’s on‑site solar energy systems relative to nonrenewable sources, so the answer cannot be determined with confidence."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8×80G) servers."" 
""We fix our budget to be 100K with 192 A800 GPUs.""","The context states the cluster consists of 24 servers, each equipped with 8 A800 GPUs, totaling 24×8 = 192 GPUs. This is confirmed by the explicit mention of 192 A800 GPUs in the budget statement."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8,8,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Model Size
     65B  8",The provided table lists the bare minimum hardware needed for each LLaMA variant and shows that the 65B model requires 8 NVIDIA V100 32 GB GPUs. This matches the statement that the 65B was run on 8 V100 GPUs in the experiments.
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,56.4%,56.4,%,schwartz2019,https://arxiv.org/pdf/1907.10597,"""AlexNet 61.1 0.7 56.4 2012""","The table in the document lists AlexNet’s top‑1 accuracy on ImageNet as 56.4 %, published in 2012."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided contexts do not contain a clear statement of the total number of floating point operations used to train GPT‑3 as published by OpenAI. Therefore a confident answer cannot be produced from the supplied documents.
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,This implementation of the model uses Pytorch and the FairScale library to enable model sharding across multiple GPUs and nodes.,"The cited passage from doc [samsi2024] explicitly states that FairScale was used to enable model sharding across multiple GPUs and nodes, indicating it is the framework employed for deployment."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about Yelp sentiment analysis benchmarks or a comparison between traditional models and large language models. Therefore, the question cannot be answered with confidence based on the documents given."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide emissions for NAS training (626,155 lbs / 284 metric tons) but give American life consumption only per year (36,156 lbs). They do not supply the average American lifetime in years, so the equivalence in lifetimes cannot be derived with certainty."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages give annual water consumption estimates for GPT models and aggregate user or session counts, but none specify per‑session water usage for 2023. Without a direct figure or sufficient data to calculate it, the answer cannot be determined with confidence from the documents."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,is_blank,is_blank,is_blank,"""Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes""","The cited passage directly states that the annual electricity consumption from 700 million daily GPT‑4o queries equals that of 35,000 U.S. residential homes, providing the required numeric answer."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention the JetMoE-8B model or its final average score on the OpenLLM Leaderboard, so the answer cannot be derived from the documents."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Microsoft reporting a 34 increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.""","The quoted sentence from the 2025 paper states Microsoft’s global water consumption rose by 34% from 2021 to 2022, providing the required percentage increase."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide a clear numerical value for the throughput of the final 101B training stage or the exact duration of that stage, preventing a reliable calculation of total computational work in zettaFLOPs."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"""yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15 of the annual number of Google searches in 2024""","The quoted passage explicitly states that the projected total number of GPT‑4o queries in 2025 is about 772 billion, which directly answers the question."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not contain a clear statement or numerical value indicating how many AI training runs were conducted globally on renewable-only power in 2022. Therefore, a reliable answer cannot be derived from the documents."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""RD staff costs including equity are between 29 and 49 of the total amortized cost.""","The cited sentence directly states that, for the four models studied, R&D staff costs (including equity) comprise 29% to 49% of the total amortized cost."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,is_blank,is_blank,"""we used the Code Carbon package to measure both the energy consumed and the carbon emitted during inference.""","Both referenced documents explicitly state that the CodeCarbon package was used to measure energy consumption during inference runs, confirming that CodeCarbon is the software package in question."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""75 of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53 of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search""","The document states that out of 100 analyzed news articles, 53 cited the contested 3 Wh/10×‑Google‑search estimate, giving a percentage of 53 %."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.58,1.58,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""average PUE for a typical data center in 2020 is 1.58.""","The context from Doc wu2021b explicitly states that the average Power Usage Effectiveness for a typical data center in 2020 was 1.58, which corresponds to the US national average for that year."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).""","The quotation states that open‑source general‑purpose AI models are excluded from reporting requirements unless they pose a systemic risk, directly supporting the claim that they are fully exempt under those conditions."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,TRUE,1,is_blank,is_blank,is_blank,"Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall.","The cited passage explicitly states that adding more GPU shards increases energy cost per response for the LLaMA‑65B model, confirming the statement is true."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain clear information about the energy consumption of the Llama 3.1 70B model on one node versus two nodes, nor does it provide a factor of increase. Therefore, I cannot determine the requested factor with confidence."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),CTCF,is_blank,kim2025,https://arxiv.org/pdf/2504.11816,proposes the Compute Time Calibration Function (CTCF) to accurately determine the optimal instance based on user requirements.,"The cited Kim 2025 documents state that the proposed method is the Compute Time Calibration Function (CTCF), which adjusts predictions to match actual GPU performance, thereby improving instance selection accuracy."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a;wu2021b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2108.06738,"""In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30 of all PPAs purchased by corporations worldwide."" (luccioni2025a)","The quoted sentence from luccioni2025a explicitly states that the four companies accounted for about 30% of all corporate PPAs in 2020, matching the figure reported in wu2021b, confirming the 30% answer."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain a specific PUE value for Google's Iowa datacenter during the run of the Evolved Transformer, so the answer cannot be determined with confidence."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the KV Cache size when the batch size is 32 for the OPT-2.7B model, so the answer cannot be determined with confidence."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","606,096 MWh",606096,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""BLOOMz-7B has been downloaded 606,096 times""; ""energy required per inference varies from 5.4 × for the smallest model, BLOOMz-560M to 1.0 × kWh for the biggest one, BLOOMz-7B.""","The download count gives 606,096 downloads, each assumed to trigger 1,000,000 inferences. The inference energy per inference for BLOOMz-7B is 1.0 kWh per 1,000 inferences, i.e., 0.001 kWh per inference. Total energy = 606,096 × 1,000,000 × 0.001 kWh = 606,096,000 kWh, which equals 606,096 MWh."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability.""","The quoted text indicates that under current EU rules, open‑source models are exempt from reporting obligations, so they are not required to report their energy consumption to authorities."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a specific numeric value for the total execution time in seconds of a sparse Mixtral model with batch size 1 on an NVIDIA A40-48 GB GPU.
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about a storage service used for sharding and streaming datasets to spot VMs that can terminate at any time.
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any mention of the Arena dataset, 120 ms SLO, or the cost‑reduction percentage range achieved by Mélange for short‑context workloads, so the answer cannot be derived from the documents."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,is_blank,is_blank,"Importantly, the health costs are unevenly distributed across counties and communities, particularly affecting low-income counties that could experience approximately 200x per-household health costs than others. The ratio of the highest to lowest county-level per-household health cost reaches approximately 200.","Both documents state that public health costs vary widely across counties, with some counties incurring up to 200 times higher per-household costs, indicating an uneven distribution rather than an even one."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.0022 kL,0.0022,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"""Together, these add an additional 2.2 liters consumed and 0.013 kg COeq per GPU.""","The context states that mining the rare‑earth content of an H100 GPU (0.1% by mass) results in an additional 2.2 liters of water use per GPU. Converting 2.2 L to kiloliters (1 kL = 1000 L) gives 0.0022 kL, which is the estimated water consumption for mining the rare‑earth materials of a single H100 GPU."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",6.7 billion USD,6700000000.0,USD,is_blank,is_blank,"""health cost of about 6.7 billion, or 47.5 per household, in 2023""","The provided context states that the total public health cost of U.S. data centers in 2023, using the average attribution method, is about 6.7 billion dollars."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years.""","The quoted statement from Doc [wu2021b] directly confirms that GPU theoretical performance per watt doubles approximately every 3-4 years, matching the claim in the question."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain a specific numeric value for the total estimated net carbon emissions of FLM-101B’s pre‑training. No table or sentence with that figure is present, so the answer cannot be determined from the documents given."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""the bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""",The supporting quote explicitly names Senator Edward J. Markey as the introducer of the AI Environmental Impacts Act bill in February 2024.
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,4 years,4,years,morrison2025,https://arxiv.org/pdf/2503.05804,"""Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of COeq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.""","The document explicitly states that the authors assume a 4‑year lifespan for GPUs in their calculations, providing the estimated average lifetime before retirement in AI data centers."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,2,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"The table in the paper lists for LLaMA‑13B a count of 2 V100 GPUs (""13B  2  64"") and a statement that the 13B model was run on two GPUs.","The table specifies that LLaMA‑13B requires 2 V100 32 GB GPUs for bare‑minimum inference, and the text confirms that the 13B model was run on two GPUs, establishing the required number."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied passages discuss a relationship between runtime and energy consumption, let alone describe it as nearly linear. Therefore the answer cannot be supported by the provided documents."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,is_blank,is_blank,"""report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).""","The cited passage directly states that ELMo was trained on 3 GTX 1080 GPUs for a duration of 2 weeks, which equals 336 hours, matching the requested training time on 3 NVIDIA GTX 1080 Ti GPUs."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,14.8×,14.78,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082
GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094","The third number in each row corresponds to long‑prompt energy consumption. Dividing o3’s 12.222 Wh by GPT‑4.1 nano’s 0.827 Wh yields ≈14.8, so o3 consumes about 14.8 times more energy than GPT‑4.1 nano for a long prompt."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The pre‑training of Google’s Gemma family emitted 1,247.61 tCO₂e, which is more than four times the “five‑cars” estimate.",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons COe, over 4x the estimate that forms the basis for the five cars number.""",The quoted passage directly states the Gemma emissions and notes that this figure exceeds the five‑cars estimate by a factor of about four.
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""GPU alone accounts for 74 of the total energy consumption due to these components.""","The context from Dodge 2022 explicitly states that the GPU accounts for 74% of the total energy consumption in a typical data center, indicating the proportion of provisioned power attributable to GPUs."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not explicitly state the name of the open‑source tool used for 4‑bit quantization; the relevant sentence contains a placeholder instead of the tool name, so the answer cannot be determined from the provided context."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain sufficient information to calculate or directly state the net cost of fine‑tuning a sparse Mixtral model with 2 million queries on an NVIDIA H100 GPU.
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",600 W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"""When actively training, the average GPU power is over 600W, over 85 of an H100's maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15 maximum.""",The passage states that during active training the average GPU power exceeds 600 W. Thus the average GPU power for a single node during the first 300 logging steps while actively training is approximately 600 W.
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement or data regarding the estimated pounds of CO2e for an average human life in one year globally, so the answer cannot be determined from the documents."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021).","The cited passage explicitly states that Google's hyperscale data centers had a PUE of 1.10 in 2021, providing the required value."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""A single short GPT-4o query consumes 0.42 Wh ( Wh)""",The quoted passage from Doc jegham2025 explicitly states that a single short GPT‑4o query consumes 0.42 Wh. This directly answers the question with the specified unit.
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,morrison2025,https://arxiv.org/pdf/2503.05804,"""Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server.""","The context states that each server contains 8 H100 GPUs. With 12 nodes in the cluster, the total number of H100 GPUs is 12 × 8 = 96."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages mention TDP values for TPU v3 and Tesla V100, but there is no information about TPU v2 or its power consumption per processor. Therefore, the difference cannot be determined from the given documents."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25 times,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100""","The cited passage reports a 1.25‑fold increase in performance (interpreted as throughput) for LLaMA‑13B when moving from V100 to A100 GPUs, providing the required speedup figure."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.14 L/kWh,3.14,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"""our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the 4.35 L/kWh recently reported by .""","The Electricity Water Intensity Factor (EWIF) reported in the document represents the national average water consumption for electricity generation. The value cited is 3.14 L/kWh, which directly answers the question."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents provided do not contain a clear statement of the price per hour for an NVIDIA H100 according to Chen et al. (2025).
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""reaching 62 million tonnes in 2022.""","The 2025 paper explicitly states that electronic waste worldwide reached 62 million tonnes in 2022, providing the required figure."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""proposing reporting the financial cost or price tag of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.""; ""Reporting the computational price tag of finding, training, and running models is a key practice.""","Both cited documents state that Green AI involves reporting the financial cost of developing, training, and running models, supporting a true answer."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context discusses LLM inference efficiency and memory bottlenecks but contains no comparison to diffusion models or a statement about overall lower power draw of LLMs. Therefore, there is insufficient evidence to determine the truth value of the claim."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any specific information about the size of Facebook’s recommendation and ranking models for 2019 or 2021, so the increase cannot be determined from these documents."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain information about intra‑zone scaling performance, T4 GPUs, or CV model speedup, so the truth value cannot be determined with confidence."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"""a context window of 2,048 tokens""","The FLM-101B configuration is described as having a context window of 2,048 tokens, which directly answers the question."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context documents do not contain any information about Amazon's People Managers or the percentage of women among them in 2023, so a confident answer cannot be derived from the available data."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""longer lifetimes than the current averages of less than 3 years for cell phones""","The quoted passage from Wu 2021b states that cell phones (smartphones) currently average lifetimes of less than 3 years, which supports the claim that such short lifetimes contribute to e‑waste concerns."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,Ampere (A10G),Ampere,is_blank,is_blank,is_blank,"""smaller sizes are best served on A10G, and larger sizes are best served on A100.""","The quoted sentence from Griggs 2024 indicates that for small request sizes—such as those that output a single classification token—the A10G GPU, which uses the Ampere architecture, provides the highest cost‑efficiency, implying it is the most energy‑efficient for this scenario."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied passages contain data on the number of packages Amazon delivered via electric vehicles in Europe during 2023.
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement specifying the percent reduction in carbon footprint that AWS reports for moving workloads from on-premises data centers to AWS in North America. Therefore, I cannot confidently provide the requested figure."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,700 kiloliters,700,kL,jegham2025,https://arxiv.org/pdf/2505.09598,"""Training GPT‑3 is estimated to consume 1,287 MWh of electricity and emit over 550 metric tons of CO₂‑eq, while requiring more than 700 kiloliters (kL) of water for cooling alone...""","The context states that more than 700 kL of water is used for cooling during GPT‑3 training in Microsoft’s U.S. data centers, implying that this volume of clean freshwater is directly evaporated."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61-76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""With equity excluded from RD costs, the fractions of computing hardware costs and energy rise to 6176 and 27 respectively.""","The quoted passage states that when equity is excluded, computing hardware accounts for 61–76% of the total amortized model development cost for the four key models. This provides the required percentage range."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the average CO2 produced in pounds per kilowatt-hour as reported by the EPA, so the answer cannot be determined with confidence."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context discusses eco‑efficiency analyses using DEA but does not specify which model achieved the highest ranking, so the answer cannot be determined with confidence from the documents."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",About 280%,280,percent,kim2025,https://arxiv.org/pdf/2504.11816,"""With an SLO requirement of 400 TPS, selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of 0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of 2.699,""","The cost of InferSave’s top choice (g4dn.xlarge) is 0.71, while the Max‑Performance choice (g6e.xlarge) costs 2.699. The increase is (2.699‑0.71)/0.71≈2.80, i.e., about 280% more expensive. The quoted passage provides the two cost figures used in this calculation."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off""","The cited passage from Doc [khan2025] states that accuracy and F1 scores are slightly lower after optimization, contradicting the claim that they always improved. Therefore, the statement is false."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided contexts do not contain any information about Yelp sentiment analysis benchmarks or the accuracy of traditional models versus large language models for that specific dataset. Therefore the question cannot be answered with confidence from the supplied documents.
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024;luccioni2025a,https://arxiv.org/pdf/2410.06681;https://arxiv.org/pdf/2501.16548,"""The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration."" ""The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration ).""","Both documents state that the AI Act does not require disclosure of greenhouse gas emissions from AI applications, including those used in oil and gas exploration, indicating the statement is false."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages contain a clear statement of the number of Amazon Renewable Energy Projects announced in the United States as of January 2024, so the answer cannot be determined from the documents."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""GPU alone accounts for 74 of the total energy consumption""","The cited experiment reports that the GPU contributed 74% of the total electricity consumption, so the percentage attributed to the GPU is 74%."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"""for very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30 in multiple regions, and up to 80 in West US;""",The quoted passage from the Flexible Start analysis states that DenseNet 201 can achieve a maximum emissions reduction of up to 80 % in the West US region.
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",over 1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""from the least energy‑intensive task, text classification, with mean consumption of 0.002 kWh per 1,000 inferences, to the most energy‑intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.""","The cited passage explicitly states that image generation consumes 2.9 kWh versus 0.002 kWh for text classification, yielding a factor of over 1450. This directly answers the question."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Amazon's AWS (the largest cloud computing platform) only covered fifty percent of its power usage with renewable energy,"The context explicitly states that AWS covered fifty percent of its power usage with renewable energy, which corresponds to 50%."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Energy consumption should be reported at the cumulative server level.""","The quoted sentence from the authors in the ebert2024 document explicitly states that AI energy consumption should be reported at the cumulative server level, which balances accuracy and feasibility."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any information about the length of fiber optic cable installed globally for AI workloads in 2023, so the answer cannot be determined with confidence."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain a clear statement or numeric value indicating the total operational energy footprint reduction achieved at Facebook over the 2019–2021 period due to iterative hardware-software optimization. Therefore, I cannot provide a confident answer."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google reports a 48 increase in GHG emissions since 2019""","The quoted sentence from the 2025 document states that Google’s 2024 environmental report cites a 48% increase in greenhouse gas emissions since 2019, providing the exact percentage increase requested."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"The umbrella term `Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The quoted passage shows that Sustainable AI was meant to include both climate-positive applications and efforts to improve AI’s own environmental sustainability, indicating it was not limited to just climate-positive uses."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents mention the JetMoE-8B architecture or its inference computation relative to Llama2-7B, so the requested percentage cannot be determined from the provided information."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages contain information about recomputation or swapping as preemption mechanisms or their energy consumption, so the answer cannot be determined from the documents."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied passages mention McKinsey projections or the percentage of U.S. national electricity consumption that data centers are expected to account for in 2030.
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,800M,800,M USD,cottier2024,https://arxiv.org/pdf/2405.21015,we estimate that it cost 800M to acquire the hardware used to train GPT-4,"The context states that the estimated upfront hardware acquisition cost for GPT‑4 is 800 million USD, which directly answers the question."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?","1,287 MWh",1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity","The context from Doc [jegham2025] explicitly states that training GPT-3 consumes 1,287 MWh, which directly answers the question."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a specific statement or table indicating the maximum batch size for fine‑tuning a Mixtral model on an NVIDIA A100‑40GB GPU.
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about CV models, high granularity, intercontinental training, or a 7% performance slowdown, so the statement cannot be verified."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any numerical information about the land area occupied by new AI data centers in 2022, so a confident answer cannot be derived from them."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of emissions.","The quoted passage from Doc [luccioni2025b] directly states that the 2019 Strubell et al. study estimated the BERT training emissions at 626,155 pounds of CO2e, providing the required value."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""peaked in 2022, with 10 of notable models that year releasing some degree of information.""","The context explicitly states that the direct release of environmental information peaked in 2022, after which it declined."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,8,8,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""We stratify LLMs into five hardware classes based on model size:  (7B),  (720B),  (2040B),  (4070B), and  (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.""","The document states that models larger than 70B are assigned 8 GPUs, implying that serving a Llama2‑70B model at BF16 precision requires 8 NVIDIA A100‑80GB GPUs."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement of the maximum batch size for dense fine‑tuning of Mixtral on Hellaswag with an NVIDIA A40 GPU. The excerpts mention a table of batch sizes, but the specific values for the requested configuration are not identifiable."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so the answer cannot be determined from these documents."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",True,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"The MoE layer is the most time‑consuming, accounting for 85 of the overall execution time on average, and the authors specifically perform a kernel‑level analysis within the MoE layer to improve performance.","The context shows that the MoE layer dominates fine‑tuning execution time and is the focus of performance optimization efforts, supporting the statement that it is often a targeted layer."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value or clear statement of the total energy consumption used to train the FLM-101B model, so a confident answer cannot be given."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement of a projected range of electricity consumption by global AI for the year 2027. The closest references are to current levels or projections for 2026 or 2028, but none specify a 2027 range. Therefore, I cannot answer the question with confidence."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",9.6%,9.6,%,dodge2022,https://arxiv.org/pdf/2206.05229,PR 9.6,"The table for the Pause and Resume (PR) optimization shows a 9.6% emissions saving for the 6B parameter transformer, which is the highest percent reported for that model under the PR optimization."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context documents contain projections up to 2028 but do not mention any projections for the year 2030. Therefore there is insufficient information to answer the question about a 2030 value.
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","Approximately 27,000 grams",27,thousands of grams,is_blank,is_blank,"""We find (grams) per GPU hour in the most efficient region to average 34, and in the least efficient region to average 128""","The cited values give emissions per GPU hour for the most (34 g) and least (128 g) efficient regions. For the BERT experiment (8 GPUs, 36 h), the difference per hour is 94 g, and over 8 × 36 = 288 GPU‑hours the total difference is 94 × 288 ≈ 27,072 g, i.e., about 27 thousand grams."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""training accounted for only half of the model's overall emissions""","The quoted passage from the 2023 article indicates that training contributed 50% of the total emissions, so the percentage is 50%."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the total execution time of a sparse Mixtral model fine‑tuned on an NVIDIA A40‑48GB with a batch size of 10. No table or figure gives a time in seconds for that configuration, so the answer cannot be determined from the documents."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""The 7B model was run on a single GPU"" and the table entry showing 7B Count 1, Max. Batch size 64 for V100.","The document explicitly states that the LLaMA‑7B model was run on a single V100 GPU; the accompanying table lists Count 1 for the 7B model, confirming that one 32 GB V100 is the bare minimum required for inference without compression or quantization."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""Sparse model significantly improves throughput, reducing end-to-end cost of fine-tuning.""","The passage indicates that accelerating the MoE layer (e.g., via sparse models) reduces fine‑tuning cost, implying that adding compute resources to speed up MoE layers does not inherently increase costs."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g CO2eq,0.32,g CO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"""emits just 0.32g of CO2 per 1,000 queries""","The 2024 study reports that the BERT‑based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 g CO2eq for every 1,000 text‑classification queries, as stated in the cited document."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"We advocate against using GPU-level or other component-based power consumpt

against using GPU-level or other component-based power consumption tracking for overall energy measurements.","Both excerpts from ebert2024 state that GPU‑level power monitoring is not recommended for overall energy reporting, indicating the statement is false."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",20,20,samples,xia2024,https://arxiv.org/pdf/2408.04693,Maximum batch size supported by LLM fine-tuning; D: dense and S: sparse.\n\n- D  -S  -D  -S  \n     2  8  6  20  \n          1  3  2  8,"The table in the document lists the maximum batch sizes that fit into the 48 GB memory of an NVIDIA A40 for various models and sparsity settings. For the sparse Mixtral model the listed maximum batch size is 20 samples, which corresponds to the longest-running MoE layer under these conditions."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,tCO2,is_blank,is_blank,"""...emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons per year.""","The quoted passage states that the average U.S. home’s annual energy use results in 8.3 metric tons of CO₂ emissions, which directly answers the question."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided context documents mention the JetMoE-8B model or its GSM8k benchmark score. Therefore there is insufficient information to answer the question with confidence.
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not give a specific water consumption estimate for training GPT‑3 in an Arizona data center, nor do they relate the 500 ml consumption to a number of user requests for that location. Therefore a confident answer cannot be derived solely from the supplied context."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,836,000 tokens",1836000,tokens,jegham2025,https://arxiv.org/pdf/2505.09598,"From Jegham: ""short-form (100 input, 300 output token"" (indicating 100 input tokens). From Kim: ""ut tokens and a 512 output tokens. The workload evaluates a total of 3000 requests.""","The online inference workload consisted of 3000 requests. Each request used 100 input tokens (from the short-form definition) and 512 output tokens (from the Kim document). Thus each request processed 612 tokens, and 612 × 3000 = 1,836,000 tokens in total."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","2,738 days",2738,days,is_blank,is_blank,"""the amount of water consumed by one average person in the United States in about 7.5 years.""","The context states that training a sub‑billion parameter model (such as the 60M OLMo) consumes water equivalent to about 7.5 years of an average U.S. person’s water use. Converting 7.5 years to days (7.5 × 365 ≈ 2,738 days) yields the answer."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a numerical estimate of the total cost for fine‑tuning a Mixtral model on GSM8K with sparse MoE on an NVIDIA A40‑48GB GPU, nor does it provide the necessary cost per GPU hour or GPU usage time to compute such an estimate."
