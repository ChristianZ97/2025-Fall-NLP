id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""total energy consumption of the US data centers increased by about 4% from 2010-2014""","The context from document wu2021b explicitly states that U.S. data center electricity consumption rose by approximately 4% between 2010 and 2014, which is the average increase requested."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 t CO2e,1.2,tCO2e,patterson2021,https://arxiv.org/pdf/2104.10350,"""a single passenger round trip SF-NY is ~1.2t CO 2 e (Table 2).""","The context from the Patterson 2021 document explicitly states that a single passenger round trip from San Francisco to New York emits approximately 1.2 t CO₂e, which directly answers the question."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,morrison2025,https://arxiv.org/pdf/2503.05804,"Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.","The Morrison 2025 document explicitly states that the embodied carbon per GPU is 463 kg, derived from a 3700 kg CO2eq per 8‑node server. This directly answers the question for embodied emissions per GPU."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,8 0.036 12.0 0.054 12.64 21.5 bil.,"The table in Morrison 2025 (section #0138) lists the GPU power usage for Llama 3.2 1B at an 8 req/s frequency as 0.036 kWh, which is the requested estimate."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"The total permitted site‑level annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context explicitly states that the aggregate NOx limit for data center backup generators in the relevant period is about 13,000 tons, which directly answers the question."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24,24,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The document explicitly states that the number of data centers using recycled water for cooling rose to 24 in 2023, which answers the question."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"""the total time cost for training FLM-101B is 21.54 days, which is 72% time‑saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""",The quoted sentence states that the growth strategy saved 72% of the training time compared to a from‑scratch approach for the 101B model.
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021;luccioni2023,https://arxiv.org/pdf/2104.10350;https://arxiv.org/pdf/2302.08476,"Doc [patterson2021] states: ""It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.""  Doc [luccioni2023] notes: ""the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs).""","Both documents explicitly report that using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec requires approximately 14.8 days to train GPT‑3, which is the value used in the answer."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,cottier2024;li2025a,https://arxiv.org/pdf/2405.21015;https://arxiv.org/pdf/2309.03852,"""the largest amortized hardware and energy cost, at $40M"" (cottier2024) 
""FLM-101B, ... trained from scratch within a $100,000 budget"" (li2025a)","The amortized training cost of GPT‑4 is given as $40 million while the total training budget for FLM‑101B is $100 000. Dividing 40 million by 100 000 gives a factor of 400, indicating GPT‑4’s amortized cost was 400 times greater than FLM‑101B’s budget."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.""","The report states that the avoided emissions of the Maryland–CPV Backbone are equivalent to taking more than 13,900 cars off the road, providing the estimated car count."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750-fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""…Intel 4004…ran at 740 kHz…typical microprocessor…running at 5 000 000 kHz…more than 6 750 fold improvement in processor clock speed…""","The cited passage states that the Intel 4004 ran at 740 kHz while a typical 2021 processor runs at 5 000 000 kHz, yielding a more than 6 750‑fold increase in clock speed."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give estimates for 2028 (approximately 1,300 premature deaths) but do not contain any information about premature deaths for the year 2030. Therefore, the question cannot be answered with confidence from the supplied context."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation...""","The quoted sentence directly states that the study was launched in the fall of 2014, providing the year of launch."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,dodge2022,https://arxiv.org/pdf/2206.05229,"""a full training run would take 60 days.""","The context states that the 6.1 billion‑parameter model was only trained for 13 % of its full run, estimating a full training run would take 60 days. This directly answers the question."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command‑R Plus)""","The appendix lists GPU energy consumption per 1,000 queries and shows Command‑R Plus at over 3,426 Wh, the highest value among the models reported."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Table 1: JetMoE-8B hyperparameters – ""Nexperts 8""; ""We set the same number of experts to 8 and top-k to 2 for every layer.""","The hyperparameter table lists the number of experts (Nexperts) as 8, and the accompanying text confirms that 8 experts are used in each MoE layer."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled,"The quoted sentence from the Luccioni 2025a document states that approximately 22% of e-waste has been formally collected and recycled, directly answering the question."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,is_blank,is_blank,"""To be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected, whereas others were designed to be used as zero-shot or multi-task models, to allow comparisons both for different architectures on a given task and between tasks for the same architecture.""","The context explicitly states that the study sampled 88 models for analysis, providing the numeric answer directly from the cited passage."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.1%,0.1,%,is_blank,is_blank,"""1500B parameters but only 0.1% activated per token""","The context from document patterson2021#0063 explicitly states that the 1500‑billion‑parameter Switch Transformer activates only 0.1% of its parameters on each token, which directly answers the question."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3×,3,is_blank,is_blank,is_blank,"Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context explicitly states that raising GPU utilization to 80% reduces the overall carbon footprint by a factor of three, which is the requested value."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,"""The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).""","The passage explicitly states that the English portion of FLM‑101B required 28.22 zettaFLOPs for training, which directly answers the question."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,51.686 MWh,51.686,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Training energy (kWh) 51,686","The table in Luccioni et al. (2024) lists the training energy for the BLOOMz‑7B model as 51,686 kWh, which corresponds to the pre‑training energy of the original BLOOM‑176B model. Converting 51,686 kWh to MWh yields 51.686 MWh, which is the requested energy consumption."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.94,43.94,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""Results. On average, FLM-101B achieves a score of 43.94,""","The document states that, on the Open LLM Leaderboard, FLM-101B’s average performance across the four tasks is 43.94, which is the final average score requested."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"‘the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.’","The 2025 paper cites Jevons’ Paradox, an economic principle where efficiency improvements can lead to higher overall consumption, explaining why AI efficiency gains might not yield net environmental benefits."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 – 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"""global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027""","The cited passage explicitly states the projected range of global AI water withdrawal for 2027 as 4.2 to 6.6 billion cubic meters, which directly answers the question."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context includes figure captions and plot axes but does not contain a specific numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU. Therefore, the answer cannot be determined with confidence from the documents."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x,2.2,x,cottier2024,https://arxiv.org/pdf/2405.21015,"""we estimated that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.""","The cited passage states the median ratio of total compute required for model development to the compute of the final training run is 2.2, which is the figure sought by the question."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""",The quoted sentence directly states that the total wall‑clock time required to train FLM‑101B using a growth strategy is 21.54 days.
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,dodge2022,https://arxiv.org/pdf/2206.05229,"Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8)∗13.8 = 103.5 MWh, or 103,500 kWh","The partial training of the 6.1 billion‑parameter transformer consumed 13.8 MWh over 8 days. Extrapolating to the full 60‑day run (60/8 times longer) yields 103.5 MWh, as stated in the document."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).""","The cited passage from the Schwartz 2019 document states that Red AI is on the rise, not declining, even in the presence of diminishing returns from increased cost, thus contradicting the claim that Red AI is on the decline."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025;li2025b;patterson2021,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2104.10350,"""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity"" (jegham2025). ""GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh"" (li2025b). ""Its energy consumption is 1287 MWh"" (patterson2021).","All three cited documents state that GPT‑3 training consumes 1,287 MWh of electricity. The consistent figure across multiple sources confirms the answer."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",False,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.""","The cited passage states that the majority of experimentation uses GPUs at 30–50% capacity, which is far below 80%. Therefore the claim that the majority uses GPUs at over 80% capacity is false."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,is_blank,is_blank,"""AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.""",The quoted sentence explicitly names the AI Energy Score project and states its goal of providing a standardized method for comparing inference efficiency across AI models.
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,is_blank,is_blank,"""global carbon emissions for 2020 dropped by 6.4%""","The context explicitly states that global carbon emissions fell by 6.4% in 2020, indicating the reported drop during the COVID‑19 pandemic."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,About 60 years,60,years,stone2022,https://arxiv.org/pdf/2211.06318,"""Since the field’s inception sixty years ago.""","The context states that AI has been in existence for sixty years, which, relative to the year 2025, corresponds to an approximate age of 60 years."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"""we set the same number of experts to 8 and top-k to 2 for every layer.""","The quoted sentence from the JetMoE‑8B description specifies that the top‑k value is 2, meaning two experts are activated for each token in every layer."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain the specific numeric value for the longest kernel execution time of the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU. Therefore the answer cannot be determined with confidence from the documents.
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22,22,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy""","The Amazon 2023 sustainability report explicitly states that 22 AWS data center regions achieved 100% renewable energy matching, directly answering the question."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit information specifying the hardware processor used for the experimental setup of energy‑efficient local inference in financial sentiment classification. Therefore, a confident answer cannot be derived from the documents given."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a specific numeric value for the maximum batch size of BlackMamba with sparse fine‑tuning on the GSM8K dataset for an NVIDIA A40 GPU with 48 GB memory. Therefore a confident answer cannot be extracted from the documents.
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context includes a 44% energy reduction for a 77 ms TPOT target but does not provide a percent reduction for a 100 ms TPOT target for Llama 3.1 8B. Therefore, the documents do not contain enough information to answer the question confidently."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""…comparable to the emissions from approximately 272 transatlantic flights between Boston and London.""","The context states that GPT‑4o’s projected annual carbon emissions are comparable to the cumulative emissions of about 272 transatlantic flights, providing the required number."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The provided context states a 300,000‑fold increase in compute over six years, not 200,000‑fold, so the claim of a 200,000‑fold increase is not supported and is therefore false."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,is_blank,is_blank,"""In 2023, we added seven solar projects paired with battery energy storage systems to our portfolio in the U.S. We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.""","Both documents state that Amazon’s energy storage capacity reached 1.3 gigawatts in 2023, directly answering the question."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.""",The quoted sentence from the context explicitly states that the field was officially christened in 1956 at the Dartmouth workshop. This directly answers the question about the date.
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].""","The context from doc ebert2024 states that the global average power usage effectiveness (PUE) of data centers in 2023 was 1.58, which is the figure provided for AI‑dedicated data centers as well."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not contain any statement about the percentage of AI inference workloads in Asia powered by coal in 2023, so the answer cannot be determined with confidence."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B,2000000000,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8B total parameters but activates only 2B per input token during inference, so the answer is 2B."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided contexts do not contain a specific numerical value for the GHG emissions (in tCO2e) associated with pre‑training the Llama 7B model.
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules per token,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"""with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.""","The quoted passage states that for a max generation length of 512 tokens, LLaMA‑65B consumes roughly 3–4 Joules per output token. This directly answers the question with the specified energy per token value."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].""","The quoted sentence from Doc wu2021b explicitly states that about 770 million people lack stable electricity supply, confirming the claim as true."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.""","The cited passage from Morrison 2025 explicitly states that operational impacts encompass GHG emissions from servers and data‑center cooling, contradicting the claim that they do not include such emissions."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The quoted sentence directly states that quantizing RM2 from 32-bit to 16-bit reduces its overall model size by 15%, which is the exact percentage requested."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,2.195 L/kWh,2.195,L/kWh,is_blank,is_blank,"""For Jupiter, the cluster used to train all models but the 13B, we assume a ... total water usage effectiveness (WUE) of 1.29 liters per kWh. For Augusta, the cluster used to train the 13B, we assume ... a total WUE of 3.1 liters per kWh.""","The context provides two WUE values for Google’s AI‑dedicated clusters: 1.29 L/kWh for Jupiter and 3.1 L/kWh for Augusta. Averaging these two values yields (1.29+3.1)/2 ≈ 2.195 L/kWh, which is the requested average WUE for 2024."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",TRUE,1,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy.","The quoted sentence from the Patterson 2021 document directly states that sparsely activated DNNs use less than one‑tenth the energy of dense counterparts while maintaining accuracy, confirming the statement is true."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,"""up to 53% when conducting sparse fine‑tuning with batch size = 1""","The provided passage states that in BlackMamba sparse fine‑tuning on a NVIDIA A40‑48GB GPU, the optimizer stage accounts for up to 53% of the total running time when the batch size is 1, which directly answers the question."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s exp,"The cited document states that while the 5‑10% figure is often quoted, the calculations supporting it are not detailed and the reasoning behind the estimate is unclear, indicating a lack of clear, publicly available calculations and sound scientific grounding."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",25000,USD,is_blank,is_blank,"""Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.""","The quoted sentence from the Schwartz 2019 document explicitly states that Grover’s training on 256 TPU chips for two weeks cost an estimated $25,000, providing the required numeric answer."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally""","The quoted sentence from ebert2024 directly states the average global PUE for 2023 as 1.58, which is the value required."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"""Gemini Ultra has the highest fraction of R&D staff cost at 49%""","The context explicitly states that Gemini Ultra’s R&D staff costs (including equity) represent 49% of the total amortized model development cost, which is the percentage asked for."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The seventeen-member Study Panel""","The context explicitly states that the inaugural 2015 Study Panel was a seventeen‑member panel, giving the count of 17."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,is_blank,is_blank,is_blank,"""reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.""","The cited passage explicitly states that manufacturing accounts for 74% of a client device’s total carbon footprint, providing the required percentage answer."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B). Even though Small has nearly twice the parameters, the larger Small model can consume less energy than Mini as batch size grows.","The passage explicitly states that while larger models tend to use more energy, there are documented instances where a larger model consumes less energy during inference. Therefore, the claim that a model with more parameters will always consume more energy is false."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.""","The quoted passage explicitly states that the Study Panel did not find any cause for concern that AI poses an imminent threat to humankind, contradicting the claim that they are concerned."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages discuss emissions, energy usage, and related topics but contain no information about the distance between Earth and the Sun."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts includes a specific figure for CO2 emissions from OpenAI API requests in January 2024, so the answer cannot be determined from the given documents."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The necessary energy per 1,000 queries value (0.083 kWh/1k queries) is not present in any of the supplied context passages, so the calculation cannot be performed with the given information."
q080,True or False: The AlphaGo program defeated the human Go champion.,TRUE,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match""","The quoted passage explicitly states that AlphaGo beat the human Go champion, confirming the statement is true."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,2.875 Wh,2.875,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.,"The quoted sentence from the Jegham 2025 document states that GPT‑4o, which corresponds to the o3 model, uses approximately 2.875 Wh for a long prompt. This directly answers the question with the specified unit."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9×,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.""",The referenced document explicitly states that the explosive growth in AI use cases drove a 2.9‑fold increase in AI training infrastructure capacity over the 1.5‑year period from 2019 to 2021.
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"14,380",14380,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.""; ""Europe 1,220 3,000+""; ""India 3,800 7,200+""; ""We deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles.""; ""We nearly doubled the number of EVs in our Indian delivery fleet to more than 7,200, including 3,600 electric delivery vans.""","The U.S. vans increased from ~2,600 to 11,800 (+9,200), Europe from 1,220 to ~3,000 (+1,780), and India from 3,800 to ~7,200 (+3,400). Adding these increases yields a total of 9,200 + 1,780 + 3,400 = 14,380 vans added across 2022 and 2023."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",language model pretraining,language model pretraining,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""language model pretraining typically exceed the 'five cars' estimate""","The context shows that the five‑cars carbon footprint estimate comes from measuring emissions of language‑model pretraining, which is a specific, infrequently performed AI process. The quoted sentence directly links the estimate to pretraining, supporting the answer."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,fernandez2025,https://arxiv.org/pdf/2504.17674,"""Continuous batching mitigates this by dynamically replacing completed requests with new ones"" (Doc fernandez2025#0045)","The quoted sentence explicitly states that continuous batching reduces idle GPU time by dynamically replacing completed requests with new ones, which is the definition of the requested batching strategy."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Meta's Llama 3 family emitted 11,390 tCO₂e, which is over 40 × the five‑cars estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the","The quoted passage states the exact emissions from Meta’s Llama 3 pre‑training (11,390 tCO₂e) and indicates this is more than forty times the five‑cars reference value, directly answering both the numeric value and the comparison."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams",1594,grams,is_blank,is_blank,"For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle.","The context explicitly states that stable‑diffusion‑xl‑base‑1.0 emits 1,594 g CO2eq per 1,000 inferences, so that is the required value."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"""The entire alignment process takes 60 H100 GPU hours.""",The cited sentence from the JetMoE‑8B alignment description explicitly states that the combined dSFT and dDPO fine‑tuning requires 60 H100 GPU hours.
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,is_blank,is_blank,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).""","The quoted passage from the 2025 study’s appendix lists the minimum GPU energy usage of 0.06 Wh for bert‑tiny and the maximum of over 3,426 Wh for Command‑R Plus for 1,000 inference queries, giving the requested range."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist).,"The cited document explicitly states that researchers do not believe a universal, one‑size‑fits‑all approach to AI ethics and sustainability can be developed, contradicting the claim."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,is_blank,is_blank,"""The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].""","The context directly states the gross carbon intensity for the U.S. average mix, giving 0.429 kg CO2e per kilowatt‑hour, which answers the question for 2021."
q093,How many parameters does the largest T5 model have?,11B parameters,11,B,is_blank,is_blank,The largest size has 11B parameters,"The passage explicitly states that the largest T5 model has 11 billion parameters, which is the answer. "
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training..."" ""We used the Hivemind framework for all of our experiments, as it provided the base for training on spot instances in high latency, low bandwidth networks."",",The excerpts identify Hivemind as a PyTorch‑based decentralized framework that the authors employed to run distributed spot instance training across multiple clouds and continents.
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the notion of transparency in AI can be expanded to encompass ‘social transparency’, which involves integrating socio‑technical aspects in the description and understanding of AI systems [56].""","The passage explicitly defines the expanded form of transparency as ""social transparency,"" indicating that this is the proposed term for including socio‑technical and societal/environmental aspects in AI transparency."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied context does not contain any information about classification experiments on German public administration texts or which sentence‑embedding model achieved the highest accuracy. Therefore the answer cannot be determined from the provided documents.
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"""the company’s data center water consumption increased by ∼20% from 2021 to 2022""","The excerpt from Doc [li2025b] explicitly states that Google’s data center water consumption rose by approximately 20% between 2021 and 2022, which answers the question. The percentage figure is directly taken from the quoted sentence."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,chen2024,https://arxiv.org/pdf/2405.01814,"""We develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.""","The quoted sentence from the Chen et al. 2024 paper identifies Lamina as the LLM inference system that uses model-attention disaggregation, directly answering the question."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,101 4 4 12 192 2160 165 52.88%,The table in Doc [li2025a] lists the FLOPs utilization for each growth stage; the final 101B stage shows a utilization of 52.88%.
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"SLO Evaluated PoliciesSelected InstancesCoff(%) TPS(avg.)Total Price($) 100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13 Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699","The table shows Max‑Performance selected g6e.xlarge at $2.699, while InferSave selected g4dn.xlarge at $2.13. The price difference is $0.569, which is 0.569/2.13 ≈ 0.267, i.e., about 26.7% more expensive."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,59%,59,%,erben2023,https://arxiv.org/pdf/2306.03163,"""intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).""","The text reports a 41% performance drop for the NLP experiment when training is spread across four continents (C‑8) versus fully local (A‑8). Therefore, the throughput achieved is 100 % – 41 % = 59 % of the local throughput."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810×,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Efficiency Optimization: Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer‑based universal translation model by 810×.""","The document states that combining platform‑level caching, GPU acceleration, low‑precision formats, and algorithmic changes achieves an overall operational carbon footprint reduction of 810× relative to a CPU server baseline, directly answering the question."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5 billion liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""3.5B Liters of water returned to communities from replenishment projects in 2023"" and ""AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.""","Both excerpts state that 3.5 billion liters of water were returned to communities through Amazon’s replenishment projects in 2023, directly answering the question."
q094,What is the total number of parameters in the JetMoE-8B model?,8B,8,B,shen2024,https://arxiv.org/pdf/2404.07413,"""JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."" (Doc [shen2024])","The provided document states that JetMoE-8B contains 8 billion total parameters, which directly answers the question."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,cottier2024,https://arxiv.org/pdf/2405.21015,"""Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.""","The cited passage states that, on average, 44 % of the amortized hardware and energy cost is attributed to AI accelerator chips, directly answering the question."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents provided do not contain a clear statement of the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems","The passage explicitly names the Finnish project as ETAIROS and states it proposed the integration of ethics, sustainability, design and foresight for governance of AI systems."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3700000,GPUs,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023)""","The quoted sentence from the 2025 paper specifies that NVIDIA shipped 3.7 million GPUs in 2024, which is the number requested. The answer reflects that figure exactly."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""most expensive publicly-announced training runs to date are OpenAI’s GPT‑4 at $40M and Google’s Gemini Ultra at $30M.""","The context explicitly states Gemini Ultra’s amortized training cost as $30M, which is the figure requested."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40,M USD,cottier2024,https://arxiv.org/pdf/2405.21015,"Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context states that the amortized hardware and energy cost for GPT‑4 is $40 million, indicating the estimated amortized training cost."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,"""Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks."" ""With the best configuration, zero‑shot reduced the consumption of about 7%, whereas one‑shot and few‑shots decreased their consumption of about 99% and 83%, respectively.""","The cited statements from the document explicitly state that custom tags lower energy consumption for zero‑shot, one‑shot, and few‑shot prompt engineering techniques in source code completion tasks, confirming the claim as true."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with…"" and later, ""These provisions lack sufficient emphasis on environmental factors. Although environmental protection is included in the Act’s objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required."",","The AI Act requires risk assessments for GPAI models with systemic risk, but the documents state that environmental risks are not explicitly mandated to be included in those assessments. Therefore the statement that environmental risks must be included is false."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""115 books would produce the same amount of CO2 as a single Amazon Kindle device""","The life‑cycle assessment cited in the document states that 115 print books emit the same amount of CO₂ as one Amazon Kindle e‑reader, directly providing the numeric comparison needed."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""per-household health burden could be 200x more than that in less-impacted communities.""","All cited documents state that disadvantaged communities can experience up to 200 times the per-household health burden compared to less-impacted areas, establishing the factor as 200."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.1,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,Achieving a Power Usage Effectiveness (PUE) of about 1.10,"The quoted sentence from Wu 2021a states that Facebook’s data centers achieve a PUE of about 1.10, which directly answers the question."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not contain any mention of a DS Llama 70B model or the FKTG dataset, so the energy consumption value cannot be determined from these documents."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 µg/m³,9,µg/m³,han2024,https://arxiv.org/pdf/2412.06288,"""the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3""","The context explicitly states that the EPA’s tightened primary standard for the annual average limit of PM2.5 is 9 µg/m³, which is why the answer is 9 µg/m³."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,gCO2eq/kWh,gCO2eq/kWh,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"""grams of CO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh)""","The context from luccioni2023 defines the emissions metric as grams of CO2 emitted per kilowatt‑hour of electricity, i.e., gCO2eq/kWh, which matches the description ‘CO₂ emissions per unit of electricity consumed’."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",rebound effect,rebound effect,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""rebound effects, in which improved efficiency of a given technology can lead to increased usage of it and therefore increase the overall consumption of resources""","The context defines rebound effects as the phenomenon where technological efficiency gains lead to higher usage and greater overall resource consumption, which matches the question’s description."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,image generation 2.907 3.31,"Table 2 gives the mean energy per 1,000 queries; the row for image generation lists a mean of 2.907 kWh, which is the average energy consumption for 1,000 image generation inferences."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.75,0.75,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Mistral-small 0.70 0.67 0.65 0.67 0.020\nMistral-small 0.73 0.70 0.69 0.70 0.015,"The emissions for Mistral-small drop from 0.020 kg CO2 before optimization to 0.015 kg CO2 after. Dividing 0.015 by 0.020 gives a multiplier of 0.75, indicating a 25 % reduction."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy""",The quoted sentence from the Power Hungry Processing study explicitly states that the total energy consumed for all model experimentation and evaluation was 754.66 kWh.
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The excerpts from the Dodge et al. 2022 paper included in the context do not contain the total number of parameters of the large language model they analyzed, so the answer cannot be determined with confidence."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"""we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters""","The document explicitly states that FLM-101B contains 101B parameters, which directly answers the question."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Training energy (kWh) 51,686 ... Finetuning energy (kWh) 7,571""","The study reports 51,686 kWh for training and 7,571 kWh for fine‑tuning the BLOOMz‑7B model. Adding these gives 51,686 + 7,571 = 59,257 kWh, which is the combined energy cost."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The table lists the number of inferences needed for each BLOOMz model to reach parity with its training and fine‑tuning energy cost. For BLOOMz‑7B the value is 592,570,000 inferences, which directly answers the question."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any information about a dataset used for German nuclear waste site objection texts, so the answer cannot be determined with confidence."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents contain information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals, so a confident answer cannot be derived from the provided context."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Model Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128""","The table in the cited document lists the bare minimum GPU count for each LLaMA variant. For the 13B model, it shows 1 NVIDIA A100‑80GB GPU is sufficient, whereas two V100 GPUs are needed. Thus the minimum number of A100 GPUs required is one."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit 2030 projection data for county-level per-household health costs in West Virginia, so the answer cannot be determined with confidence."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific numeric value for the total carbon emissions avoided by pruning and quantizing LLMs in 2023. They only mention a percentage reduction (up to 45%) but no absolute tCO2e figure.
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""84% of LLM usage is through models with no disclosure""","The passage explicitly states that 84% of token usage on OpenRouter in May 2025 went through models that did not disclose their environmental impact, which directly answers the question."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,3 passengers,3,passengers,patterson2021,https://arxiv.org/pdf/2104.10350,"""yielding 1.2t of CO 2 e per passenger round trip. Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.""","The Evolved Transformer NAS emits 3.2 tCO2e. Dividing by the per‑passenger round‑trip emission of 1.2 tCO2e gives 3.2/1.2≈2.7, which is rounded to about 3 passengers."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.,"The quoted passage explicitly states that the hybrid 2 A100 + 1 A10G configuration achieves a 24% cost savings compared to using A100s only, which directly answers the question."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied passages provide a specific freshwater consumption figure for Meta’s Llama 3 inference serving clusters in 2024. The documents mention 2023 data and general estimates, but no 2024 cluster‑level figure is available."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied context does not contain any explicit numeric value for the hourly price of an NVIDIA H20 as reported by Chen et al. (2025).
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give an estimate of total energy consumption (30 GWh) for the JetMoE project but do not provide the training budget or the total number of GPU hours used. Without both the cost and the hours, a cost per GPU‑hour cannot be calculated with confidence."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Model Size V100 32GB A100 80GB
Count Max. Batch size
7B 1 64 1 64","The table lists the bare minimum hardware for LLaMA‑7B, indicating one A100‑80GB GPU is sufficient for inference without compression or quantization."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,percent,han2024,https://arxiv.org/pdf/2412.06288,"""This is equivalent to approximately 44% of the data centers’ total electricity cost.""","The context explicitly states that the 2023 public health cost of U.S. data centers is about 44% of their total electricity cost, indicating the percentage requested."
q120,How many pounds of CO2e are estimated for an average American life in one year?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a statement or estimate of CO₂e in pounds for an average American life in one year. The available passages refer to emissions for model training, households, or specific workloads, but do not provide the requested figure for an average life span."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",122%,122,%,han2024,https://arxiv.org/pdf/2412.06288,"Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34","The table for Altoona, IA shows the health cost as 2.51 million $ and lists the percentage of electricity cost as 122%, indicating that the health cost was 122% of the electricity cost."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""In fact, most carbon foot print analyses gather the information manually by writing to authors.""","The cited passage states that most carbon footprint analyses are collected manually by contacting authors, contradicting the claim that they are gathered automatically without author contact."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,3 runs,3,runs,li2025b,https://arxiv.org/pdf/2304.03271,"""carbon footprint is 1.8 × higher than that of the open‑source Meena model [30] and one‑third of GPT‑3’s training footprint."" – wu2021a#0047
""GPT‑3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29]."" – li2025b","The Meena model’s training footprint is one‑third of GPT‑3’s, so its energy use is 1287 MWh / 3 ≈ 429 MWh. Dividing GPT‑3’s total training energy by Meena’s energy gives 1287 MWh / 429 MWh = 3, meaning three full Meena training runs would equal the energy of one GPT‑3 run."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",Approximately 1.0 billion inferences.,1035930000,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""we estimate a full training run would consume approximately 103,593 kWh."" (dodge2022#0054)  ""Inference energy (kWh) 1.0 × 10−4"" (luccioni2024)","The full training of the 6.1‑B‑parameter model consumes about 103,593 kWh. BLOOMz‑7B uses roughly 1.0×10⁻⁴ kWh per inference. Dividing the training energy by the per‑inference energy yields ≈1.04×10⁹ inferences, i.e., about one billion."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,dodge2022,https://arxiv.org/pdf/2206.05229,"If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The quoted passage from the Dodge 2022 document explicitly states the estimated CO2 emissions range for a full training run of a 6.1 billion‑parameter transformer model, giving 21–78 metric tons."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization""","The quoted passage explicitly states that the deployment techniques (including quantization) achieved up to a 45% reduction in carbon emissions, confirming the statement as true."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,United Kingdom 36 901,The table of Amazon’s renewable energy projects announced as of January 2024 lists 36 projects located in the United Kingdom. This directly answers the question about the number of UK projects announced.
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""were only able to collect 95 answers""","The documents state that after contacting more than 500 authors, the researchers collected only 95 responses, confirming the number of answers obtained."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""we introduce the granularity metric, the ratio of calculation to communication time""",The context explicitly states that the authors introduced the granularity metric to quantify the ratio of calculation to communication time for distributed training across continents.
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently""","The provided passage defines freshwater taken from ground or surface sources, either temporarily or permanently, as ""Water withdrawal""."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"""forms a Study Panel every five years to assess the current state of AI.""",The quoted sentence explicitly states that the Standing Committee forms a Study Panel every five years.
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not specify the execution time for a sparse BlackMamba model fine‑tuned on an NVIDIA A40‑48GB with a batch size of 84, and the maximum supported batch sizes are far smaller (e.g., 20 for CS, 8 for MATH)."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,li2025b,https://arxiv.org/pdf/2304.03271,Apple reports that its supply chain accounts for 99% of its total water footprint [23].,"The quoted statement directly states that 99% of Apple’s total water footprint is attributed to its supply chain, providing the required percentage."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25 trillion tokens,1250000000000.0,tokens,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context explicitly states that JetMoE-8B was pre‑trained on 1.25 trillion tokens, which is the required number of tokens for the pre‑training phase."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]""","The context explicitly states that each U.S. household had an average of 25 connected devices in 2021, which directly answers the question."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011","The context explicitly states that Watson beat human contenders to win the Jeopardy challenge, so the claim that it did NOT beat them is false."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year""","The context states the deal could add 640 percent more emissions than Microsoft’s yearly removal targets. 640 percent corresponds to 6.4 times the target, so the answer is 6.4 times more."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.""",The quoted sentence from the Griggs et al. 2024 paper explicitly states that Mélange can reduce deployment costs by up to 77% in conversational chat settings.
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8 to 3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)""","The cited passage from luccioni2025c provides the lowest and highest publicly reported pre‑training energy consumptions, giving a clear range of 0.8 to 3,500 MWh."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80–90%,"[80, 90]",is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].""","The passage from the Patterson 2021 document states that NVIDIA estimated the inference portion of the ML workload to be between 80% and 90%, giving the required percentage range."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,"""one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].""","The cited study explicitly states that 10 to 50 GPT‑3 queries use roughly 0.5 L of water, providing the numeric range for the answer."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10–50 completions,"[10, 50]",is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.""","The cited passage directly states that a 500 mL bottle of water supports between 10 and 50 medium-length GPT‑3 completions, which yields the answer range 10–50."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024;rubei2025,https://arxiv.org/pdf/2310.03003;https://arxiv.org/pdf/2501.05899,"""8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.""",The cited sentence explicitly states that the bare minimum configuration for LLaMA‑65B inference without compression or quantization is 4 A100 GPUs with 80 GB each.
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg CO2eq,178.97,kg,luccioni2024,https://arxiv.org/pdf/2311.16863,"""in total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.""","The passage explicitly states that the entire Power Hungry Processing study emitted 178.97 kg of CO₂ equivalent, which is the total amount of CO₂eq generated during the study."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650","The MT‑Bench table in the referenced document lists JetMoE‑8B‑Chat with a score of 6.681, which is higher than the 6.650 score for Llama‑2‑13b‑Chat, indicating that JetMoE‑8B‑Chat surpassed Llama‑2‑13b‑Chat after alignment."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000",10000,round trips,han2024;luccioni2025c,https://arxiv.org/pdf/2412.06288;https://arxiv.org/pdf/2506.15572,"""training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.""","The cited sentence directly states that training a Llama‑3.1‑scale model generates air pollutants comparable to over 10,000 LA‑NYC round‑trip car journeys, thus providing the required numerical value."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.5164 per hour,7.5164,$ /h,is_blank,is_blank,"(4.69/2.29) × 3.67 = $7.516 for H100. In the table of on-demand prices, the H100 entry is listed as 7.5164 $/h.","The context provides the calculation for the normalized H100 price and the table lists the exact value 7.5164 $/h, which is the on‑demand hourly price used in the Griggs et al. 2024 evaluation."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include a specific numeric value for the amount of water used for cooling during OpenAI's GPT‑4 training run, so the answer cannot be determined with confidence from these documents."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",1000× larger,1000,is_blank,is_blank,is_blank,"""with GPT‑3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size."",  Fig. 2 in Doc [wu2021a#0013]","The context explicitly states that achieving a BLEU score increase from 5 to 40 for a GPT‑3‑based translation task requires a model 1,000 times larger."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.""  ""GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.""","Both quoted passages indicate that GPT-4o mini consumes more energy per query than GPT-4o, contradicting the statement that it consumes less. Therefore the statement is False."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$7.22 per hour (≈$5200/month ÷ 720 hrs),7.22,USD,griggs2024,https://arxiv.org/pdf/2404.14527,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.""","The context gives a monthly cost of $5,200 for two A100 GPUs. Dividing that by 30 days × 24 hours = 720 hours yields roughly $7.22 per hour. This matches the requested hourly estimate."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,GPU hours,is_blank,is_blank,"Doc [shen2024#0001] states: ""JetMoE-8B... 30,000 H100 GPU hours.""  Doc [shen2024#0008] also states: ""JetMoE-8B... 30,000 H100 GPU hours.""","Both documents explicitly list 30,000 H100 GPU hours as the pre‑training effort for JetMoE‑8B, so the answer is 30,000."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",49.7%,49.7,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Other Gender Men Women 49.7%50.0%,The table of Amazon’s workforce by gender shows that 49.7% of employees identified as men in 2023.
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"""the largest training runs will cost more than a billion dollars by 2027"",""the amortized cost of frontier training runs will exceed one billion dollars by 2027""","The cited passages explicitly state that, following the observed growth trend, the cost of the most expensive training runs will surpass one billion dollars by 2027, establishing the answer."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,OpenAI published the total number of floating point operations to train their model: 3.14E+23,"The context states that OpenAI released the total FLOPs for GPT‑3 as 3.14 × 10^23, which is the required number of floating point operations."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied context passages state a specific top‑1 accuracy value for AlexNet 2012. Therefore I cannot provide a confident numeric answer based on the documents alone.
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"BLOOMz-7B has been downloaded 606,096 times; Inference energy (kWh) 1.0 × 10−4","Each download triggers 1 million inferences, each costing 1.0 × 10⁻⁴ kWh. 1 million × 1.0 × 10⁻⁴ kWh = 100 kWh per download. 606,096 downloads × 100 kWh = 60,609,600 kWh, which is 60,609.6 MWh. "
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.""","The document states that FLM-101B training uses 24 servers, each containing 8 A800 GPUs. Multiplying 24 by 8 gives a total of 192 GPUs."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,luccioni2023;chung2025,https://arxiv.org/pdf/2302.08476;https://arxiv.org/pdf/2505.06371,"""Estimations using TDP are nearly always an overestimation since it is rare for a GPU … to draw its maximum power at every moment in time. … can lead to a worst‑case overestimation of energy consumption."" – Doc [chung2025]; ""The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used – while it assumes 100% GPU utilization, it is the most accurate estimate possible without energy usage tracking during training."" – Doc [luccioni2024#0089]",The cited passages show that TDP‐based estimates tend to overestimate actual GPU power draw and are not considered accurate. Therefore estimating GPU energy consumption solely from TDP is not a reliable or accurate method.
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The 2025 paper reports that after the 2022 peak, disclosures declined—there was a reversal rather than a continued increase—so the statement is false."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO₂ emission amount for training with NAS (626,155 lbs) but do not include a specific emissions‑to‑driving‑distance ratio needed to compute the equivalent miles. Without that ratio, the driving distance cannot be calculated with confidence from the given context."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"""FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.""","The context explicitly states that training FAIR’s RoBERTa on 160 GB of text required about 25,000 GPU hours, so that is the answer."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide a CO2 emission estimate for NAS (284 tCO₂e) but do not give a direct comparison to the CO₂ emissions of an average American lifetime, so the requested equivalence cannot be determined from the provided passages."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The context does not provide sufficient explicit numeric data linking the 101B stage throughput (165 teraFLOPs/s) to a known duration or token count that would allow a confident calculation of total FLOPs for that stage. Hence the answer cannot be derived with confidence from the provided documents.
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons",47400,metric tons,is_blank,is_blank,"""avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.""","The context from Amazon’s 2023 Sustainability Report states that the 58 MW of on‑site solar systems generate 123,000 MWh annually and avoid approximately 47,400 metric tons of CO₂e each year versus nonrenewable electricity sources. This figure directly answers the question about avoided emissions from Amazon’s on‑site solar energy systems."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any information about Yelp sentiment analysis benchmarks or comparisons between traditional models and large language models. Therefore, the answer cannot be determined with confidence from the given documents."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context documents do not contain a specific figure for gallons of water consumed per ChatGPT user session in 2023, nor enough data to calculate it reliably."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 GPUs,8,GPUs,rubei2025;samsi2024,https://arxiv.org/pdf/2501.05899;https://arxiv.org/pdf/2310.03003,"""8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model""","The cited statement and Table II in the documents specify that the bare‑minimum configuration for the 65B LLaMA model on NVIDIA V100 32 GB GPUs is 8 GPUs, confirming the required number."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes""","The context explicitly states that scaling a 0.42 Wh short query to 700 million daily queries yields annual electricity use equivalent to 35,000 U.S. residential households, directly providing the numerical answer."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The cited passage from document ""patterson2021"" explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter during the run of the Evolved Transformer was 1.11. This value directly answers the question."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons,""","The 2025 paper by Luccioni et al. explicitly states Microsoft’s global water consumption rose by 34% from 2021 to 2022, which directly answers the question."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit numerical energy consumption values for the Llama 3.1 70B model deployed on one versus two nodes, so the factor of increase cannot be determined from the documents alone."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"""yielding a total of approximately 772 billion GPT-4o queries in 2025""","The context states that the analysis estimates about 772 billion GPT‑4o queries will be made in 2025, which is the required total."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,Megatron‑LM,Megatron‑LM,is_blank,fernandez2025;samsi2024,https://arxiv.org/pdf/2504.17674;https://arxiv.org/pdf/2310.03003,"""Efficient large-scale language model training on gpu clusters using megatron‑lm."" (Fernandez 2025) – and ""the largest NVIDIA Megatron‑LM model required 3,072 A100 GPUs."" (Samsi 2024)","Both documents state that Megatron‑LM is the framework used to train and deploy large language models across many GPUs and nodes, confirming it as the answer."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""R&D staff costs including equity are between 29% and 49% of the total amortized cost.""","The passage explicitly states that for the four notable models (GPT‑3, OPT‑175B, GPT‑4, and Gemini Ultra), R&D staff costs—including equity—make up 29% to 49% of the total amortized cost, which is the requested percentage range."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312 GB,5.312,GB,kim2025,https://arxiv.org/pdf/2504.11816,"""When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.""","The provided context explicitly states that for the OPT-2.7B model on a g4dn.xlarge instance, the KV Cache size grows to 5.312 GB when the batch size is 32, which directly answers the question."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"3) Open‑source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The context states that open‑source general‑purpose AI models are exempt from reporting requirements unless they pose a systemic risk, matching the question’s claim that they are fully exempt except in that case."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit information about the number of AI training runs conducted globally on renewable-only power in 2022, so a confident answer cannot be derived."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The US national datacenter average in 2018 was 1.58, which is the value used ; In 2020, it was 1.59.""","The context from Patterson 2021 explicitly states that the US national datacenter average PUE in 2020 was 1.59, providing the required value."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Table 3: OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0,"Table 3 lists the average OpenLLM leaderboard scores for four models, with JetMoE-8B being the last column. The value in that column is 53.0, which is the final average score for JetMoE-8B."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""... 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it…""",The passage from Doc [luccioni2025c] explicitly states that 53 % of the 100 analyzed news articles cited the 3 Wh estimate or the claim that a single query is ten‑times more energy‑intensive than a Google search.
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",2.2 qps,2.2,qps,xia2024,https://arxiv.org/pdf/2408.04693,Dense(bsz=1) … 2.2,"The figure for dense Mixtral-CS-A100-40GB lists a throughput of 2.2 queries per second for batch size 1, which is the ground‑truth value requested."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,is_blank,is_blank,"""Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall"" (Doc samsi2024#0060). ""Increasing the number of shards always increases the wattage"" (Doc samsi2024#0054).","The quoted statements from the documents explicitly state that as the shard count rises, the energy cost per response for the 65B model rises, confirming the claim."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a;wu2021b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2108.06738,"""In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide."" (luccioni2025a)  ""In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally."" (wu2021b)","Both cited documents report that the four companies were responsible for roughly 30% of corporate PPAs in 2020, so the percentage is 30%."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"""Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU pe"" (Doc [kim2025#0001]) and ""CTCF is a linear transformation function that adjusts theoretical computation time to match actual execution time"" (Doc [kim2025#0081])",The quoted passages explicitly name the function as the Compute Time Calibration Function (CTCF) and describe its purpose of improving instance selection accuracy by correcting theoretical GPU performance estimates to match actual measurements.
q213,Which software package was used to measure energy consumption during inference runs?,Code Carbon package,Code Carbon,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference"" (doc [luccioni2024#0034]); ""measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking"" (doc [morrison2025])","Both cited documents explicitly state that the Code Carbon package was employed to measure energy consumption during inference runs, confirming the answer."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",15-77%,"[15, 77]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) for the short‑context Arena dataset.","The cited passage states that for the Arena short‑context workload with a 120 ms SLO, Mélange’s cost savings range from 15% to 77% compared to single‑GPU baselines, which directly answers the question."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"""the total public health cost of about $6.7 billion, or $47.5 per household, in 2023.""","The context states that, under the average attribution method, U.S. data centers incurred a public health cost of approximately $6.7 billion in 2023. This figure directly answers the question and is supported by the quoted sentence."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4].","The cited passage explicitly states that open‑source general‑purpose AI models are excluded from the transparency obligations that require reporting of energy consumption to authorities, indicating the statement is false."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context explicitly states that 4‑bit quantization was performed using the open‑source tool Ollama, which also enables local deployment of LLMs."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Backblaze B2,Backblaze B2,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""we chose an independent S3 storage provider, Backblaze (B2) [4].""",The passage states that Backblaze B2 was selected as the independent S3 storage provider for sharding and streaming datasets to spot VMs that could terminate at any time. This directly answers the question.
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The provided document explicitly states that GPU theoretical performance per watt doubles every 3‑4 years, directly supporting the claim in the question."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",2.2e-5 kL,2.2e-05,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12‑inch silicon wafer weighs 125 grams and produces about 63 H100s.",The wafer weight gives ~1.984 g per H100. 0.1% of that mass is 0.001984 g (1.984×10⁻⁶ kg) of rare earth. 11 kL water per kg of rare earth yields 11×1.984×10⁻⁶ ≈ 2.2×10⁻⁵ kL of water per GPU.
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,is_blank,is_blank,"The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. Public health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The cited passages explicitly state that AI’s public health costs are unevenly distributed, contradicting the claim that they are evenly distributed across U.S. communities."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied passages explicitly state that runtime and energy consumption have a nearly linear relationship, so the answer cannot be derived from the provided documents."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""...a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D‑MA) on 1 Feb 2024""","The context explicitly states that Senator Edward J. Markey introduced the bill on February 1, 2024. This directly answers the question."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information from Chen et al. (2025) regarding the price per hour for an NVIDIA H100, so the answer cannot be determined with confidence."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Approximately 3.5 times greater,3.5,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4.1 nano remains among the most efficient proprietary models at 0.827 Wh"" and ""GPT-4o consumes around 2.875 Wh""","The energy consumption for a long prompt is 0.827 Wh for GPT‑4.1 nano and 2.875 Wh for GPT‑4o, giving a factor of 2.875/0.827≈3.5."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,3.7 years,3.7,years,cottier2024,https://arxiv.org/pdf/2405.21015,"""expected hardware lifetime would be 3.7 years.""","The provided context from Doc [cottier2024] explicitly states an expected GPU hardware lifetime of 3.7 years, which directly answers the question about the average lifetime before retirement in AI data centers in 2024."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not contain a clear numeric value for the total estimated net carbon emissions of FLM‑101B’s pre‑training.
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",0.99 s,0.99,s,xia2024,https://arxiv.org/pdf/2408.04693,"Table IV: ""Estimated Cost of Fine‑Tuning Mixtral on GS with Sparse MoE"" lists for A40 48GB a throughput of 1.01 qps.","The table gives the fine‑tuning throughput for a sparse Mixtral model on an NVIDIA A40 as 1.01 queries per second.  Throughput is the reciprocal of execution time per query, so the time for a single batch (batch size 1) is 1/1.01 ≈ 0.99 s.  Thus the total execution time per step is about 0.99 seconds."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,2,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""13B 2 64 1 64"" (Table II: bare minimum hardware required for LLaMA 13B on V100 and A100 GPUs)","The table lists the minimum number of GPUs needed for each model; for the 13B variant it shows 2 V100 GPUs are required, indicating that the bare minimum is two V100 GPUs."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","Gemma’s pre‑training emitted 1,247.61 tCO₂e, which is more than four times the five‑cars estimate (≈312 tCO₂e).",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the “five cars” number""","The context explicitly states that Gemma’s pre‑training emissions are 1,247.61 tCO₂e and that this is over four times the five‑cars estimate, which is typically around 312 tCO₂e. Thus the answer reflects both the reported value and its comparison to the five‑cars figure."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on‑premises computing workloads when the electricity AWS uses is matched with 100% renewable energy""","The passage explicitly states a 96% reduction in carbon footprint for North American workloads when powered by 100% renewable energy, which directly answers the question."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,is_blank,is_blank,Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).,"The quoted sentence directly states that training ELMo on 3 NVIDIA GTX 1080 GPUs took 2 weeks, which equals 336 hours."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96,96,GPUs,shen2024,https://arxiv.org/pdf/2404.07413,"""We conduct training on a cluster containing 12 nodes and 96 H100s.""","The context explicitly states that the training cluster has 12 nodes and uses 96 H100 GPUs, which gives the total number of GPUs used for training."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50–70%,"[50, 70]",percent,chung2025,https://arxiv.org/pdf/2505.06371,"GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].","The Chung 2025 document explicitly states that GPUs contribute 50–70% of the total provisioned power in a typical datacenter, which directly answers the question."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,"""fine‑tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.""","The context explicitly states that fine‑tuning a sparse Mixtral model on 2 million queries with an NVIDIA H100 GPU incurs a net cost of $3460, which directly answers the question."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)""","The context explicitly states that a single short GPT‑4o query consumes 0.42 Wh, which directly answers the question."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,"""Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021"" (Doc [dodge2022])
""PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021)"" (Doc [wu2021b])","Both documents state that Google’s hyperscale data centers reported a PUE of 1.10 in 2021, confirming the answer."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The cited passage directly provides the U.S. national average water consumption for electricity generation as 3.1 L/kWh, supporting the answer."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,Approximately 1.25×,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The quoted sentence directly states that for the 13B model the throughput on an A100 is about 1.25 times that of a V100, giving the required speedup factor."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts contain a global per-capita annual CO2e estimate in pounds; the only pound-based figure refers to a specific AI training workload, not an average human life."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""AI’s expanding operational footprint also contributes to electronic waste (e‑waste), which is now the fastest‑growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.""","The 2025 paper states that worldwide e‑waste generation in 2022 was 62 million tonnes, which is the figure requested."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"""…training the GPT‑3 language model in Microsoft’s state‑of‑the‑art U.S. data centers can directly evaporate 700,000 liters of clean freshwater…""","The context from Doc li2025b explicitly states that training GPT‑3 in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater, which is the value requested."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"""but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.""","The quoted sentence explicitly states that the Max‑Performance instance costs about 280% more than InferSave’s top choice for the 400 TPS SLO, providing the required percentage difference."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"""Figure 2: Average GPU power for a single node for the first 300 logging steps during OLMo 2 7B training. … When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.""","The quoted sentence explicitly states that during active training the average GPU power is over 600 W, which answers the question for the first 300 logging steps."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""reporting the ﬁnancial cost or “price tag” of developing, training, and running models to provide baselines for the investigation of increasingly efﬁcient methods."" ""Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice.""","The cited passages from Schwartz et al. state that Green AI practice includes reporting the financial cost of developing, training, and running models, confirming the statement is true."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"""a context window of 2,048 tokens""","The document states that the FLM-101B model uses a context window of 2,048 tokens, which directly answers the question."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20×,20,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021.""","The quoted passage from Wu et al. (2021) explicitly states that the recommendation and ranking model sizes at Facebook grew 20‑fold during the 2019 to 2021 period, which directly answers the question."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,is_blank,is_blank,"As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825).","The quoted sentence states that o3-mini has the highest cross‑efficiency score in the DEA‑based eco‑efficiency analysis, indicating it ranked highest among the evaluated models."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,is_blank,is_blank,"""Processor Average (Watts) ... TPU v2 221 ... V100 GPU 325 ...""",The table lists the average system power per processor as 221 Watts for TPU v2 and 325 Watts for the V100 GPU. Subtracting 221 from 325 yields a difference of 104 Watts.
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61–76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.""","The cited passage states that, when equity is excluded, computing hardware accounts for 61–76% of the total amortized cost for the four key models. This directly gives the required percentage range."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,is_blank,is_blank,current averages of less than 3 years for cell phones,"The cited passage states that the current average lifetime for cell phones is less than 3 years, directly supporting the claim that smartphones currently average lifetimes of under three years."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 – 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,"""a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7]""","The quoted sentence directly states the projected electricity consumption range for global AI in 2027, supporting the answer of 85–134 TWh."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,A10G,A10G,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"""A10G’s cost‑normalized batch size exceeds A100’s at short request lengths, leading to greater overall T/$.""","The cited passage shows that for short request lengths, which include single‑token classification tasks, the lower‑end A10G GPU provides better cost (and thus energy) efficiency compared to higher‑end GPUs such as the A100. Hence A10G is the most energy‑efficient architecture for single‑classification‑token generation."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,TRUE,1,is_blank,chung2025,https://arxiv.org/pdf/2505.06371,"""LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations per byte of memory loaded is low... Diffusion models are significantly more compute-intensive compared to LLM decoding.""","The quoted passage directly states that LLM inference draws much less power than diffusion models and explains that this is due to lower compute intensity and VRAM bandwidth bottlenecking, which matches the claim in the question."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"""The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO 2 emissions: CO2e = 0.954pt""","The citation from Strubell 2019 states that the EPA’s average CO₂ per kWh for U.S. electricity is 0.954 lbs/kWh, which directly answers the question."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million,150,million packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We delivered 150 million packages via EVs.""","The Europe section of the Amazon 2023 Sustainability Report states that 150 million packages were delivered via electric vehicles in Europe, directly providing the requested figure."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;""","The figure caption in the Dodge et al. paper states that the Flexible Start optimization can yield up to an 80% reduction in CO₂ emissions for the short DenseNet 201 job in the West US region, which is the maximum mentioned for that region."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,kim2025,https://arxiv.org/pdf/2504.11816,"""time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario ... The workload evaluates a total of 3000 requests.""","The online inference workload processes 128 input tokens plus 512 output tokens per request, totaling 640 tokens per request. With 3000 requests, the total tokens processed are 640 × 3000 = 1,920,000 tokens."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""from the least energy‑intensive task, text classification, with mean consumption of 0.002 kWh per 1,000 inferences, to the most energy‑intensive one, image generation, whose mean consumption is 2.9 kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.""","The text gives the mean consumptions for text classification (0.002 kWh) and image generation (2.9 kWh). Dividing 2.9 by 0.002 yields 1450, matching the factor stated in the passage."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration"".  ""The AI Act fails to address indirect greenhouse gas emissions from AI applications.""","The quoted passages from the ebert2024 document state that the AI Act does not mandate disclosure of GHG emissions from AI applications such as oil and gas exploration, so the statement is false."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,is_blank,is_blank,"""Amazon’s AWS only covered fifty percent of its power usage with renewable energy.""","The cited passage from Doc [schwartz2019#0060] states that AWS covered fifty percent of its power usage with renewable energy, which is the figure requested for 2018."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244,244,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"United States 244 17,706","The table in the Amazon 2023 sustainability report lists 244 renewable energy projects in the United States as of January 2024, which is the figure that answers the question."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about Yelp sentiment analysis benchmarks or a comparison between traditional models and large language models, so the truth value of the statement cannot be determined from these documents."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.","The quoted sentence from Doc griggs2024 directly states that Llama2‑70b at BF16 precision needs 2 NVIDIA A100‑80GB GPUs, so the answer is 2."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,is_blank,dodge2022,https://arxiv.org/pdf/2206.05229,"""The GPU alone accounts for 74% of the total energy consumption""",The provided passage explicitly states that during the BERT‑base training experiment the GPU was responsible for 74% of the total electricity consumption.
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context documents do not contain any explicit statement or numeric value indicating the amount of fiber optic cable installed worldwide for AI workloads in 2023. Therefore, a confident answer cannot be derived from the given material."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,is_blank,is_blank,We recommend measurement at the cumulative server level.,"The authors explicitly state that energy consumption should be measured and reported at the cumulative server level, indicating this level balances accuracy and feasibility."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Google reports a 48% increase in GHG emissions since 2019,"The passage explicitly states that Google’s 2024 environmental report indicates a 48% rise in greenhouse gas emissions compared to 2019, providing the required percentage increase."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping,Swapping,is_blank,chung2025,https://arxiv.org/pdf/2505.06371,"""It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.""","The quoted passage directly states that in overloaded conditions Swapping consumes less energy than Recomputation, attributing the difference to computational versus memory‑operation costs."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,"$800,000,000",800000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.""","The cited passage from the cited document states the upfront hardware acquisition cost for GPT‑4 as $800 million, which is the answer to the question."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,After Optimization Metrics following quantization and local inference techniques Llama 3.2 0.57 0.48 0.47 0.48; Phi 3.2 1.00 0.84 0.91 0.84; Qwen 0.80 0.81 0.80 0.81; Mistral‑small 0.73 0.70 0.69 0.70; Llava‑Llama 3 0.61 0.54 0.5 0.54,"The table lists accuracy and F1 for each model before and after optimization; all shown values increase (e.g., Llama 3.2 accuracy 0.45→0.48, F1 0.44→0.47; Phi 3.2 accuracy 0.82→0.84, F1 0.88→0.91; etc.). Thus, in the financial sentiment case study, accuracy and F1 scores always improved after optimization."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5%,28.5,%,is_blank,is_blank,"""… the net effect, with Jevon’s Paradox, is a 28.5% operational power footprint reduction over two years (Figure 8).""","The quoted sentence from the document specifies a 28.5% reduction in operational energy (power) footprint over the two‑year period, directly answering the question."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,about 70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,"""reducing inference computation by about 70% compared to Llama2-7B.""","The quoted sentence from the JetMoE-8B paper states that inference computation is reduced by roughly 70% relative to Llama2-7B, providing the required percentage."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",9.1% to 11.7%,"[9.1, 11.7]",%,fernandez2025,https://arxiv.org/pdf/2504.17674,"""projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024).""","The quoted passage from fernandez2025 cites McKinsey projections indicating data centers will consume between 9.1% and 11.7% of total U.S. energy demand in 2030, which corresponds to the requested percentage of national electricity consumption."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""A100-40GB bsz=28"" and ""the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.""","The figure and accompanying text for Mixtral on NVIDIA A100-40GB report a ground‑truth maximum batch size of 28 samples, confirming the answer."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a ﬁeld of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].""","The quote demonstrates that Sustainable AI was intended to encompass both climate‑positive applications and the improvement of AI’s environmental sustainability, not exclusively the former, making the statement false."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,li2025b;jegham2025;patterson2021,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2104.10350,"""estimated training energy of 1287 MWh"" (li2025b); ""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh)"" (jegham2025); ""its energy consumption is 1287 MWh"" (patterson2021).","All cited documents report the same estimated training energy for GPT‑3 as 1,287 MWh, which is expressed as 1287 MWh in the answer."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context explicitly states that Strubell et al.’s 2019 study quantified BERT’s training carbon footprint as 626,155 pounds of CO2e, providing the numeric value and unit needed for the answer."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning."" – Doc [xia2024]
""The MoE layer is the most time‑consuming, accounting for 85% of the overall execution time on average."" – Doc [xia2024]","Both excerpts state that the Mixture of Experts layer is the most expensive part of fine‑tuning and is specifically targeted for performance improvements, confirming the statement is true."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32g,0.32,g,luccioni2024,https://arxiv.org/pdf/2311.16863,"""bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries""","The quoted passage directly states that the BERT-based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 grams of CO2eq for every 1,000 text classification queries, which is the requested value."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.""","The passage explicitly states that direct release of environmental information peaked in 2022, indicating that year as the peak before the subsequent decline. "
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",TRUE,1,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""rity tasks like CV, even distributing VMs over four continents only slows down performance by 7%""; ""on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.""","The excerpts from the erben2023 document explicitly state that intercontinental training for CV models results in only a 7% slowdown relative to local training, confirming the statement is true."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not contain any statement or data indicating the number of hectares occupied by new AI data centers globally in 2022, so a confident answer cannot be derived from the documents."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",19,19,k grams,dodge2022,https://arxiv.org/pdf/2206.05229,"""7k grams vs 26k grams, for the most efficient vs least efficient regions""",The document reports emissions of 7 k grams in the most efficient region and 26 k grams in the least efficient region; the difference is approximately 19 k grams.
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any specific figure for the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, nor do they mention AlphaFold at all. Therefore the answer cannot be determined from the given documents."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context documents contain projections for 2028 but do not mention any projections for 2030, so the answer cannot be derived from the given information."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",26.1%,26.1,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"In the Amazon 2023 Sustainability Report, under the “People Managers” section, the table lists the percentages of women: “16.6% 31.9%26.1%23.5%”. The value corresponding to 2023 is 26.1%.","The table under the People Managers heading lists women’s percentages for each year; the 2023 entry is 26.1%, indicating that 26.1% of Amazon’s People Managers globally identified as women in 2023."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain explicit evidence that intra‑zone scaling with T4 GPUs achieved nearly linear per‑GPU speedup for CV models, so a confident answer cannot be given. "
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Model Size V100 32GB Count Max. Batch size 7B 1 64,"The table in the cited document lists the bare minimum GPU count for the 7B LLaMA model on V100 32GB GPUs as 1, indicating that a single V100 is sufficient for inference without compression or quantization."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,32.7,32.7,dollars,xia2024,https://arxiv.org/pdf/2408.04693,Table IV shows for A40 48GB the estimated cost for fine‑tuning Mixtral on GS with sparse MoE is 32.7.,"The table lists the total cost for the A40 GPU as 32.7, which directly answers the question about the estimated total cost for that setup."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.""","The quoted sentence from ebert2024 explicitly states that GPU-level monitoring is not recommended for reporting overall AI energy use, contradicting the claim that it is the preferred method."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2,2,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE- TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20,The table lists the maximum batch size for Mixtral fine‑tuning in dense mode on the CS dataset (which corresponds to Hellaswag) as 2 samples on an NVIDIA A40 GPU with 48 GB memory. Hence the answer is 2. 
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""training accounted for only half of the model’s overall emissions [121]""","The 2023 article by Luccioni et al. states that training made up half of BLOOM’s total emissions, which is 50%."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.,"The quoted passage states that adding compute resources to accelerate the MoE layers reduces cost, contradicting the claim that it can increase costs. Hence the statement is false."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts mention execution time breakdown graphs for various batch sizes, but they do not give an explicit numeric total execution time for a sparse Mixtral model fine‑tuned on an NVIDIA A40-48GB with batch size 10. No text or table lists this value, so the answer cannot be determined with confidence from the documents alone."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",8943 days,8943,days,morrison2025,https://arxiv.org/pdf/2503.05804,"""consumed 2.769 million liters of water , equivalent to about 24.5 years of water usage by a person in the United States""","The context states the water consumption of the OLMo series is equivalent to about 24.5 years for a single U.S. person. Multiplying 24.5 years by 365 days per year gives 8942.5 days, which we round to 8943 days."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,dodge2022,https://arxiv.org/pdf/2206.05229,"average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year)",The context from document dodge2022 explicitly states that one average US home’s yearly energy use emits 8.3 metric tons of CO₂. This figure directly answers the question.
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,GSM8k 14.5 17.3 16.9 27.8,"The GSM8k row in Table 3 lists four scores corresponding to four models; the last value 27.8 matches the JetMoE‑8B column, indicating its GSM8k benchmark score."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the total energy consumption of training the FLM-101B model. The only mentions of FLM-101B refer to FLOPs, training time, and a general carbon‑footprint comparison, but no explicit energy figure is given."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",84,84,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""Sparse(bsz=84)"" appears in the MoE layer execution breakdown figure for sparse Mixtral, which lists the batch sizes in increasing order, implying the largest batch size corresponds to the longest-running MoE layer.","The execution breakdown figure lists the sparse Mixtral batch sizes as Sparse(bsz=1), Sparse(bsz=30), and Sparse(bsz=84). The largest of these, bsz=84, therefore represents the longest-running MoE layer for a sparse Mixtral model on an NVIDIA A40 with 48 GB."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",8.5%,8.5,%,dodge2022,https://arxiv.org/pdf/2206.05229,"Table 13: P&R 8.5% for the 6B parameter transformer (""P&R 19.0% 8.5% 2.5% ..."" in the table for 6B).","The table shows the percentage emissions savings for each model under the Pause and Resume optimization. For the 6B parameter transformer, the value is 8.5%, which is the maximum potential saving reported for that model."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10 – 50,"[10, 50]",requests,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed."",","The passage states that a 500ml bottle of water corresponds to about 10–50 medium‑length user requests, which is the number of requests required to consume that volume of water."
