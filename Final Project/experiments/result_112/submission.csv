id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the Amazon Solar Farm Maryland-CPV Backbone or its CO2e avoidance, so I cannot answer the question based on the available context."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,luccioni2023,https://arxiv.org/pdf/2302.08476,"Doc [luccioni2023]: 'the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs).'","The question asks for the training duration with 10,000 V100 GPUs. Doc [luccioni2023] explicitly states that GPT‑3 training took 14.8 days under that configuration, so 14.8 days is the answer."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,percent,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: '...the total time cost for training FLM-101B is 21.54 days, which is 72 time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The passage states that using the growth strategy saved 72% of the training time for the 101B model compared to a from-scratch approach, so 72% is the answer."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the total permitted annual emission limits for nitrogen oxides from data center backup generators in northern Virginia for the specified period.
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4% increase,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'The total energy consumption of the US data centers increased by about 4 from 2010-2014, compared with the estimated 24 increase from 2005-10 and nearly 90 increase from 2000-05.'","The question asks for the average increase in U.S. data center electricity consumption between 2010 and 2014. Doc [wu2021b] states that consumption increased by about 4% in that period, so the answer is 4%."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention AWS using recycled water for cooling or the number of data centers involved, so the answer cannot be determined from the given context."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give a clear numeric average performance score for FLM-101B on the Open LLM Leaderboard.
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg,463,kg,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as , or 3700 kg of COeq per 8x server node, equal 463 kg per GPU.","The question asks for the estimated embodied carbon emissions per GPU. Doc [morrison2025] explicitly states that the assumed embodied emissions are 463 kg COeq per GPU, so that value is used as the answer."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention premature deaths from scope-2 pollutants in U.S. data centers for the year 2030, so the question cannot be answered with confidence based on the available context."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,cottier2024;li2025a,https://arxiv.org/pdf/2405.21015;https://arxiv.org/pdf/2309.03852,"Doc [cottier2024]: 'GPT-4 has the largest amortized hardware and energy cost, at 40M.'\nDoc [li2025a]: 'FLM-101B ... within a 100,000 budget.'","The amortized training cost of GPT-4 is 40 million dollars, and the total training budget for FLM‑101B is 100,000 dollars. Dividing 40,000,000 by 100,000 yields a factor of 400."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the launch year of the One Hundred Year Study on Artificial Intelligence, so I cannot answer this question based on the available context."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'This is a more than 6,750 fold improvement in processor clock speed'","The question asks for the fold increase in clock speed between the Intel 4004 (740 kHz) and a typical 2021 microprocessor (5,000,000 kHz). Doc [wu2021b] explicitly states that this improvement is more than 6,750 times, so the answer is 6,750 fold."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"Doc [dodge2022#0052]: 'This model was not trained to completion, but only until 13; a full training run would take 60 days.'","The question asks for the estimated duration of a full training run of the 6.1 billion parameter model according to Dodge et al. (2022). The cited passage explicitly states that a full training run would take 60 days, so 60 days is the correct answer."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific CO2e value for a single passenger round trip from San Francisco to New York, so the answer cannot be determined from the context."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,3426,Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)'","The 2025 study lists GPU energy consumption per 1,000 queries, and the highest value reported is 3,426 Wh for Command‑R Plus, so that model has the highest GPU energy consumption among those listed."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain enough information to determine the estimated GPU Power Usage in kWh for the specified SGLang benchmarking scenario.
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention a change in GPU utilization to 80% or the resulting factor by which the overall carbon footprint would decrease.
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain explicit information about the execution time in seconds for a dense BlackMamba model with a batch size of 30 fine-tuned on an NVIDIA A40-48 GB GPU.
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,is_blank,is_blank,is_blank,Doc [xia2024#0001]: 'Both models incorporate eight experts in their MoE layers.',"The question asks for the number of experts per MoE layer in the JetMoE-8B architecture. Doc [xia2024#0001] explicitly states that the models (including the 2.8B variant) incorporate eight experts in each MoE layer, so the answer is 8 experts."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the percentage of parameters activated in the Switch Transformer per token.
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'The UN’s Global E-Waste Monitor 2024 showed that about 22 of e-waste has been shown to be formally collected and recycled, with global ge'","The question asks for the percentage of e‑waste formally collected and recycled per the UN report. Doc [luccioni2025a] explicitly states that about 22% of e‑waste has been formally collected and recycled, so 22% is the answer."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a specific energy consumption value in MWh for the pre‑training of the BLOOM model.
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The question asks for the total wall‑clock time of training FLM‑101B using a growth strategy. Doc [li2025a] explicitly states that the total time cost is 21.54 days, so that is the answer."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents mention the term Red AI but do not provide any statement about its decline, so the claim cannot be verified from the provided context."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",6.6 billion cubic meters,6.6,billion cubic meters,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'AI-related withdrawals could reach 6.6 trillion liters annually by 2027.',"The question asks for the projected global AI water withdrawal in 2027 in cubic meters. Doc [jegham2025] states 6.6 trillion liters, and converting liters to cubic meters (1 m³ = 1,000 L) gives 6.6 trillion / 1,000 = 6.6 billion cubic meters."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity","The question asks for the estimated electricity consumption of GPT-3 training in MWh. Doc [jegham2025] explicitly states that the training consumed 1,287 MWh, so that value is used as the answer."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision.'","The question asks how many models were sampled and analyzed in the Power Hungry Processing study. Doc [luccioni2024] explicitly states that 88 models were studied, and Doc [luccioni2024#0003] also reports sampling 88 models, confirming the number."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x,2.2,x,is_blank,is_blank,"Doc [cottier2024#0093]: 'the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.'","The question asks for how many times larger the total compute for model development is compared to the final training run. Doc [cottier2024#0093] states that the median ratio is 2.2x, which directly answers the question."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied passages include information about Facebook’s GPU utilization rates or whether a majority of its model experimentation workflows run at over 80% capacity, so the question cannot be answered confidently from the given context."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a numeric value for the cost of the English portion of FLM-101B, so the answer cannot be determined from the given context."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'These methodologies were then adapted into the AI Energy Score , a project aiming to establish a unified approach for comparing the inference efficiency of AI models.'","The question asks for the name of the collaborative project that standardizes inference efficiency comparison. Doc [luccioni2025c] explicitly states that this project is the AI Energy Score, so that is the answer."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a numeric value for the longest kernel execution time in microseconds for the specified configuration.
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons Paradox,Jevons Paradox,is_blank,is_blank,is_blank,"Doc [jegham2025#0108]: 'aligned with the Jevons Paradox, where increased efficiency drives systemic demand.' Doc [jegham2025#0114]: 'Jevons Paradox: As AI becomes cheaper and faster, total usage expands, intensifying environmental strain despite gains in per-query efficiency.'","The question seeks the economic principle that explains why technical efficiency gains may not yield net environmental benefits. Both Doc [jegham2025#0108] and Doc [jegham2025#0114] explicitly reference Jevons Paradox as the underlying principle, so the answer is Jevons Paradox."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not specify how many AWS data center regions had 100% renewable energy matching for 2023, so I cannot answer this question with confidence."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'In addition, the global carbon emissions for 2020 dropped by 6.4 with vehicle transportation in the US accounting for a portion of the global reduction.'","The question asks for the reported drop in global carbon emissions in 2020. Doc [wu2021b] states that emissions dropped by 6.4, which is interpreted as a percentage decrease, so the answer is 6.4%."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,NVIDIA A40 GPU,NVIDIA A40 GPU,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 'Specifically, we conduct our experiments using NVIDIA A40 GPU with Ampere architecture.'","The question asks for the hardware processor used in the experimental setup for energy‑efficient local inference in financial sentiment classification. Doc [xia2024] explicitly reports that the experiments were performed on an NVIDIA A40 GPU, which is the relevant processor."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'The amount of compute used to train deep learning models has increased 300,000x in 6 years.'","The question claims a 200,000x increase, but Doc [schwartz2019] reports a 300,000x increase from 2012 to 2018, so the statement is not correct."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.6 MWh,103.593,MWh,is_blank,is_blank,"Doc [dodge2022#0049]: 'We note our training run of the 6 billion parameter transformer only trained for approximately 13 of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.'  Doc [dodge2022#0052]: 'The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model.'","The question asks for the estimated total energy consumption for a full training run of a 6.1‑billion‑parameter transformer. Doc [dodge2022#0049] provides a direct estimate of 103,593 kWh for a full run, which equals 103.593 MWh. Doc [dodge2022#0052] gives the partial‑run consumption of 13.8 MWh over 8 days and notes a full run would take 60 days, implying the same full‑run estimate. Thus the answer is 103.6 MWh."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not contain any information about the percentage of AI inference workloads in Asia powered by coal in 2023.
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,is_blank,is_blank,"Doc [xia2024#0001]: 'Both models incorporate eight experts in their MoE layers. For dense fine‑tuning, all experts are activated, whereas for sparse fine‑tuning, only the top two experts are selected for each token.'","The question asks how many experts are activated per token. Doc [xia2024#0001] explicitly states that only the top two experts are selected for each token in the MoE layers, so the answer is two experts."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention JetMoE-8B or the number of parameters activated per input token, so I cannot answer the question based on the available context."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'the cumulative emissions from approximately 272 transatlantic flights between Boston and London.',"The question asks for the number of transatlantic flights comparable to GPT‑4o’s annual emissions. Doc [jegham2025] explicitly states that the annual emissions are equivalent to about 272 transatlantic flights, so that figure is used as the answer."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'The average data center PUE in 2023 was 1.58 globally and 1.6 in the EU.',"The question asks for the global average PUE in 2023; Doc [ebert2024] explicitly states that the average data center PUE globally was 1.58 in 2023, so that value is used as the answer."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain enough information to determine Amazon's energy storage capacity as of 2023.
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,15 years,15,years,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'e period starting in 2010 as this is the beginning of the modern deep learning era (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed, including all 754 models from 2010 to the first quarter of 2025.'","The question asks for the approximate age of the AI field in 2025. Doc [luccioni2025c] states the modern deep learning era began in 2010 and extends to 2025, indicating about 15 years. Thus the age is roughly 15 years."
q056,When was the field of Artificial Intelligence officially christened?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the official christening date of the field of Artificial Intelligence.
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Facebook's second representative recommendation model (RM2) or its quantization size reduction, so I cannot answer this question based on the available context."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'Approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity.'","The statement in the question matches the exact wording in Doc [wu2021b], confirming that approximately 770 million people lack stable electricity access, so the answer is True."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain explicit information about the number of Amazon electric delivery vans added in 2022 and 2023.
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS),Neural Architecture Search,is_blank,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: 'quantified the emissions produced by training a Transformer model with Neural Architecture Search, finding it to be comparable to the lifetime carbon emissions of five cars.'","The question asks for the specific AI process underpinning the five cars estimate. The cited passage from Doc [dodge2022] explicitly states that the estimate is based on training a Transformer model with Neural Architecture Search (NAS), making NAS the correct answer."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents supply a numeric value for the average water use effectiveness (WUE) of Google's AI-dedicated data centers in 2024, so the answer cannot be determined from the available context."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.'","The question asserts that operational impacts exclude GHG emissions from servers and cooling, but Doc [morrison2025] explicitly states that these emissions are included in operational impacts, so the statement is false."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 J per token,"[3, 4]",J,is_blank,is_blank,"Doc [samsi2024#0031]: 'For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.'","The question asks for the energy per token at a maximum generation length of 512 tokens. Doc [samsi2024#0031] explicitly states that at length 512 the energy per output token is about 3–4 Joules, so that is the reported value."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific numeric value for the GHG emissions of pre-training the Llama 7B model, so the answer cannot be determined with confidence from the context."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a clear, explicit statement of the energy consumption in Wh for the o3 model on a long prompt, so I cannot answer confidently."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","25,000 USD",25000,USD,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of 25,000.'","The question asks for the estimated cost of training AI2's Grover on 256 TPU chips for two weeks. Doc [schwartz2019] explicitly states that this training was estimated to cost 25,000, which we interpret as USD, providing the required answer."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 'optimizer stage in fine-tuning takes a considerable portion of the running time (up to 53 when conducting sparse fine-tuning with batch size = 1)',","The passage states that the optimizer stage accounts for up to 53% of the running time during sparse fine‑tuning with batch size = 1 on an NVIDIA A40‑48GB GPU, which directly answers the question."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the number of members in the inaugural 2015 Study Panel of the One Hundred Year Study on AI.
q067,What was the average global data center PUE in 2023?,1.58 PUE,1.58,PUE,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: The average data center PUE in 2023 was 1.58 globally and 1.6 in the EU.,"The question asks for the average global data center PUE in 2023. Doc [ebert2024] explicitly states that the global average PUE in 2023 was 1.58, which provides the required value."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,20 samples,20,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse. -D -S -D -S 2 8 6 20 1 3 2 8',"The table in Doc [xia2024] lists the maximum batch sizes for BlackMamba models. For the 2.8B variant using sparse fine‑tuning, the value shown is 20 samples, which answers the question for the GSM8K dataset on an NVIDIA A40 GPU with 48 GB memory."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about wind turbines contracted by Microsoft in 2023.
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'The reasoning behind the 5-10 reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their' . Doc [luccioni2025c#0047]: 'Applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding in fact, many of the emissions reductions on a global scale require individual, societal and political shifts.'","The question asks whether the 5‑10% global GHG reduction claim is supported by clear, publicly available calculations and sound scientific grounding. Doc [luccioni2025c] states that the reasoning is unclear and calculations are not detailed, while Doc [luccioni2025c#0047] explicitly notes a lack of scientific grounding. Therefore, the claim is not supported as described."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'Gemini Ultra has the highest fraction of RD staff cost at 49',"The question asks for the percentage of Gemini Ultra’s development cost attributed to R&D staff (including equity). The passage from Cottier et al. states that Gemini Ultra’s RD staff cost fraction is 49%, so that is the required answer."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,is_blank,is_blank,Doc [luccioni2024#0063]: '0.083',"The doc states Flan‑T5‑xxl consumes 0.083 kWh per 1,000 queries. For 1 billion queries (1,000,000 × 1,000) the energy is 0.083 × 1 000 000 kWh = 83,000 kWh, which equals 83 MWh per day."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the Earth‑Sun distance, so I cannot answer this question based on the available context."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the Study Panel from the 100 Year Study on AI or their stance on AI as an imminent threat to humankind, so the answer cannot be determined from the context."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a clear statement that sparsely activated DNNs consume less than 1/10th the energy of large dense DNNs while maintaining accuracy.
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a specific percentage of a client device's total carbon footprint that is attributable to manufacturing, so the answer cannot be determined from the context."
q080,True or False: The AlphaGo program defeated the human Go champion.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention whether AlphaGo defeated a human Go champion, so the answer cannot be determined from the given context."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",45%,45,%,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'Table shows significant  reductions in carbon emissions across all models, with some achieving up to 45 after optimization.'","The question asks for the percentage decrease in energy use for Llama 3.1 8B when targeting a 100 ms TPOT versus minimizing latency.  Doc [jegham2025] states that energy (carbon) reductions can be as high as 45% for some models, which applies to Llama 3.1 8B, so the answer is 45%."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e, which is over 40× the five cars estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Meta reports that their Llama 3 family of models emitted 11,390 tons COe or over 40x the five cars estimate.'","The question asks for the pre‑training emissions of Meta's Llama 3 family and how that compares to the five‑cars estimate. Doc [luccioni2025c] explicitly states the emissions as 11,390 tCO2e and that this value is over 40 times the five‑cars estimate, so the answer reflects both the number and the comparison."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,dynamic batching,dynamic batching,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization while maintaining low latency.',"The question asks for the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones. Doc [jegham2025] identifies this strategy as dynamic batching, which is designed to keep GPUs busy by filling batches as requests complete."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,schwartz2019;luccioni2024,https://arxiv.org/pdf/1907.10597;https://arxiv.org/pdf/2311.16863,"Doc [schwartz2019]: 'The number of model parameters does not tell the whole story: AlexNet (first point in the graph) actually has more parameters than ResNet (second point), but dramatically less ...' Doc [luccioni2024]: 'we observe a relationship between model size and quantity of emissions produced during inference, with differing progressions for each modality however, the task structure accounts for more of the variation than the model size does.'","The documents show that while model size correlates with inference energy, it does not always determine it—task structure and architecture also influence energy consumption—so the statement that more parameters always consume more energy is false."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain information about classification experiments on German public administration texts using sentence embeddings or their accuracies, so the answer cannot be determined from the context."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not contain any explicit statement about the CO2 emissions from OpenAI's API requests in January 2024, so I cannot provide a confident answer."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on'","The question asks for the range of GPU energy usage for 1,000 inference queries in the 2025 study appendix. Doc [luccioni2025c] explicitly states that the range is from 0.06 Wh to over 3,426 Wh, so that is the answer."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the factor increase in AI training infrastructure capacity at Facebook over the specified period, so I cannot answer this question based on the available context."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a clear name for the inference system described in the 2025 Chen et al. paper.
q093,How many parameters does the largest T5 model have?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents specify the parameter count for the largest T5 model, so the answer cannot be determined from the available context."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific value for the gross carbon intensity of energy according to the U.S. average mix in 2021.
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the H100 GPU hours required for the JetMoE-8B alignment process, so I cannot answer this question based on the available context."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams of CO2eq",1594,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: to the image captioning models with a similar number of parameters.  For context, the most carbon-intensive image generation model () generates 1,594 grams of  for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle, whereas the least carbon-intensive text generation model () generates as much carbon as 0.0006 miles driven by a similar vehicle, i.e. 6,833 times less.","The question asks for the emissions per 1,000 inferences of the most carbon‑intensive model, stable‑diffusion‑xl‑base‑1.0.  The 2024 study states that this model emits 1,594 grams of CO2eq per 1,000 inferences, so that is the answer."
q094,What is the total number of parameters in the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE-8B model or its parameter count, so I cannot answer the question with confidence."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist),"The question asks if researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. The cited passage from Doc [luccioni2025b] explicitly states that researchers do not believe such a universal approach can exist, so the correct answer is False."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34 increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20 uptick in the same period.'","The question asks for the percentage increase in Google's data center water consumption from 2021 to 2022. Doc [luccioni2025a] states that Google experienced a 20% uptick during that period, so the answer is 20%."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems.'","The question asks for the term proposed for expanding transparency to include socio-technical and environmental aspects. Doc [luccioni2025b] explicitly introduces ""social transparency"" as that expanded concept."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the amount of water returned to communities from Amazon's replenishment projects in 2023, so I cannot answer this question based on the available context."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a clear statement of the fraction of local throughput achieved for NLP when training was spread across four continents instead of remaining local.
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,40M,40,million USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: we find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at 40M and Google's,"The question asks for the estimated amortized training cost of GPT‑4. Doc [cottier2024] explicitly states that GPT‑4’s amortized hardware and energy cost is 40M, which we interpret as 40 million USD."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems'",The question asks for the acronym of the Finnish project; the context explicitly names the Finnish ETAIROS project as the one proposing this integration. Hence the acronym is ETAIROS.
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain enough information to determine the decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents.
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Doc [rubei2025]: 'Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts.',"The question asks whether custom tags can reduce energy consumption in source code completion tasks. Doc [rubei2025] explicitly states that using specific tags reduces LLM energy consumption during inference, and the context involves the CodeXGLUE benchmark for source code completion, supporting a true answer."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,Doc [wu2021b]: 'the PUE of Facebook datacenters is 1.10 (2020)',"The question asks for Facebook’s data center PUE. Doc [wu2021b] explicitly states the value as 1.10 (for 2020), so that is the answer."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: components of amortized hardware CapEx + energy in , we find that on average, 44 goes toward AI accelerator chips. The rest of the server (including markup) makes up 29 of the cost, while cluster level interconnect makes up 17.","The question asks for the average share of amortized hardware and energy cost attributed to AI accelerator chips. Doc [cottier2024] states that, on average, 44% of the cost is attributed to AI accelerator chips, which directly answers the question."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",51.3%,51.3,%,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: ciency. The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3.,"The context states that in all training stages, including the final growth stage, the FLOPs utilization rate is at least 51.3%, so 51.3% is the reported percentage."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"Doc [luccioni2023]: C = P T I = E I.
Doc [luccioni2023]: The unit of measurement typically used for quantifying and comparing carbon emissions is . This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of  emitted per kilowatt hour of electricity generated ()","The question asks for the emissions metric defined as CO₂ emissions per unit of electricity consumed. In Doc [luccioni2023], the metric is referred to as ""Carbon Intensity"", with the formula C = P T I = E I and the unit described as grams of CO₂ emitted per kWh. Therefore, the answer is ""Carbon Intensity""."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the EPA’s primary PM2.5 standard, so the answer cannot be determined from the given context."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '115 books would produce the same amount of CO as a single Amazon Kindle device.',"The question asks how many physical print books equal the CO2 of one Kindle. Doc [luccioni2025a] states that 115 books produce the same amount of CO, so 115 is the answer."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3700000,GPUs,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: hardware used for training and deploying AI models, NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand","The question asks for the number of data center GPUs shipped by NVIDIA in 2024. Doc [luccioni2025a] explicitly states that NVIDIA shipped 3.7 million GPUs that year, which directly answers the question."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain information about the parameter count of the model analyzed in the 2022 paper by Dodge et al., so I cannot answer the question based on the available context."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,30 million USD,30,million USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: The most expensive publicly-announced training runs to date are OpenAI's GPT-4 at 40M and Google's Gemini Ultra at 30M.,"The question asks for the estimated amortized training cost of Gemini Ultra. Doc [cottier2024] explicitly states that Gemini Ultra’s cost is 30M, which we interpret as 30 million USD, hence the answer."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",approximately 200 times,200,is_blank,is_blank,is_blank,Doc [han2024#0002]: 'low-income counties that could experience approximately 200x per-household health costs than others.',"The question asks for the factor by which per-household health burden could exceed that in less-impacted communities. Both Doc [han2024#0002] and Doc [han2024#0019] state that the ratio is about 200, so the answer is 200 times."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give a clear numeric factor comparing full‑stack optimization to a CPU server baseline for the operational carbon footprint of a Transformer‑based universal translation model.
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,jegham2025;luccioni2025a,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2501.16548,"Doc [jegham2025]: 'aligned with the Jevons Paradox, where increased efficiency drives systemic demand.'","The question asks for a phenomenon where improved efficiency leads to higher overall consumption. The context cites Jevons Paradox in both Doc [jegham2025] and Doc [luccioni2025a], confirming that this is the correct answer."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain explicit numerical values for the total energy consumption of a full GPT‑3 training run or of a Meena training run, so the question cannot be answered with confidence."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'the energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process',' adding another 15  to the initial consumption.","The question asks for the combined training and fine‑tuning energy costs for the BLOOMz-7B model. Doc [ebert2024] provides the training cost (51,686 kWh) and fine‑tuning cost (7,571 kWh). Summing these gives 59,257 kWh, which is the answer reported in the Power Hungry Processing study."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the DS Llama 70B model or the FKTG dataset, so the energy consumption cannot be determined from the available context."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents specify a West Virginia county or its projected per-household health cost for 2030, so I cannot answer the question with confidence."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors.'","The passage states that the Act’s risk assessment and mitigation obligations do not sufficiently emphasize environmental factors, indicating that environmental risks are not explicitly required to be included in the assessments for GPAI models with systemic risk."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101B,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'We produce three models with 16B, 51B, and 101B (FLM-101B) parameters'","The question asks for the total number of parameters in the final FLM-101B model. Doc [li2025a] explicitly states that the FLM-101B has 101B parameters, which is the value used in the answer."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.75,0.75,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: Mistral-small  0.70  0.67  0.65  0.67  0.020  Mistral-small  0.73  0.70  0.69  0.70  0.015,"The table in Doc [khan2025] lists Mistral‑small emissions before optimization as 0.020 and after optimization as 0.015. Dividing the after value by the before value gives 0.015 ÷ 0.020 = 0.75, so the emissions were multiplied by 0.75."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.9 kWh,2.9,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: ... image generation, whose mean consumption is 2.9kWh.","The question asks for the average energy per 1,000 image generation inferences. Doc [luccioni2024] explicitly states that image generation’s mean consumption is 2.9 kWh for 1,000 inferences, so that is the required value."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: Given a SLO requirement of 100 TPS,  selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of 2.13. On the other hand, both Max-Performance and  without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, bu","The passage shows InferSave selected g4dn.xlarge at a cost of 2.13, while Max-Performance selected g6e.xlarge at 2.699.  ((2.699-2.13)/2.13)*100 ≈ 26.7%, so Max-Performance was about 26.7% more expensive."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the documents contain a percentage of recycled rare earth metals used in NVIDIA H100 GPU manufacturing in 2024, so the answer cannot be determined from the provided context."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of .'","The question asks for the total energy consumed for all model experimentation and evaluation in the 2024 study. Doc [luccioni2024] explicitly states that 754.66 kWh of energy were used, which directly answers the question."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention a dataset name for German nuclear waste site objection texts classified in the experiments, so the answer cannot be determined from the available context."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'In terms of token usage, 84 of LLM usage is through models with no disclosure, 14 for indirec'","The question asks for the percentage of token usage through non‑disclosed models. Doc [luccioni2025c] states that 84 % of LLM usage is through models with no disclosure, so 84% is the answer."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the full training energy for a 6B transformer (≈103,593 kWh) but do not give a specific inference energy cost for the BLOOMz‑7B model, so the number of inferences required to match that training cost cannot be determined from the available context."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention pruning, quantizing, or the total carbon emissions avoided by these techniques in 2023, so the answer cannot be determined from the context."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provided do not contain a comparison between 3.2 tCO2e and the number of passengers on a round trip between San Francisco and New York, so the question cannot be answered from the context."
q120,How many pounds of CO2e are estimated for an average American life in one year?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a specific estimate of pounds of CO2e for an average American life in one year.
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,is_blank,is_blank,Doc [griggs2024#0021]: 'A10G results in a 24 cost saving over A100-only and 31 over A10G-only.'  Doc [griggs2024#0020]: 'highlight a case where using 2 A100s and 1 A10G results in a 24 cost saving over A100-only and 31 over A10G.',"Both documents state that using a mix of A10G and A100 GPUs achieves a 24% cost saving compared to an A100-only strategy, so the percentage cost savings is 24%."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: 'most carbon footprint analyses gather the information manually by writing to authors.',"The question asserts that most analyses gather data automatically. Doc [luccioni2025b] states that most gather information manually by contacting authors, which directly contradicts the claim, so the correct answer is False."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain the requested price per hour for an NVIDIA H20 according to Chen et al. (2025).
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE project, its training budget, or total GPU hours, so I cannot estimate the cost per H100 GPU-hour from the available information."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,"1,354 liters",1354,liters,is_blank,is_blank,"Doc [luccioni2025c#0073]: '1,354 ... Llama 3 70B Meta'","The table in Doc [luccioni2025c#0073] lists the freshwater consumption for Meta’s Llama 3 70B inference serving clusters as 1,354 liters, directly answering the question."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,is_blank,is_blank,Doc [han2024#0015]: 'This is equivalent to approximately 44 of the data centers' total electricity cost.',"The question asks for the public health cost as a percentage of total electricity cost. Doc [han2024#0015] states that the 2023 health cost is approximately 44% of the data centers' total electricity cost, so 44% is the answer."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,is_blank,is_blank,"Doc [dodge2022#0065]: 'If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of ...'","The question asks for the estimated range of CO2 emissions for a complete training run of a 6.1 billion parameter transformer model. Doc [dodge2022#0065] explicitly states that a full training run would emit 21 to 78 metric tons of CO2e, providing the required range."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",59257 inferences,59257,inferences,ebert2024;luccioni2024,https://arxiv.org/pdf/2410.06681;https://arxiv.org/pdf/2311.16863,"Doc ebert2024: 'fine‑tuning the Bloomz‑7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15 to the initial consumption.'  Doc luccioni2024: 'the amount of energy required per inference varies from 5.4 × for the smallest model, BLOOMz‑560M to 1.0 × kWh for the biggest one, BLOOMz‑7B.'","The total initial energy cost of training and fine‑tuning is 51,686 kWh + 7,571 kWh = 59,257 kWh.  Each inference of BLOOMz‑7B consumes 1.0 kWh, so 59,257 kWh ÷ 1.0 kWh = 59,257 inferences are required for cumulative deployment energy to equal the initial training energy."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 A100 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: Model Size Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8,"The table in Doc [samsi2024] lists the bare minimum GPU count for each model size. For LLaMA‑13B, the A100 column shows a count of 1, indicating that one NVIDIA A100‑80GB GPU is sufficient for inference without compression or quantization."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the percentage of Amazon's workforce that identified as men in 2023.
q149,How many tokens were used to pre-train the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about the JetMoE-8B model or its pre-training token count.
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Doc [khan2025]: 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45 post quantization, making them particularly suitable for resource-constrained environments.'","The question asks whether sustainable deployment techniques (quantization) achieved up to a 45% reduction in carbon emissions. Doc [khan2025] explicitly states that the methods reduce emissions by up to 45 after quantization, confirming the statement as true."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about Apple's water footprint or the share attributable to its supply chain, so I cannot answer the question based on the available context."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the number of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024, so the answer cannot be determined from the given context."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the total execution time for a sparse BlackMamba model fine-tuned on a NVIDIA A40-48GB with batch size 84, so the answer cannot be determined from the given context."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the Standing Committee of the One Hundred Year Study or the frequency of forming a Study Panel.
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,answers,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: ... reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide ...","The question asks how many answers researchers collected after contacting more than 500 authors. Doc [luccioni2025b] explicitly states that they were only able to collect 95 answers, so the answer is 95."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention IBM Watson’s performance in the Jeopardy challenge, so the question cannot be answered with confidence."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times more,6.4,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: ... could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year,"The document reports that the deal could add up to 640 percent more emissions, which is equivalent to 6.4 times the yearly carbon removal target."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA A100 80GB GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 7B   1  64  1  64,"The table in Doc [samsi2024] lists the bare minimum GPU count for each LLaMA model; for the 7B model it shows 1 NVIDIA A100 80GB GPU, indicating that one GPU is sufficient for inference without compression or quantization."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '...with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water.',"The question asks how many GPT‑3 queries consume about half a liter of water. Doc [luccioni2025a] explicitly states that 10–50 queries on GPT‑3 consume around half a liter, so the correct answer is 10–50 queries."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not mention the JetMoE-8B-Chat model or its MT‑Bench score, so the question cannot be answered with confidence from the given context."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8–3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)'","The question asks for the energy range for pre‑training LLMs. Doc [luccioni2025c] explicitly states the range as 0.8 MWh to 3,500 MWh, which is used as the answer."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,CTCF (Communication-to-Compute Time Factor),CTCF,is_blank,is_blank,is_blank,Doc [kim2025#0012]: 'CTCF-adjusted values closely matched the actual measurements.',"The question asks for the metric used to evaluate the ratio of computation to communication time in distributed training across continents. Doc [kim2025#0012] introduces the CTCF-adjusted values, indicating that the Communication‑to‑Compute Time Factor (CTCF) is the metric used."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not contain a definition or term for freshwater taken from ground or surface sources, so I cannot answer this question from the provided context."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 connected devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.'","The question asks for the 2021 average number of connected devices per U.S. household. Doc [wu2021b] explicitly states that the average household has 25 connected devices, so 25 is the answer."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Griggs et al. or the Mélange system, so there is no information to determine the reported cost reduction percentage."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10–50 completions,"[10, 50]",completions,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '10–50 queries on GPT-3 consumes around half a liter of water.',"The question asks how many medium-length completions can be produced with the water needed to fill a 500 mL bottle (0.5 L). Doc [luccioni2025a] states that 0.5 L of water supports 10–50 GPT‑3 queries, which correspond to medium-length requests. Therefore, 0.5 L allows between 10 and 50 completions."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a percentage of ML workload that NVIDIA estimated to be inference processing in 2019.
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the throughput of a dense Mixtral-CS-A100-40GB at batch size 1.
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not give a percentage for the health cost relative to the electricity cost in Altoona, Iowa."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,luccioni2023;ebert2024,https://arxiv.org/pdf/2302.08476;https://arxiv.org/pdf/2410.06681,"Doc [luccioni2023]: 'the TDP-based approach is often used in practice when estimating the carbon emissions of AI model training and it remains a fair approximation of the actual energy consumption of many hardware models.'; Doc [ebert2024]: 'the upper bound is typically higher than the actual consumption, as GPU utilization rarely reaches its theoretical peak.'","The documents state that using TDP only provides a fair approximation or an upper bound, not a reliable and accurate measurement of actual GPU energy consumption."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg CO2e,178.97,kg CO2e,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of .'","The question asks for the total CO2 equivalent emissions of the 2024 study. Doc [luccioni2024] states that the study emitted 178.97 kg of CO2 equivalent, so that value is reported as the answer."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'peaked in 2022, with 10 of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, ... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.'","The 2025 paper notes that after the 2022 peak, a reversal occurred that dramatically reduced direct environmental disclosures, indicating the trend did not continue to increase."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents include a monthly on‑demand rental cost for serving Llama‑2‑70B on 2 NVIDIA A100 GPUs.
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4 A100 GPUs,4,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'the 65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).',"The question asks for the bare minimum number of 80GB A100 GPUs needed for LLaMA‑65B inference. Doc [samsi2024] explicitly states that the model was run on 4 A100 GPUs, indicating that 4 is the minimum required."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,FALSE,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'GPT-4o consumes around 2.875Wh while GPT-4omini's consumption is slightly higher at 3.098Wh due to deployment...',"The quoted passage shows GPT-4o mini consumes 3.098Wh per query, which is higher than GPT-4o's 2.875Wh, so the claim that GPT-4o mini consumes less energy per query is false."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.' Doc [han2024#0029]: 'training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.' Doc [han2024#0002]: 'training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-' Doc [han2024#0003]: 'equivalent to driving a passenger car for more than 10,000 LA-NYC round trips.'","All cited documents state that training an Llama‑3.1 scale model emits air pollutants equivalent to over 10,000 Los Angeles‑New York City car round trips, so the answer is more than 10,000 round trips."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide enough information to calculate the total computational work for the final 101B training stage in zettaFLOPs.
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a clear statement linking a BLEU score increase from 5 to 40 with a specific model size multiplier for GPT-3-based translation tasks, so the answer cannot be determined from the available context."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",4.5e22 FLOPs,4.5e+22,FLOPs,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'smaller versions of GPT-3 used 4.5e22 FLOP (based on ).',"The question asks for the total number of floating point operations to train GPT‑3. Doc [cottier2024] explicitly states that smaller versions of GPT‑3 used 4.5e22 FLOPs, which is the figure reported by OpenAI."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'AI training costs will have a major impact on the future of AI development. Our findings suggest that if the current trend of 2.4x per year growth continues, then the amortized cost of frontier training runs will exceed one billion dollars by 2027.'","The question asks for the cost threshold that the largest training runs will exceed by 2027. Doc [cottier2024] explicitly states that the cost will exceed one billion dollars by that year, so the answer is 1 billion dollars."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,"more than 700,000 liters",700000,liters,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'more than 700 kiloliters (kL) of water for cooling alone , enough to fill a quarter of an Olympic-sized swimming pool.'","The question asks for the cooling water used during GPT‑4 training. Doc [jegham2025] states that cooling used more than 700 kiloliters, which equals more than 700,000 liters, so that is the value provided."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,56.4 %,56.4,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Doc [schwartz2019]: 'AlexNet 61.1 0.7 56.4 2012',"The table in Doc [schwartz2019] lists AlexNet’s top-1 accuracy on ImageNet as 56.4 %, matching the year 2012. Thus the requested accuracy is 56.4 %."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","606,096,000 MWh",606096000,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: en-source community: at the time of writing this article (November 2023), BLOOMz-7B has been downloaded 606,096 times.
Doc [luccioni2024]: e energy consumption per inference, in Table. We can see that the amount of energy required per inference varies from 5.4 ×  for the smallest model, BLOOMz-560M to 1.0 ×  kWh for the biggest one, BLOOMz-7B.","The question asks for total energy if each of the 606,096 downloads triggers 1,000,000 inferences. The document states each inference consumes 1.0 kWh. Multiplying 606,096 downloads × 1,000,000 inferences/download × 1.0 kWh/inference gives 606,096,000,000 kWh, which is 606,096,000 MWh."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 A800 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8×80G) servers.'  Doc [li2025a]: 'We fix our budget to be 100K with 192 A800 GPUs.',"The document states FLM-101B uses 24 servers, each with 8 A800 GPUs, giving 24×8=192 GPUs, which is also explicitly mentioned as 192 A800 GPUs."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'FAIR's RoBERTa  was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.'","The question asks for the GPU hours used to train FAIR's RoBERTa. Doc [schwartz2019] explicitly states that the training required about 25,000 GPU hours, which directly answers the query."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 GPUs,8,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 7B   1  64  1  64 
     13B  2  64  1  64 
     65B  8","The table in Doc [samsi2024] lists the bare minimum hardware for each LLaMA variant. For the 65B model it specifies 8 V100 32GB GPUs, indicating that at least 8 GPUs are required to run inference without compression or quantization."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about gallons of water consumed per ChatGPT user session in 2023, so I cannot answer this question based on the available context."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale library,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'This implementation of the model uses Pytorch and the FairScale library to enable model sharding across multiple GPUs and nodes.',"The question asks for the framework used to deploy LLMs across GPUs and nodes, and Doc [samsi2024] explicitly states that the FairScale library is used for model sharding across multiple GPUs and nodes."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'Microsoft reporting a 34 increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20 uptick in the same period.'","The question asks for Microsoft's reported percentage increase in global water consumption between 2021 and 2022. Doc [luccioni2025a] explicitly states a 34% increase, so that is the answer."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","≈730,000 miles",730000,miles,luccioni2023;luccioni2025c;luccioni2024,https://arxiv.org/pdf/2302.08476;https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2311.16863,"Doc [luccioni2023]: 'estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of ...'; Doc [luccioni2025c]: 'could yield 626,155 pounds (284 metric tons) CO-equivalent GHG emissions'; Doc [luccioni2024]: 'the most carbon-intensive image generation model generates 1,594 grams of CO2 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle'","The emissions figure is 626,155 lbs (284,019 kg) from the Transformer model with NAS (Doc [luccioni2023] and Doc [luccioni2025c]). The emissions‑to‑driving‑distance ratio is 4.1 miles per 1,594 grams (Doc [luccioni2024]). Converting 284,019,000 grams by this ratio gives about 730,000 miles."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give a specific number of GPU hours consumed during the pre‑training of the JetMoE‑8B model.
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite.
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,U.S. homes,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025#0002]: 'Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people...' ","The question asks for the number of U.S. homes whose annual electricity use matches that of 700 million daily GPT‑4o queries. Doc [jegham2025#0002] explicitly states that this scenario equates to 35,000 U.S. homes, providing the answer."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the documents in the context mention Yelp sentiment analysis benchmarks or compare traditional models’ accuracy to that of large language models, so the statement cannot be verified from the provided material."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.10 PUE,1.1,PUE,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) …'","The question asks for the PUE of Google's Iowa datacenter during the run of the Evolved Transformer. The only PUE value provided for Google datacenters is 1.10 (2021) in Doc [wu2021b], which is the most specific figure available in the given context."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emissions for NAS training (≈626,155 lbs or 284 metric tons) and its equivalence to five car lifetimes, but they do not give a direct conversion to average American lifetimes, so I cannot answer that part confidently."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion GPT-4o queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'PT’s weekly active user base from 300 million to 800 million between December 2024 and April 2025 . This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15 of the annual number of Google searches in 2024 .'","The question asks for the 2025 total GPT‑4o query estimate. Doc [jegham2025] states that the estimate is about 772 billion queries, so that figure is used as the answer."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a numeric value for the CO2e avoided by Amazon's on-site solar energy systems compared to nonrenewable electricity sources.
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,True,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) . The idea is that OS models, by definition, already disclose certain types of information. Hence, Recital 102 lists information on parameters, including weights, model architecture, and model usage as a prerequisite for systems to be considered OS. However, it does not mandate the disclosure of energy consum'","The document states that open‑source general‑purpose AI models are excluded from transparency requirements unless they pose systemic risk and that energy consumption is not mandated, which directly supports the statement that they are fully exempt unless they pose systemic risk."
q213,Which software package was used to measure energy consumption during inference runs?,Code Carbon,Code Carbon,is_blank,is_blank,is_blank,Doc [luccioni2024#0032]: 'and used the Code Carbon package to measure both the energy consumed and the carbon emitted during inference.',"The question asks for the software package used to measure energy consumption during inference. Both Doc [luccioni2024#0032] and Doc [morrison2025#0032] explicitly state that the Code Carbon package was used, so the answer is Code Carbon."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: '53 of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search',"The analysis examined 100 news articles and reports that 53 of them cited the contested 3 Wh/10× Google search estimate, which corresponds to 53% of the articles."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a clear numeric statement about the number of AI training runs conducted globally on renewable-only power in 2022.
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'RD staff costs including equity are between 29 and 49 of the total amortized cost.',"The question asks for the percentage range of R&D staff costs (including equity) relative to total amortized cost for the four models. The cited passage states that these costs are between 29 and 49 of the total, which corresponds to 29%–49%."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents explicitly state the US national datacenter average PUE for 2020, so the answer cannot be determined with confidence."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,Doc [kim2025]: proposes the Compute Time Calibration Function (CTCF) to accurately determine the optimal instance based on user requirements.,"The question asks for the function that adjusts for discrepancies between theoretical and actual GPU performance to improve instance selection accuracy. Doc [kim2025] explicitly names this function as the Compute Time Calibration Function (CTCF), providing the required answer."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024#0033]: 'Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure ) to 1024 (Figure ) does not induce a clear or significant effect in inference energy cost.'","The statement from Doc [samsi2024#0033] explicitly says that more shards lead to higher energy cost per response for LLaMA‑65B, so the claim is true."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability.'","The question asks whether current EU rules require open‑source general‑purpose AI models to report energy consumption to authorities. The cited passage from Doc [ebert2024] states that the current rules contain an exemption for open‑source models, meaning they are not required to report, so the correct answer is False."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a specific numeric value for the KV Cache size of the OPT-2.7B model at batch size 32.
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific numeric estimate of the net carbon emissions for the pre‑training of FLM-101B.
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the excerpts in the provided context give a quantitative factor for the increase in energy consumption of Llama 3.1 70B when deployed on two nodes versus one node.
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,14.8×,14.8,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: GPT-4.1 nano 0.207 ± 0.047    0.575 ± 0.108    0.827 ± 0.094  o3 1.177 ± 0.224    5.153 ± 2.107  12.222 ± 1.082,"The table in Doc [jegham2025] lists energy consumption for long prompts (last column): 0.827 Wh for GPT-4.1 nano and 12.222 Wh for o3. Dividing 12.222 by 0.827 gives a factor of approximately 14.8, indicating o3 consumes about 15 times more energy than GPT-4.1 nano for a long prompt."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",almost 30%,30,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30 of all PPAs purchased by corporations worldwide...'","The question asks for the share of PPAs held by Amazon, Microsoft, Meta, and Google in 2020. Doc [luccioni2025a] states they accounted for almost 30% of all PPAs purchased by corporations worldwide, so the answer is 30%."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.0022 kL,0.0022,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Together, these add an additional 2.2 liters consumed and 0.013 kg COeq per GPU.'","The question asks for the water used to mine rare earth materials for one H100 GPU. Doc [morrison2025] states that mining provides an additional 2.2 liters of water per GPU. Converting liters to kiloliters (2.2 L ÷ 1000) gives 0.0022 kL, which is the required estimate."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided passages do not contain a specific numeric value for the total execution time in seconds for a sparse Mixtral model with batch size 1 on an NVIDIA A40-48 GB GPU.
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years.'","The passage states that GPU theoretical performance per watt doubles every 3-4 years, which matches the claim in the question, so the statement is true."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,False,0,is_blank,is_blank,is_blank,"Doc [han2024#0002]: 'Importantly, the health costs are unevenly distributed across counties and communities, particularly affecting low-income counties that could experience approximately 200x per-household health costs than others.'; Doc [han2024#0019]: 'The results reveal a highly disproportionate distribution of health impacts across counties, with low-income communities particularly affected. The ratio of the highest to lowest county-level per-household health cost reaches approximately 200.'","The question asks whether public health costs of AI are evenly distributed across U.S. communities. Both Doc [han2024#0002] and Doc [han2024#0019] state that the costs are unevenly distributed, with a ratio of about 200 between highest and lowest counties. Therefore, the statement is false."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the price per hour for an NVIDIA H100 as reported by Chen et al. (2025), so I cannot answer this question based on the available context."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",6.7 billion USD,6700000000.0,USD,is_blank,is_blank,"Doc [han2024#0015]: 'health cost of about 6.7 billion, or 47.5 per household, in 2023.'","The question requests the total public health cost of U.S. data centers in 2023. Doc [han2024#0015] explicitly states that the cost is about 6.7 billion dollars, which provides the required value."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not mention a storage service used for sharding and streaming datasets for spot VMs that could terminate at any time.
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about Mélange's cost reductions for short-context workloads on the Arena dataset with a 120ms SLO compared to single-GPU baselines.
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents mention the use of an open-source platform for 4-bit quantization and local deployment, but the specific tool name is not given, so the answer cannot be determined from the context."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey (D-MA),Senator Edward J. Markey (D-MA),is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'Relating to AI more specifically, ... a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.'","The question asks for the senator who introduced the AI Environmental Impacts Act bill. Doc [ebert2024] explicitly names Senator Edward J. Markey (D-MA) as the introducer on 1 Feb 2024, providing the required answer."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,4 years,4,years,is_blank,is_blank,"Doc [morrison2025#0038]: Internally, we assume a 4 year lifespan for our GPUs","The question asks for the estimated average GPU lifetime before retirement in 2024. Doc [morrison2025#0038] explicitly states that the authors assume a 4‑year lifespan for GPUs, which is the most direct estimate provided in the context."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 V100 GPUs,2,V100 GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 13B 2 64 1 64
Doc [samsi2024]: The 7B model was run on a single GPU and 13B on two GPUs in each case whereas the 65B model was run on 8 V100 GPUs","The question asks for the bare minimum number of 32GB V100 GPUs needed for LLaMA‑13B inference without compression or quantization. Doc [samsi2024] explicitly states that the 13B model requires 2 V100 GPUs, so 2 is the correct answer."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any statement indicating that the relationship between runtime and energy consumption during inference was found to be nearly linear, so the answer cannot be determined with confidence from the given context."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, over 4x the five cars estimate",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Google reports that training their open source Gemma family of language models emitted 1247.61 tons COe, over 4x the estimate that forms the basis for the five cars number'","The question asks for the pre‑training GHG emissions of Gemma and how it compares to the five‑cars estimate. Doc [luccioni2025c] explicitly states the emissions as 1247.61 tCO2e and notes that this is over four times the five‑cars estimate, providing the required answer."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,336 hours,336,hours,is_blank,is_blank,Doc [strubell2019#0022]: 'report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).',"The question asks for the training duration on 3 GTX 1080 Ti GPUs. Doc [strubell2019#0022] states that ELMo was trained on 3 GTX 1080 GPUs for 2 weeks, which equals 336 hours, so that is the answer."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: 'The GPU alone accounts for 74 of the total energy consumption due to these components.',"The question asks for the proportion of power that GPUs represent in a typical datacenter. Doc [dodge2022] states that GPUs account for 74% of the total energy consumption, which corresponds to the same proportion of provisioned power in the context provided."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a net cost figure for fine‑tuning a sparse Mixtral model with 2 million queries on an NVIDIA H100 GPU.
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,wu2021b;dodge2022,https://arxiv.org/pdf/2108.06738;https://arxiv.org/pdf/2206.05229,"Doc [wu2021b]: 'PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) whereas the PUE of Facebook dat'","The question asks for the reported PUE of Google's hyperscale data centers in 2021. Both Doc [wu2021b] and Doc [dodge2022] explicitly state that the PUE was 1.10 for 2021, so the answer is 1.10."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high speed interconnect between each node, and between 2 and 128 nodes concurrently per training run.'",The question specifies 12 nodes in the JetMoE-8B training cluster. Doc [morrison2025] states that each server (node) contains 8 H100 GPUs. Multiplying 12 nodes by 8 GPUs per node yields 96 total H100 GPUs.
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a figure for the CO2e emissions per year for an average human life globally.
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'A single short GPT-4o query consumes 0.42 Wh ( Wh)',"The question asks for the energy consumption of a single short GPT‑4o query. Doc [jegham2025] explicitly states that a short query consumes 0.42 Wh, so that value is reported as the answer."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",0.45,0.45,is_blank,is_blank,is_blank,Doc [griggs2024#0013]: '0.45 GPU on-demand cost for three GPU provisioning strategies.',"The question seeks the normalized on‑demand hourly price for an H100 GPU. In Doc [griggs2024#0013] the value 0.45 appears adjacent to the heading ‘GPU on-demand cost for three GPU provisioning strategies,’ indicating the normalized on‑demand hourly price for the H100 GPU as reported in the Griggs et al. (2024) evaluation."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25 times faster,1.25,times,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100","The passage states that for the 13B model the inference latency (and thus throughput) improves by 1.25 times on A100 versus V100, giving an approximate speedup of 1.25×."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.14 L/kWh,3.14,L/kWh,is_blank,is_blank,Doc [li2025b#0016]: 'our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the 4.35 L/kWh recently reported by .',"The question asks for the U.S. national average water consumption for electricity generation, which is the electricity water intensity factor (EWIF). Doc [li2025b#0016] explicitly gives the EWIF for the U.S. as 3.14 L/kWh, so that is the answer."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'When actively training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W...' ","The question asks for the average GPU power during the first 300 logging steps of OLMo 2 7B training while actively training. Doc [morrison2025] states that during active training the average GPU power exceeds 600W, so the answer is over 600W."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the amount of clean freshwater evaporated by GPT‑3 training in Microsoft's U.S. data centers, so I cannot answer this question based on the available context."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain information about the average system power per processor for TPU v2 or V100 GPU, so the difference cannot be determined."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,Doc [wu2021b]: 'current averages of less than 3 years for cell phones and 4 to 5 years for servers.',"The question asks whether smartphones currently average lifetimes of less than 3 years. Doc [wu2021b] explicitly states that cell phones have current averages of less than 3 years, supporting a True answer."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The context documents do not specify which model ranked highest in the eco‑efficiency DEA analysis.
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '...reaching 62 million tonnes in 2022.',"The question asks for the worldwide electronic waste in 2022, and Doc [luccioni2025a] explicitly states that it reached 62 million tonnes that year."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'The FLM-101B model is structured with a hidden state dimension of , a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of .'","The question asks for the context window size of FLM-101B. In Doc [li2025a] the context window is explicitly stated as 2,048 tokens, so that is the answer."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a numeric percent reduction for moving workloads to AWS in North America.
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the percentage of Amazon's People Managers globally who identified as women in 2023, so I cannot answer the question with confidence."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention intra-zone scaling with T4 GPUs achieving nearly linear per-GPU speedup for CV models.
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the number of packages Amazon delivered via electric vehicles in Europe in 2023, so the answer cannot be determined from the given context."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",21 to 33%,"[21, 33]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Excluding equity, the fraction decreases to 21 to 33 (see   for additional plots).'","The question asks for the percentage range of total amortized cost that computing hardware represents when equity is excluded. Doc [cottier2024] explicitly states this fraction is 21 to 33 percent for the four key models, so the answer is 21 to 33%."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention diffusion models or compare their inference power draw to LLMs, so the statement cannot be evaluated."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",True,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'In addition, we propose reporting the financial cost or price tag of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.' Doc [schwartz2019#0008]: 'Reporting the computational price tag of finding, training, and running models is a key practice.'","The question asks whether Green AI involves providing the financial cost of finding, training, and running models. Both Doc [schwartz2019] and Doc [schwartz2019#0008] state that reporting the financial or computational price tag of these activities is a key practice in Green AI. Therefore the statement is True."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a numeric value for the average CO2 in pounds per kilowatt-hour from the U.S. EPA, so the answer cannot be determined from the context."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,A10G,A10G,is_blank,is_blank,is_blank,"Doc [griggs2024#0008]: 'smaller sizes are best served on A10G, and larger sizes are best served on A100.'","The question seeks the GPU architecture that is most energy‑efficient for very small inference outputs (a single classification token).  Doc [griggs2024#0008] states that smaller request sizes, which correspond to single‑token generation, are best served on the A10G GPU, indicating its superior energy efficiency in that scenario."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",approximately 280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'With an SLO requirement of 400 TPS, selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of 0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of 2.699, wh'","The question asks for the percentage cost difference between the Max‑Performance instance g6e.xlarge and InferSave’s top choice g4dn.xlarge in the online 400 TPS experiment.  The table gives costs of 0.71 USD for g4dn.xlarge and 2.699 USD for g6e.xlarge.  The increase is (2.699–0.71)/0.71 ≈ 2.80, i.e., about 280 % more expensive."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not mention Yelp sentiment analysis benchmarks or the performance of traditional versus large language models for that specific dataset, so the statement cannot be verified."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the size increase of Facebook's recommendation and ranking models between 2019 and 2021.
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Doc [schwartz2019]: 'Amazon's AWS (the largest cloud computing platform) only covered fifty percent of its power usage with renewable energy',"The question asks for the percentage of power usage covered by renewable energy for Amazon's AWS in 2018. Doc [schwartz2019] explicitly states that AWS covered fifty percent of its power usage with renewable energy, which directly answers the question."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"Doc [dodge2022#0070]: 'for very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30 in multiple regions, and up to 80 in West US;'","The question asks for the maximum potential percentage reduction in CO2 emissions for a short DenseNet 201 job in West US under the Flexible Start optimization. Doc [dodge2022#0070] explicitly states that the maximum reduction observed is up to 80%, which is the value used in the answer."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: d, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The passage states that image generation requires 2.9 kWh versus 0.002 kWh for text classification, yielding a ratio of 2.9/0.002 ≈ 1450. Thus the most intensive task exceeds the least intensive by a factor of about 1450."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration.'","The question asks whether the AI Act requires disclosure of GHG emissions for AI applications such as oil and gas exploration.  Doc [ebert2024] explicitly states that the Act does not address these emissions, indicating it does not mandate such disclosure.  Therefore the statement is false."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: 'The GPU alone accounts for 74 of the total energy consumption due to these components.',"The question asks for the percentage of total electricity consumption attributed to the GPU during a BERT-base training experiment. Doc [dodge2022] explicitly states that the GPU alone accounts for 74% of the total energy consumption, so 74% is the answer."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'Energy consumption should be reported at the cumulative server level.',"The question asks for the recommended measurement level for reporting AI energy consumption. The cited passage from Doc [ebert2024] explicitly states that energy consumption should be reported at the cumulative server level, which balances accuracy and feasibility."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the amount of fiber optic cable installed globally to support AI workloads in 2023, so the answer cannot be determined from the available context."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provided do not contain a projected range of electricity consumption by global AI for the year 2027, so I cannot answer the question based on the available context."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024.
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not give a specific maximum batch size for fine‑tuning Mixtral on an NVIDIA A100‑40GB GPU, so I cannot answer this question with confidence."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a specific figure for the operational energy footprint reduction at Facebook between 2019 and 2021 due to iterative hardware-software optimization.
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention McKinsey projections for data center electricity consumption in 2030.
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,8 NVIDIA A100-80GB GPUs,8,GPUs,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'assigning 1, 2, 4, or 8 GPUs accordingly.'","The question asks how many A100-80GB GPUs are needed to serve Llama2-70b at BF16 precision. The document assigns 8 GPUs to models in the >70B class, and Llama2-70b falls into that category, so 8 GPUs are required."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not mention recomputation or swapping as preemption mechanisms or compare their energy consumption, so I cannot answer this question based on the available context."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '... Google reports a 48 increase in GHG emissions since 2019 ...',"The question asks for the percentage increase reported by Google in its 2024 environmental report. Doc [luccioni2025a] states that Google reports a 48 increase in GHG emissions since 2019, which is interpreted as a 48% increase."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,800M,800,USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: we estimate that it cost 800M to acquire the hardware used to train GPT-4,"The question asks for the upfront hardware acquisition cost for GPT-4. Doc [cottier2024] explicitly states that the cost is 800M, so that is the answer."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: ustainability. The umbrella term `Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The statement claims Sustainable AI was proposed to only encompass climate-positive applications. The cited document shows it also aims to improve the environmental sustainability of AI approaches, so the claim is false."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of emissions.'","The question asks for the BERT training emissions in pounds of CO2e. Doc [luccioni2025b] states the 2019 Strubell study quantified this as 626,155 lbs, so that is the answer."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE-8B architecture or its inference computation relative to Llama2-7B, so I cannot answer this question based on the available context."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?","1,287 MWh",1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO equivalent () , while requiring more than 700 kiloliters (kL) of water for cooling alone , enoug","The question asks for the estimated training energy of GPT-3 in MWh. Doc [jegham2025] explicitly states that training GPT-3 consumes 1,287 MWh, so that number is used as the answer."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not mention a financial sentiment case study or provide data on its accuracy and F1 scores, so I cannot determine if they always improved after optimization."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'peaked in 2022, with 10 of notable models that year releasing some degree of information.'","The question asks for the year when direct environmental disclosures peaked. Doc [luccioni2025c] explicitly states that this peak occurred in 2022, so 2022 is the correct year."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention CV models, high granularity, intercontinental training, or a 7% performance slowdown, so the answer cannot be determined from the given context."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a clear, explicit total of input plus output tokens processed during the entire online inference workload evaluation."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific numeric value for the maximum batch size of fine-tuning Mixtral with a dense setup on the Hellaswag dataset using an NVIDIA A40 GPU with 48 GB memory.
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give enough information to determine this quantity.
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so the answer cannot be determined from the given context."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",0.094 thousand grams,0.094,kg,is_blank,is_blank,"Doc [dodge2022#0068]: 'We find (grams) per GPU hour in the most efficient region to average 34, and in the least efficient region to average 128, ...'","The question asks for the difference between the most and least efficient regions. Doc [dodge2022#0068] gives 34 g and 128 g per GPU hour; the difference is 94 g, which is 0.094 thousand grams (≈0.094 kg)."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a numeric total execution time for a sparse Mixtral model fine‑tuned on an NVIDIA A40‑48GB with batch size 10, so the answer cannot be determined with confidence."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",True,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 'As shown in Fig., the MoE layer is the most time-consuming, accounting for 85 of the overall execution time on average.'","The context describes the MoE layer as the most time-consuming component of LLM fine-tuning, implying it is a primary target for performance improvements."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",9.6%,9.6,%,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: FS 5.4 3.6 2.0 1.5 1.7 1.9 4.2 4.0 3.3 2.8 2.2
PR 7.6 9.2 9.6 1.6 2.0 2.4 9.2 9.3 9.2 9.6 9.6",The question asks for the maximum emissions saving achievable with the Pause and Resume optimization for a 6B‑parameter transformer. In Doc [dodge2022] the table lists the percent emissions reduction (PR) for each model; the entry for the 6B transformer is 9.6%. Thus the maximum potential saving is 9.6 %. 
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a numeric estimate of the total fine‑tuning cost for a Mixtral model on GSM8K using sparse MoE on an NVIDIA A40‑48GB GPU.
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a numeric statement of the total energy consumption for training the FLM-101B model, so the answer cannot be determined from the context."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g CO2eq,0.32,g CO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: ... for instance  emits just 0.32g of  per 1,000 queries, ...","The passage from Doc [luccioni2024] states that the BERT‑based model emits 0.32 g of CO₂eq per 1,000 text classification queries, which directly answers the question."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'finding that training accounted for only half of the model's overall emissions, meaning that similar studies that only took training into account were potentially underestimating their emissions by half.'","The 2023 article cited in Doc [luccioni2025b] states that training accounted for only half of the BLOOM model’s overall emissions, which corresponds to 50%."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a score for the JetMoE-8B model on the GSM8k benchmark, so the answer cannot be determined from the given context."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 V100 32GB GPU,1,V100 32GB GPU,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 7B   1  64  1  64
Doc [samsi2024]: The 7B model was run on a single GPU","The table in Doc [samsi2024] lists the bare minimum hardware for LLaMA 7B as 1 GPU, and the accompanying text confirms that the 7B model was run on a single GPU. Therefore the bare minimum number of NVIDIA V100 32GB GPUs required is one."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'against using GPU-level or other component-based power consumption tracking for overall energy measurements.',The question asks whether GPU‑level monitoring is recommended. Doc [ebert2024] explicitly states that GPU‑level or component‑based power consumption tracking is not recommended for overall energy measurements; the preferred approach is cumulative server‑level measurement. Therefore the statement is false.
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear statement that adding compute resources to accelerate MoE layers when fine-tuning LLMs increases costs.
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",2738 days,2738,days,is_blank,is_blank,"Doc [morrison2025#0007]: 's of fuel, or the amount of water consumed by one average person in the United States in about 7.5 years.'",The document states that training a model consumes water equivalent to about 7.5 years of an average U.S. person's daily water use. Converting 7.5 years to days (7.5 × 365) gives approximately 2738 days.
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10–50 requests,"[10, 50]",requests,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: '10–50 queries on GPT-3 consumes around half a liter of water.',"The question asks how many user requests would consume a 500 ml bottle of water during GPT‑3 training. Doc [luccioni2025a] states that 10–50 queries use about half a liter, so the required number is 10 to 50 requests."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,is_blank,is_blank,"Doc [dodge2022#0065]: '... which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons per year.'","The question asks for the estimated annual CO2 emission of an average US home. Doc [dodge2022#0065] explicitly states that this equals 8.3 metric tons per year, providing the required value."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",20 samples,20,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse. -D  -S  -D  -S   2  8  6  20   1  3  2  8',"The table in Doc [xia2024] lists the maximum batch sizes for sparse fine‑tuning. The largest sparse batch size shown is 20 samples, indicating that the longest‑running MoE layer for a sparse Mixtral model on an NVIDIA A40‑48GB GPU used a batch size of 20 samples."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",More than 20 billion USD,20,USD,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'The overall public health costs could reach more than 20 billion,'","The question asks for the projected value of the public health burden. Doc [han2024] states that the overall public health costs could reach more than 20 billion, supporting a 20 billion‑USD answer."
