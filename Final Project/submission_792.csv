id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 t,1.2,t,patterson2021,https://arxiv.org/pdf/2104.10350,"""a single passenger round trip SF-NY is ~1.2t CO 2 e (Table 2).""",The passage explicitly states that the approximate CO2e for a single passenger round trip from San‚ÄØFrancisco to New‚ÄØYork is about 1.2‚ÄØtons.
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24,24,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""In 2023, AWS increased the number of data centers using recycled water for cooling from‚ÄØ20 to‚ÄØ24, including two data centers in Virginia, one in California, and one in Singapore.""","The passage states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, giving the total number as 24."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021,https://arxiv.org/pdf/2104.10350,"""It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.""","The Patterson 2021 document explicitly states the training duration for GPT‚Äë3 with 10,000 V100 GPUs at 24.6‚ÄØTeraFLOPS/sec is approximately 14.8 days, which directly answers the question."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,morrison2025,https://arxiv.org/pdf/2503.05804,"Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not publish embodied carbon data, so the authors adopt the estimate from Luccioni et al. (2023) of 3700‚ÄØkg CO2eq per 8‚Äënode server, which translates to 463‚ÄØkg CO2eq per GPU. This directly provides the estimated embodied carbon per GPU."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon Solar Farm Maryland‚ÄìCPV Backbone will avoid more than 64,000 metric tons of CO2e each year‚Äîthe equivalent of taking more than 13,900 cars off the road.""","The report states that the avoided emissions of the solar farm are equivalent to removing over 13,900 cars from the road, providing the requested estimate."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""",The quoted sentence states that using the growth strategy for the 101B model saves 72% of the training time compared to a from-scratch approach.
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""total energy consumption of the US data centers increased by about 4% from 2010-2014""","The context from doc wu2021b states that U.S. data center electricity consumption rose by about 4% over the period from 2010 to 2014, which is the average increase reported for that timeframe."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,morrison2025,https://arxiv.org/pdf/2503.05804,8 0.036 12.0 0.054 12.64 21.5 bil.,"The SGLang benchmarking table for Llama‚ÄØ3.2‚ÄØ1B lists the GPU Power Usage at an 8‚ÄØrequest/s frequency as 0.036‚ÄØkWh, directly answering the query."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M."" (cottier2024#0019) and ""trained with our growth strategy under a budget of $100K."" (li2025a)","The amortized training cost of GPT‚Äë4 is $40‚ÄØmillion, while the total training budget for FLM‚Äë101B is $100‚ÄØ000. Dividing 40‚ÄØmillion by 100‚ÄØk yields a factor of 400. The quoted statements from the respective documents confirm these values."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""This is a more than 6,750 fold improvement in processor clock speed""","The context states that from the Intel 4004‚Äôs 740 kHz to a typical 2021 microprocessor‚Äôs 5,000,000 kHz, the improvement is more than 6,750 times, yielding a 6,750‚Äëfold increase in clock speed."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long‚Äëterm investigation‚Ä¶""","The context explicitly states that the study was launched in the fall of 2014, giving the year of launch as 2014."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"""The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.""","The cited passage states that the total permitted NOx limit for data center backup generators in Northern Virginia is about 13,000 short tons, which directly answers the question."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.1%,0.1,%,patterson2021,https://arxiv.org/pdf/2104.10350,"""1500B parameters but only 0.1% activated per token""","The context states that the Switch Transformer has 1500‚ÄØbillion parameters and only 0.1‚ÄØ% of them are activated for each token, so the activated percentage is 0.1‚ÄØ%."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,The passage states that the English portion of FLM‚Äë101B required 28.22 zettaFLOPs of floating‚Äëpoint operations for training.
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Table 1: JetMoE-8B hyperparameters ‚Äì ""Nexperts 8"" and the text stating ""We set the same number of experts to 8 and top‚Äëk to 2 for every layer.""","The hyperparameter table and accompanying description explicitly state that each Mixture‚Äëof‚ÄëExperts layer contains 8 experts, which is the answer to the question."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"""a full training run would take 60 days.""","Both referenced documents state that the 6.1‚ÄØbillion‚Äëparameter transformer was only trained to 13% of completion over 8 days, and extrapolating this gives a full run of 60 days."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain a projected figure of approximately 1,300 premature deaths attributable to scope‚Äë2 pollutants from U.S. data centers in 2028, but no explicit estimate for 2030 is given. Without a 2030 value or a clear trend to extrapolate from, the question cannot be answered with confidence from the supplied context."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",44.41,44.41,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"Table 6 shows the performance of the three stages of FLM on Open LLM, where the 101B model lists an Average score of 44.41.","The table lists the average performance for each FLM stage; the 101B model‚Äôs average is 44.41, which is the final average score on the Open LLM Leaderboard."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""The UN‚Äôs Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled""","The quoted sentence from Doc [luccioni2025a] states that approximately 22% of e‚Äëwaste is formally collected and recycled, which directly answers the question."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided passages do not contain a clear numeric value for the energy consumption (in MWh) required to pre-train the BLOOM model.
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88,88,is_blank,is_blank,is_blank,"""To be representative of a broad diversity of deployment use cases, we sampled 88 models, some of which were trained for the tasks that we selected, whereas others were designed to be used as zero-shot or multi-task models...""","The study explicitly states that 88 models were sampled and analyzed, providing the exact count required for the answer."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3√ó,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3√ó.""","The context explicitly states that raising GPU utilization to 80‚ÄØ% for language model training reduces the overall carbon footprint by a factor of 3, so the answer is 3√ó."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""","The context explicitly states that the total wall‚Äëclock time for training FLM‚Äë101B using the growth strategy is 21.54 days, so the answer is derived directly from that sentence."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 ‚Äì 6.6,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"""global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027""","The document explicitly states that the projected water withdrawal for global AI demand in 2027 falls within the range of 4.2 to 6.6 billion cubic meters, which is used as the answer."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of a hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification. Therefore, the answer cannot be determined with confidence from the documents."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).""","The provided context explicitly states that Red AI is on the rise, not on the decline, thereby making the statement false."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,dodge2022,https://arxiv.org/pdf/2206.05229,"""Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8)‚àó13.8 = 103.5 MWh, or 103,500 kWh""",The document provides the calculation for a full 6.1‚Äëbillion‚Äëparameter transformer run and explicitly states the estimated total energy consumption as 103.5‚ÄØMWh.
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x,2.2,is_blank,cottier2024,https://arxiv.org/pdf/2405.21015,"""the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.""","The cited passage from Cottier et al. (2025) states that total compute during model development is 1.2‚Äëto‚Äë4‚Äëtimes larger than the final training run, with a median factor of 2.2. Thus the answer is 2.2 times larger."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"This paper examines how the problem of Jevons‚Äô Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The Luccioni et al. 2025 paper argues that technical efficiency gains in AI can lead to rebound effects, citing Jevons‚Äô Paradox as the economic principle that explains why efficiency can increase overall consumption rather than reduce it."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""A significant portion of machine learning model experimentation utilizes GPUs at only 30-50%"", as noted in Figure 10 and the text surrounding it in the Wu2021a document.","The provided context states that most experimentation uses GPUs at 30‚Äë50% capacity, which is far below the 80% threshold claimed in the statement. Therefore, the statement is false."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.""","The passage explicitly names the collaborative initiative as the AI Energy Score project, noting its goal to create a unified, standardized method for comparing inference efficiency across AI models."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear numeric value for the total execution time (in seconds) of a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU. The figures referenced show execution time breakdowns but the actual numeric values for the specific configuration are not explicitly stated in the text. Therefore, I cannot answer the question with confidence based on the documents alone."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,1287 MWh,1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity...""","The Jegham 2025 document explicitly states that GPT‚Äë3 training consumed 1,287‚ÄØMWh of electricity, providing the required numeric value."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,is_blank,is_blank,Table 1: JetMoE-8B hyperparameters ‚Äì ‚ÄúNexperts 8  Top‚Äëk 2‚Äù.  The text states: ‚ÄúWe set the same number of experts to 8 and top‚Äëk to 2 for every layer.‚Äù,The hyperparameter table and accompanying description explicitly state that each layer activates 2 experts (top‚Äëk‚ÄØ=‚ÄØ2) out of 8 total experts for each token.
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,69 years,69,years,stone2022,https://arxiv.org/pdf/2211.06318,"The field of Artificial Intelligence was officially born and christened at a 1956 workshop; the report also notes the field‚Äôs inception sixty years ago, indicating its age in 2025 would be roughly 69 years.","The document cites 1956 as the birth year of AI and describes its inception as about sixty years prior to the report‚Äôs context, which places the field‚Äôs age at approximately 69 years in 2025."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not contain any information about the percentage of AI inference workloads in Asia powered by coal in 2023, so the question cannot be answered with confidence."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.""","The context states that Amazon‚Äôs energy storage capacity in 2023 is 1.3 gigawatts, which directly answers the question."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22,22,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon‚Äôs energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources‚Äîan increase from 19 regions in 2022.""",The passage explicitly states that 22 AWS data center regions had 100% of their electricity matched with renewable energy in 2023.
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,"2,300 transatlantic flights",2300,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""cumulative emissions from approximately 2,300 transatlantic flights between Boston and London.""","The document explicitly states that GPT‚Äë4o‚Äôs projected annual carbon emissions are comparable to those of about 2,300 transatlantic flights, providing a direct numeric answer."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS),Neural Architecture Search (NAS),is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.","The ""five cars"" estimate from Strubell et al. 2019 was derived from the energy cost of performing Neural Architecture Search (NAS), an infrequently performed AI process used to automate model development. The quoted passage directly states that the cost estimate was based on NAS."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction""","The quoted sentence directly states that global carbon emissions fell by 6.4% in 2020, indicating the reported drop during the COVID‚Äë19 pandemic."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",44%,44,%,is_blank,is_blank,"""reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.""","The cited passage states that targeting a TPOT of 100‚ÄØms reduces energy consumption per generation by 44% relative to a latency‚Äëminimizing configuration, giving the required percentage decrease."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,is_blank,is_blank,"The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. 
Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","Both passages state the increase was 300,000‚Äëfold, not 200,000‚Äëfold. Therefore the statement claiming a 200,000x increase is incorrect, making the overall claim false."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific figure for the global average PUE of AI‚Äëdedicated data centers in 2023. The only PUE value given is the overall global average for all data centers (1.58), which does not distinguish AI‚Äëdedicated facilities. Therefore, I cannot answer the question with confidence based on the supplied context."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B,2,B,shen2024,https://arxiv.org/pdf/2404.07413,JetMoE-8B has 8B parameters while only activating 2B for each input token,"The document states that JetMoE-8B contains 8B total parameters but activates only 2B per input token, giving the activated parameter count during inference."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.""","The context explicitly states that the AI field was christened in 1956 at the Dartmouth workshop, providing the year as the answer."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provided do not contain a clear list of GPU energy consumption per 1,000 queries for the models in the appendix of a 2025 study, so the highest consumption cannot be determined with confidence."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",2000 microseconds,2000,us,xia2024,https://arxiv.org/pdf/2408.04693,"The execution‚Äêtime plot for the MoE layer (Fig.‚ÄØ6) lists the Y‚Äëaxis ticks at 0,‚ÄØ400,‚ÄØ800,‚ÄØ1200,‚ÄØ1600,‚ÄØ2000‚ÄØ¬µs for the Dense(bsz=30) case, indicating the longest kernel in that configuration runs ‚âà2000‚ÄØ¬µs.","The figure‚Äôs Y‚Äëaxis shows the maximum observed kernel time for Dense(bsz=30) as 2000‚ÄØ¬µs, which is the longest among the listed kernels. Thus the longest kernel execution time is 2000‚ÄØ¬µs."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,is_blank,is_blank,"Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed) ‚Äì Llama 7B 356 MWh, 14 tCO2e",The table in Doc¬†[luccioni2025c#0042] lists the pre‚Äëtraining GHG emissions for the Llama¬†7B model as 14¬†tCO‚ÇÇe.
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context provided includes a table of maximum batch sizes for BlackMamba on CS and MATH datasets, but does not provide a value for the GSM8K dataset. Without explicit information, I cannot determine the maximum batch size for that specific combination."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The document explicitly states that for a maximum generation length of 512 tokens, LLaMA‚Äë65B consumes about 3‚Äì4 Joules per decoded output token, providing the numeric range for energy per token."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The supporting passage from Doc wu2021b states that approximately 770‚ÄØmillion people lack access to a stable supply of electricity, directly confirming the claim in the question."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.""","The quoted passage from Morrison‚ÄØ2025 explicitly states that operational impacts include GHG emissions from servers and data‚Äëcenter cooling, contradicting the claim that they do not include such emissions."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",25000,USD,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.""","The passage explicitly states that Grover‚Äôs training on 256 TPU chips over two weeks cost approximately $25,000, which directly answers the question."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%."",","The quoted sentence from the Wu 2021a document explicitly states that quantizing RM2 from 32‚Äëbit to 16‚Äëbit reduces its overall model size by 15%, which is the requested percentage."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU""","The context from Doc ebert2024 explicitly states that the global average PUE for data centers in 2023 was 1.58, which directly answers the question."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",True,1,is_blank,is_blank,is_blank,"Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The statement is directly stated in Doc [patterson2021#0001], confirming that sparsely activated DNNs achieve the claimed energy savings without accuracy loss."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,Gemini Ultra has the highest fraction of R&D staff cost at 49%,The cited passage explicitly states that Gemini Ultra‚Äôs R&D staff cost (including equity) accounts for 49‚ÄØ% of its total development cost.
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,is_blank,is_blank,"""The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)""","The cited passage states that in BlackMamba sparse fine‚Äëtuning on an NVIDIA A40‚Äë48GB GPU with batch size‚ÄØ1, the optimizer stage accounts for up to 53% of the total running time."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"14,380 vans",14380,vans,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Last Mile Electric Delivery Vehicles table: U.S. 2,600 in 2022 ‚Üí 11,800 in 2023 (add 9,200); Europe 1,220 in 2022 ‚Üí 3,000+ in 2023 (add ~1,780); India 3,800 in 2022 ‚Üí 7,200+ in 2023 (add ~3,400).","The table gives the number of electric delivery vans per region for 2022 and 2023. Subtracting each 2022 figure from the corresponding 2023 figure gives the additions: 9,200 (U.S.) + 1,780 (Europe) + 3,400 (India) ‚âà 14,380 vans added across the two years."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about Microsoft directly contracting wind turbines for Azure AI clusters in 2023, so a confident answer cannot be given."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG‚Äôs experience in dealing with their clients and using AI to optimize and improve existing processes."" ‚Äì and ""does not offer specific calculations translating individual project numbers to a global scale.""","The cited document states that the 5‚Äë10% claim lacks detailed, publicly available calculations and does not provide sound scientific grounding, so the claim cannot be considered supported."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,1‚ÄØL/kWh,1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"""data centers can evaporate approximately 1 ‚Äì 9 liters per kWh of server energy: 1 L/kWh for Google‚Äôs annualized global on-site water efficiency [4]""","The excerpt states that Google‚Äôs annualized global on‚Äësite water efficiency is 1‚ÄØL/kWh, which represents the average water use effectiveness for Google‚Äôs data centers, including those dedicated to AI, in 2024."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,is_blank,is_blank,"""manufacturing carbon cost accounts for 74% of the total footprint""","Both Doc [wu2021a#0050] and Doc [wu2021a#0049] state that manufacturing accounts for 74% of a client device‚Äôs total carbon footprint, giving a clear percentage for the answer."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages mention the Earth‚ÄëSun distance, so there is insufficient information to answer the question."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Generally, models with more parameters consume more energy, but this is not always the case.","The cited passage explicitly states that while larger models tend to use more energy, there are instances where a model with more parameters can consume less energy, disproving the claim that greater parameter counts always lead to higher inference energy consumption."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit numeric energy consumption value for the o3 model with a long prompt. Therefore, I cannot provide a confident answer."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,is_blank,is_blank,is_blank,"The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry , and AI-savvy scholars in law , political science, policy , and economics, was launched in mid-fall 2015.",The quoted sentence explicitly states that the inaugural 2015 Study Panel consisted of seventeen members.
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The quoted passage states that the Study Panel found no cause for concern that AI poses an imminent threat, directly contradicting the claim that the panel is concerned about such a threat."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9√ó,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""the explosive growth in AI use cases at Facebook has driven 2.9√ó increase in AI training infrastructure capacity over the 1.5 years.""",The quoted sentence directly states that the explosive AI growth led to a 2.9√ó increase in training infrastructure capacity during the specified 1.5‚Äëyear period.
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Meta‚Äôs Llama‚ÄØ3 family emitted 11,390‚ÄØtCO‚ÇÇe, which is more than 40‚ÄØ√ó the ‚Äòfive cars‚Äô estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Meta reports that their Llama‚ÄØ3 family of models emitted 11,390 tons CO‚ÇÇe35 or over 40√ó the ‚Äòfive cars‚Äô estimate.",The quoted sentence from the document provides the exact pre‚Äëtraining emission figure for Llama‚ÄØ3 and explicitly states it exceeds the ‚Äòfive cars‚Äô benchmark by more than forty times.
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific figure for the metric ton CO2 emissions from OpenAI‚Äôs API requests in January 2024, so a confident answer cannot be derived."
q080,True or False: The AlphaGo program defeated the human Go champion.,True,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""AlphaGo program that recently defeated the current human champion at the game of Go"" and ""AlphaGo ... that beat the human Go champion in a five-game match""","The cited passages explicitly state that AlphaGo defeated the human Go champion, confirming the truth of the statement."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1‚ÄØbillion queries to the model per day."" ‚Äì and ""Flan‚ÄëT5‚Äëxxl at 0.083‚ÄØkWh/1k queries.""","The estimate uses 1‚ÄØbillion daily queries and the per‚Äë1k‚Äëquery energy of 0.083‚ÄØkWh for Flan‚ÄëT5‚Äëxxl. 1‚ÄØbillion queries √ó (0.083‚ÄØkWh √∑ 1,000) = 83,000‚ÄØkWh, which equals 83‚ÄØMWh."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"""The entire alignment process takes 60 H100 GPU hours."", from the JetMoE-8B alignment details.","The document explicitly states that the combined dSFT and dDPO fine‚Äëtuning for JetMoE‚Äë8B required 60 H100 GPU hours, providing the exact number requested."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""Hivemind [39] is a PyTorch-based [32] framework...runs in a decentralized fashion""","The context states that Hivemind is a PyTorch‚Äëbased, decentralized framework used to enable distributed spot instance training across multiple clouds and continents."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,is_blank,is_blank,"""The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].""","The passage explicitly states that the U.S. average mix‚Äôs gross carbon intensity in 2021 was 0.429 kg CO2e per kWh, which directly answers the question."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,fernandez2025,https://arxiv.org/pdf/2504.17674,"""Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).""","The quoted passage from Fernandez‚ÄØ2025 explicitly states that continuous batching replaces finished requests with new ones, thereby reducing idle GPU time, which directly answers the question."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams",1594,grams,is_blank,is_blank,"For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of ùê∂ùëÇ2ùëíùëû for 1,000 inferences, ...","The quoted sentence directly states that stable-diffusion-xl-base-1.0 emits 1,594 grams CO2eq per 1,000 inferences."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about classification experiments on German public administration texts or the accuracy of sentence‚Äëembedding models for that task.
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert‚Äëtiny) to over 3,426 Wh (Command‚ÄëR Plus) ... (see Tables¬†1 and¬†2 in the Appendix)""","The appendix of the 2025 study reports that the GPU energy required for 1,000 inference queries ranges from a minimum of 0.06‚ÄØWh (for bert‚Äëtiny) up to more than 3,426‚ÄØWh (for Command‚ÄëR‚ÄØPlus), providing the requested range."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The quote states that Max-Performance‚Äôs selected instance cost was about 26.7% higher than the instance chosen by InferSave, which directly answers the question."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems""","The passage explicitly defines the expanded form of transparency as ""social transparency"", indicating that it includes socio-technical aspects and the environmental footprint."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,chen2024,https://arxiv.org/pdf/2405.01814,"""To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.""","The 2025 Chen et al. paper describes the inference system named Lamina that uses model-attention disaggregation, as directly quoted in the provided context."
q094,What is the total number of parameters in the JetMoE-8B model?,8B parameters,8,B,shen2024,https://arxiv.org/pdf/2404.07413,JetMoE-8B has 8B parameters while only activating 2B for each input token,"The document explicitly states that JetMoE-8B contains 8 billion parameters, which is the total number of parameters for the model."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist),"The quote explicitly states that researchers do not believe a universal, one‚Äësize‚Äëfits‚Äëall approach can be developed."
q096,What is the name of the emissions metric defined as 'CO‚ÇÇ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed""","The context defines the metric that measures CO‚ÇÇ emissions per unit of electricity consumed as ""Carbon Intensity"", which is the name used in the evaluation framework for energy‚Äëefficient large language models."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5 billion liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""3.5B Liters of water returned to communities from replenishment projects in 2023""","The passage explicitly states that 3.5‚ÄØB liters of water were returned to communities from Amazon‚Äôs replenishment projects in 2023, providing the required numeric value."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.""","The context states that the amortized hardware and energy cost for GPT‚Äë4 is $40 million, which directly answers the question."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"""Importantly, the company‚Äôs data center water consumption increased by ‚àº20% from 2021 to 2022 and by ‚àº17% from 2022 to 2023"" (Doc [li2025b])
""while Google observed a 20% uptick in the same period"" (Doc [luccioni2025a])","Both documents explicitly state that Google's data center water consumption rose by about 20% from 2021 to 2022, which matches the question‚Äôs requested percentage increase."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.59,0.59,is_blank,is_blank,is_blank,"""intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).""","The text states a 41‚ÄØ% drop in throughput for NLP when training across four continents (C‚Äë8) versus the fully local run (A‚Äë8). Therefore, the achieved throughput is 100‚ÄØ%‚ÄØ‚Äì‚ÄØ41‚ÄØ%‚ÄØ=‚ÄØ59‚ÄØ% of the local throughput, which is expressed as 0.59."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",over 800√ó,800,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""In aggregate the optimizations reduce the infrastructure resources required to serve LM at scale by over 800 √ó.""","The context states that platform‚Äëlevel caching, GPU acceleration, and algorithmic optimizations together reduce the operational energy footprint of the Transformer‚Äëbased universal translation model by more than 800 times relative to a CPU server baseline. This directly answers the question of the factor of reduction."
q093,How many parameters does the largest T5 model have?,11‚ÄØbillion parameters,11000000000,parameters,patterson2021,https://arxiv.org/pdf/2104.10350,"""The largest size has 11B parameters""","The passage from Patterson‚ÄØ2021 states that the largest T5 model contains 11‚ÄØbillion parameters, which is the numeric answer to the question."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,Table 2: ‚Ä¶ 101 4 4 12 192 2160 165 52.88%,The table lists the FLOPs utilization for each growth stage; the final 101B stage shows a utilization of 52.88‚ÄØ%.
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",TRUE,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.,"The cited passage explicitly states that custom tags reduce energy consumption for zero‚Äëshot, one‚Äëshot, and few‚Äëshot source‚Äëcode completion tasks, supporting the true statement."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,is_blank,is_blank,"""Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.""","The context explicitly states that, on average, 44% of the total amortized hardware and energy cost is attributed to AI accelerator chips, providing a clear numeric answer."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]""","The context explicitly states that the Finnish project‚Äôs acronym is ETAIROS, confirming the answer."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the energy consumption of a DS Llama 70B model on the FKTG dataset, so the answer cannot be determined from the documents."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook‚Äôs data centers are about 40% more efficient than small-scale, typical data centers.","The passage from Wu et al. (2021) explicitly states that Facebook‚Äôs data centers have a PUE of about 1.10, which directly answers the question."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""115 books would produce the same amount of CO2 as a single Amazon Kindle device""","The life cycle assessment cited in the context states that 115 physical print books generate the same CO‚ÇÇ emissions as one Amazon Kindle e‚Äëreader, providing the numeric equivalence requested."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3700000,GPUs,is_blank,is_blank,"""NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023)""","The quoted sentence from the 2025 paper states that NVIDIA shipped 3.7 million data‚Äëcenter GPUs in 2024, giving the required number."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 ¬µg/m3,9,¬µg/m3,han2024,https://arxiv.org/pdf/2412.06288,"""EPA‚Äôs recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m3, considerably higher than the WHO‚Äôs recommended level of 5¬µg/m3.""",The quoted sentence directly states the EPA‚Äôs recently tightened primary standard for the annual average limit of PM2.5 is 9 ¬µg/m¬≥.
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,M USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""most expensive publicly-announced training runs to date are OpenAI‚Äôs GPT-4 at $40M and Google‚Äôs Gemini Ultra at $30M.""","The cited passage lists Gemini Ultra‚Äôs amortized training cost as $30‚ÄØmillion, which is the value requested."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons paradox,Jevons paradox,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""This is often referred to as Jevons paradox, which observes that when technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use""","The quoted passage explicitly names the phenomenon as Jevons paradox, describing how efficiency gains lead to increased usage and higher overall resource consumption."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,is_blank,is_blank,"""per-household health burden could be 200x more than that in less-impacted communities.""","The cited passage states that the per-household health burden in the most affected, economically-disadvantaged communities could be 200 times greater than in less-impacted communities, directly giving the factor of 200."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""image generation 2.907 3.31"" (Table¬†2) and the accompanying text states that image generation‚Äôs mean consumption is 2.9¬†kWh for 1,000 inferences.","Table¬†2 lists the mean energy per 1,000 queries; the row for image generation shows a mean of 2.907¬†kWh, which is the average consumption requested."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"18,292 lbs",18292,lbs,is_blank,is_blank,"""the average US home energy use is estimated to emit 8.30 metric tons CO2 per year""","The context states that an average US home emits 8.30 metric tons of CO2e annually. Converting 8.30 metric tons to pounds (1 metric ton = 2204.62 lbs) yields 8.30‚ÄØ√ó‚ÄØ2204.62‚ÄØ‚âà‚ÄØ18,292 lbs, which is the estimated yearly CO2e for an average American life (home use)"
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"""the we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.""","The document states that the final FLM-101B model contains 101‚ÄØbillion parameters, which matches the value listed in Table‚ÄØ1 under the Params column."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide county‚Äëlevel per‚Äëhousehold health cost projections for 2030, so the requested county cannot be identified from the given documents."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the total number of parameters in the large language model analyzed in Dodge et al.‚Äôs 2022 paper, so it cannot be answered confidently from the given context."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about Mistral‚Äësmall‚Äôs emissions change after optimization in the financial sentiment classification task, so it is not possible to determine the multiplier from these documents."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,is_blank,is_blank,"In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû.",The cited sentence from the Power Hungry Processing study directly reports the total energy consumed for all model experimentation and evaluation as 754.66 kWh.
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Crucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5].","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art.‚ÄØ55(1)(b) and Art.‚ÄØ9). The provisions cover risks for fundamental rights, and the authors interpret environmental risks as part of those fundamental rights, meaning the required risk assessment must include environmental risks."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The table in Luccioni et al. lists the number of inferences required for the cumulative inference energy to equal the combined training and fine‚Äëtuning energy for BLOOMz‚Äë7B as 592,570,000."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Training energy (kWh) 51,686  
Finetuning energy (kWh) 7,571  
(Combined: 51,686 + 7,571 = 59,257 kWh)","The table in the Power Hungry Processing study lists the training energy for BLOOMz‚Äë7B as 51,686‚ÄØkWh and the fine‚Äëtuning energy as 7,571‚ÄØkWh. Adding these two values gives the combined energy cost of 59,257‚ÄØkWh."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","592,570,000",592570000,is_blank,dodge2022;luccioni2024,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2311.16863,"From the Dodge‚ÄØ2022 study, a full training run of the 6‚ÄØB‚Äëparameter transformer would consume approximately 103,500‚ÄØkWh.  In the Luccioni‚ÄØ2024 table, the BLOOMz‚Äë7B model has a training energy of 51,686‚ÄØkWh and an inference energy of 1.0‚ÄØ√ó‚ÄØ10‚Åª‚Å¥‚ÄØkWh, yielding a cost‚Äëparity of 592,570,000 inferences.","The full training energy (103,500‚ÄØkWh) divided by the per‚Äëinference energy (1.0‚ÄØ√ó‚ÄØ10‚Åª‚Å¥‚ÄØkWh) gives 103,500‚ÄØkWh / 0.0001‚ÄØkWh ‚âà 592,570,000 inferences, matching the training energy cost."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages mention a dataset name for German nuclear waste site objection texts, so the answer cannot be determined from the given documents."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,percent,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.""","The context explicitly states that 84% of token usage on OpenRouter in May 2025 was through models that did not disclose environmental impact, which directly answers the question."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""most carbon footprint analyses gather the information manually by writing to authors.""","The provided passage explicitly states that most analyses collect data manually by contacting authors, contradicting the claim that they gather information automatically without contacting authors."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""13B 2 64 1 64"" ‚Äì Table II lists the bare minimum hardware required, showing 1 NVIDIA A100 80GB GPU for LLaMA‚Äë13B.","Table II explicitly states that the bare minimum for running LLaMA‚Äë13B inference without compression or quantization is one A100‚ÄØ80‚ÄØGB GPU, as indicated by the entry ""13B 2 64 1 64""."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific figure for the total carbon emissions avoided by pruning and quantizing large language models in 2023, nor do they provide a numeric value that can be used to compute a total avoided emission. Therefore, a confident answer cannot be derived from the available context."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any explicit information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. Therefore, a confident answer cannot be derived from the documents."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,3 passengers,3,passengers,patterson2021,https://arxiv.org/pdf/2104.10350,"Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The text states the NAS emissions are equivalent to about 3 passenger round trips, and a single passenger round trip is ~1.2t CO2e, so 3.2t/1.2t ‚âà 3."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,"""This is equivalent to approximately 44% of the data centers‚Äô total electricity cost.""","The context explicitly states that in 2023 the public health cost of U.S. data centers was about 44% of their total electricity cost, using the average attribution method."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,is_blank,is_blank,"""If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).""","The cited passage from Dodge et al. directly states the estimated CO‚ÇÇ emissions range for a full training run of the 6‚Äëbillion‚Äëparameter transformer model, giving 21‚Äì78 metric tons. This range is taken as the answer to the question."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages contain a specific numeric value for freshwater consumption by Meta‚Äôs Llama‚ÄØ3 inference serving clusters in 2024. The documents provide water usage figures for Meta‚Äôs overall data centers and other models, but no explicit figure for Llama‚ÄØ3 inference clusters. Therefore the answer cannot be determined with confidence from the given information."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only."",",The quoted sentence directly states that the hybrid configuration of 2‚ÄØA100s and 1‚ÄØA10G yields a 24% reduction in cost compared with using only A100 GPUs.
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit statement about the hourly price of an NVIDIA H20, nor does it reference a source named Chen et al. (2025) that would provide such a figure. Therefore, the answer cannot be confidently derived from the documents supplied."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the JetMoE project‚Äôs training budget or total GPU hours, so the cost per H100 GPU-hour cannot be determined from these sources."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization""","The cited passage from the paper explicitly states that after quantization, carbon emissions can be reduced by up to 45%, confirming the truth of the statement."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95,95,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""reached out to over 500 authors ... and were only able to collect 95 answers""","The passage explicitly states that after contacting more than 500 authors, the researchers obtained 95 answers, which directly answers the question."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",122%,122,%,han2024,https://arxiv.org/pdf/2412.06288,"Table¬†2: ""Altoona, IA¬†‚Ä¶¬†Health Cost¬†(2.51)¬†‚Ä¶¬†% of Electricity Cost¬†122%"",""Altoona, IA¬†6.91¬†2.1¬†2.51(1.84, 3.17)¬†122%¬†‚Ä¶""","The table lists the health cost as 2.51‚ÄØmillion‚ÄØ$ and the electricity cost as 2.1‚ÄØmillion‚ÄØ$, giving a health‚Äëto‚Äëelectricity ratio of 122‚ÄØ%. This matches the % column for Altoona, IA."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,United Kingdom 36 901,"The table of Amazon Renewable Energy Projects lists the United Kingdom with 36 projects announced as of January 2024, indicating the number of UK projects."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,li2025b,https://arxiv.org/pdf/2304.03271,Apple reports that its supply chain accounts for 99% of its total water footprint [23].,"The context explicitly states that Apple‚Äôs supply chain accounts for 99% of the company‚Äôs total water footprint, providing the required percentage."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""we introduce the granularity metric, the ratio of calculation to communication time""",The passage explicitly states that the authors introduced the granularity metric to measure the ratio of calculation to communication time for assessing scalability in geo‚Äëdistributed training.
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].""","The quoted passage explicitly defines ""water withdrawal"" as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses, matching the question."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",44.1%,44.1,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon Workforce (All Levels) 43.1%56.8% 44.1%55.7%""","The table in the Amazon 2023 Sustainability Report lists the percentage of men in the U.S. workforce across all levels as 44.1%, supporting the answer."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25T tokens,1250000000000.0,tokens,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The quoted statement explicitly states that JetMoE-8B was pre‚Äëtrained on 1.25 trillion (1.25T) tokens, which directly answers the question."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Table II lists for LLaMA‚Äë7B on A100‚Äë80GB the bare minimum count as 1 GPU (Count¬†1¬†64) and the text states the 7B model was run on a single GPU.,"The table provides the bare‚Äëminimum hardware requirement; for 7B on an 80‚ÄØGB A100 the required GPU count is 1, confirming that only one such GPU is needed for inference without compression or quantization."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,About 6 Meena runs would use the same total energy as one full GPT‚Äë3 training run.,6,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""Google Flights estimate for the emissions of a direct round trip of a whole passenger jet between San Francisco and New York is 180‚ÄØtCO‚ÇÇe‚Ä¶T5 training emissions are ~26%, Meena is 53%, ‚Ä¶, and GPT‚Äë3 is ~305% of such a round trip."", ""‚Ä¶the round trip emissions of 180‚ÄØtCO‚ÇÇe‚Ä¶Meena is 53% of that, GPT‚Äë3 is 305% of that."", ""Thus GPT‚Äë3 emits ~549‚ÄØtCO‚ÇÇe and Meena ~95‚ÄØtCO‚ÇÇe, a ratio of roughly 5.8.""","The paragraph gives the emissions of a round‚Äëtrip flight (180‚ÄØtCO‚ÇÇe), the percent of that flight‚Äôs emissions for GPT‚Äë3 (305%) and for Meena (53%). Multiplying 180‚ÄØtCO‚ÇÇe by 3.05 gives ~549‚ÄØtCO‚ÇÇe for GPT‚Äë3, and by 0.53 gives ~95‚ÄØtCO‚ÇÇe for Meena. The ratio 549/95 ‚âà‚ÄØ5.8 means about six Meena training runs consume the same energy as one GPT‚Äë3 run."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"""the Standing Committee that oversees the One Hundred Y ear Study forms a Study Panel every five years to assess the current state of AI.""","The quoted sentence from the report states that the Standing Committee convenes a Study Panel every five years, providing the exact frequency requested."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]""","The passage from wu2021b explicitly states that the average U.S. household had 25 connected devices in 2021, providing the required numeric value and unit."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"IBM‚Äôs Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context explicitly states that Watson beat human contestants in Jeopardy, contradicting the claim that it did not. Therefore the statement is false."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8‚Äì3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)""","The quoted sentence from Luccioni 2025c explicitly states that publicly available data show pre‚Äëtraining energy consumption ranging from 0.8‚ÄØMWh to 3,500‚ÄØMWh, which directly provides the requested range."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",2.0 seconds,2.0,seconds,xia2024,https://arxiv.org/pdf/2408.04693,"""Sparse(bsz=84)"" appears in the execution time breakdown chart with a value of 2.0 seconds.","The execution time chart for sparse BlackMamba on the NVIDIA A40‚Äë48GB lists the total execution time for a batch size of 84 as 2.0 seconds, which is the requested total time."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a;luccioni2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2504.00797,"""could add up to 640 percent more carbon emissions compared to the company‚Äôs carbon removal targets for the year"" (Doc luccioni2025a)  
""could enable carbon emissions adding up to 640 percent of the company‚Äôs carbon removal targets"" (Doc luccioni2025b)","Both documents state that the deal could increase emissions by 640‚ÄØpercent relative to Microsoft‚Äôs yearly carbon removal targets, which equates to 6.4 times that target."
q168,The 2024 Griggs et al. paper reports that M√©lange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Compared to using only a single GPU type, M√©lange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.""","The Griggs et al. paper states that in conversational (chat) settings M√©lange can reduce deployment costs by up to 77%, which is directly cited in the supporting quote."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650","The MT‚ÄëBench table lists JetMoE‚Äë8B‚ÄëChat at 6.681, which is higher than Llama‚Äë2‚Äë13b‚ÄëChat‚Äôs 6.650, indicating that JetMoE‚Äë8B‚ÄëChat surpassed Llama‚Äë2‚Äë13b‚ÄëChat after alignment."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.""",The quoted passage explicitly states that the bare minimum configuration for LLaMA‚Äë65B inference without compression or quantization requires 4 NVIDIA A100 80GB GPUs.
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80‚Äì90%,"[80, 90]",%,is_blank,is_blank,"""NVIDIA estimated that 80‚Äì90% of the ML workload is inference processing [Leo19].""","The passage from Patterson‚ÄØ2021 explicitly states that NVIDIA estimated 80‚Äì90% of the machine learning workload consists of inference processing, which directly answers the question."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10 to 50 completions,"[10, 50]",completions,li2025b,https://arxiv.org/pdf/2304.03271,"""Additionally, GPT-3 needs to ‚Äúdrink‚Äù (i.e., consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed."",",The context states that one 500‚ÄëmL bottle of water equals the water needed for 10‚Äì50 medium‚Äëlength GPT‚Äë3 completions. Thus the number of completions that can be produced with that water is in the range 10 to 50.
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg CO2eq,178.97,kg,is_blank,is_blank,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of ùê∂ùëÇ2ùëíùëû.""","The study‚Äôs ethics statement reports that the entire set of experiments emitted 178.97‚ÄØkg of CO‚ÇÇ‚Äëequivalent, which is the total for the 2024 Power Hungry Processing study."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the ground truth throughput of dense Mixtral-CS-A100-40GB at batch size 1, so the answer cannot be determined with confidence."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10‚Äì50 queries,"[10, 50]",queries,li2025b,https://arxiv.org/pdf/2304.03271,is_blank,"Both documents report that between ten and fifty GPT‚Äë3 queries use about 0.5‚ÄØL (half a liter) of water, establishing the range 10‚Äì50 queries as the estimate."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,FALSE,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4o consumes around 2.875 Wh while GPT-4o mini‚Äôs consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s."", ""GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes."", ""GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries...""","The quoted passages show that GPT-4o mini‚Äôs per-query energy consumption is higher (3.098‚ÄØWh or 20% more) than GPT-4o‚Äôs (2.875‚ÄØWh), proving the statement is false."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,is_blank,is_blank,"""Estimates using TDP are nearly always an overestimation since it is rare for a GPU ‚Äì or any computing device ‚Äì to draw its maximum power at every moment in time."" (Doc [chung2025#0024])","The quoted passage indicates that estimating GPU energy consumption solely by its TDP leads to an overestimation and is not a reliable or accurate method, supporting the answer ""False."""
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B, trained with a limited $100k budget, using 1.25T tokens from mixed open‚Äësource datasets and 30,000 H100 GPU hours.","The JetMoE‚Äë8B pre‚Äëtraining section explicitly states that 30,000 H100 GPU hours were consumed during training, which gives the numeric value for the answer."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000",10000,round trips,han2024,https://arxiv.org/pdf/2412.06288,"""training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.""","The cited passage explicitly states that training a Llama‚Äë3.1‚Äëscale model generates air pollutants equivalent to over 10,000 LA‚ÄëNYC round trips, providing the numeric estimate required."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","1,000√ó larger",1000,times,wu2021a,https://arxiv.org/pdf/2111.00364,"""with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000√ó larger in size.""","The document explicitly states that boosting BLEU from 5 to 40 for GPT‚Äë3 translation demands a model 1,000‚Äëfold larger, so the required increase is 1,000√ó."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific figure for the amount of water used for cooling during OpenAI‚Äôs GPT‚Äë4 training run, so a confident answer cannot be derived from the supplied context."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$7.22 per hour,7.22,$/hr,griggs2024,https://arxiv.org/pdf/2404.14527,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms."", ""30 days/month""","The monthly cost is given as over $5,200 for two A100 GPUs. Dividing $5,200 by 30 days √ó 24 hours yields approximately $7.22 per hour."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.5164 per hour,7.5164,$/h,is_blank,is_blank,"""On-demand Price ($/h) 0.7 1.01 3.67 7.5164"" ‚Äì the table lists the normalized on‚Äëdemand hourly price for the H100 GPU as $7.5164, calculated from RunPod pricing and adjusted to major cloud pricing.","The table in the Griggs et al. (2024) document explicitly lists the normalized on‚Äëdemand hourly price for the H100 GPU as $7.5164 per hour, which matches the calculation described in the text."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The cited passage indicates that after 2022 the trend reversed and direct disclosures decreased, contradicting the claim that they continued to increase."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].,"The passage from the Patterson 2021 document explicitly states that OpenAI reported 3.14√ó10^23 floating point operations for training GPT‚Äë3, which is the required number."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""BLOOMz-7B has been downloaded 606,096 times""  ""Inference energy (kWh) 1.0 √ó 10‚àí4""","The context gives the download count of 606,096 and the energy per inference of 1.0√ó10‚Åª‚Å¥‚ÄØkWh. Each download is assumed to generate 1‚ÄØmillion inferences, so total inferences = 606,096‚ÄØ√ó‚ÄØ1,000,000 = 606,096,000,000. Multiplying by 1.0√ó10‚Åª‚Å¥‚ÄØkWh yields 60,609,600‚ÄØkWh, which is 60,609.6‚ÄØMWh."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emissions for training with neural architecture search (626,155 lbs) but do not contain a numerical emissions‚Äëto‚Äëdriving‚Äëdistance ratio or information on car emissions per mile needed to compute an equivalent driving distance. Therefore the requested distance cannot be determined from the supplied context."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",$1 billion,1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"""If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027,"" and ""the amortized cost of frontier training runs will exceed one billion dollars by 2027.""","The cited passages from the Cottier 2024 study state that, assuming current growth rates, the cost of the largest training runs will surpass a billion dollars by 2027. This directly answers the question of the cost threshold expected to be exceeded."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 GPUs,8,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"""at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."", ""the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.""","The documents state that the bare minimum hardware for LLaMA‚Äë65B inference without compression or quantization requires 8 NVIDIA V100 32‚ÄØGB GPUs, confirming that 8 is the required number."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons""",The quoted sentence from Doc¬†[luccioni2025a] explicitly states that Microsoft‚Äôs global water consumption rose by 34% from 2021 to 2022.
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"""FAIR‚Äôs RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.""","The cited passage from Schwartz‚ÄØ2019 explicitly states that training FAIR‚Äôs RoBERTa on 160‚ÄØGB of text required approximately 25,000 GPU hours."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents mention AlexNet but do not include a numeric value for its top-1 accuracy on ImageNet, so the answer cannot be inferred with confidence."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192,192,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 √ó80G) servers.,"The context states that 24 servers were used and each DGX-A800 server contains 8 GPUs, so 24 √ó 8 = 192 GPUs in total."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,is_blank,is_blank,is_blank,"Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes","The document explicitly states that 700‚ÄØmillion daily GPT‚Äë4o queries produce annual electricity use equivalent to 35,000 U.S. homes, which directly answers the question."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit information about the energy consumption of the Llama‚ÄØ3.1‚ÄØ70B model when deployed on two nodes versus one node, nor does it provide a numeric factor for comparison. Therefore, I cannot answer the question with confidence based solely on the supplied documents."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,This implementation of the model uses Pytorch and the FairScale library to enable model sharding across multiple GPUs and nodes.,"The supporting passage states that FairScale was used to shard the LLaMA model across multiple GPUs and nodes, making it the framework used for deployment in that study."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain estimates of annual water consumption for GPT-4o and general data center water usage, but they do not provide a specific figure for gallons of water consumed per ChatGPT user session in 2023. Therefore, the answer cannot be derived from the given information."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents mention Yelp sentiment analysis benchmarks or provide a comparison of traditional models versus large language models in that context, so the statement cannot be evaluated with confidence."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0,"The table lists the OpenLLM Leaderboard average score for each model; the JetMoE‚Äë8B row shows the value 53.0, which is the final average score requested."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion GPT-4o queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"""yielding a total of approximately 772 billion GPT-4o queries in 2025""","The Jegham 2025 analysis explicitly states that the projected total number of GPT-4o queries for 2025 is about 772‚ÄØbillion, which directly answers the question."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11""","The provided context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter during the Evolved Transformer run was 1.11, directly answering the question."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons of CO2e",47400,metric tons of CO2e,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"These on-site solar energy systems are estimated to generate 123,000 MWh annually‚Äîenough energy to power over 33,600 European homes‚Äîand avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that Amazon‚Äôs on‚Äësite solar systems generate 123,000‚ÄØMWh annually and avoid roughly 47,400‚ÄØmetric tons of CO2e compared to nonrenewable electricity sources, which directly answers the question."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","626,155 lbs CO2e, equivalent to five average American car lifetimes.",626155,lbs,luccioni2023;luccioni2025c;strubell2019,https://arxiv.org/pdf/2302.08476;https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/1906.02243,"""estimated that the emissions of training and fine‚Äëtuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars."" and ""NAS approach‚Ä¶ could yield 626,155 pounds (284 metric tons) CO2‚Äëequivalent GHG emissions, or about five times the emissions of a car during its lifetime.""","The Strubell et al. study provides the emission figure of 626,155 lbs CO2e for NAS on a Transformer, and the same figure is described as equivalent to five car lifetimes, directly answering the question."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""In 2020, it was 1.59."" (Patterson 2021)","The Patterson 2021 document explicitly states that the US national data center average PUE in 2020 was 1.59, confirming the numeric value. The answer is dimensionless, so no unit is provided."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,kim2025,https://arxiv.org/pdf/2504.11816,"""When the batch size increases to 32, the KV Cache expands to 5.312GB""","The text explicitly states that for the OPT‚Äë2.7B model on an AWS g4dn.xlarge instance, the KV Cache size at batch size 32 is 5.312GB."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value or statement indicating how many AI training runs were conducted globally on renewable-only power in 2022. Therefore, a confident answer cannot be derived from these documents."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The cited passage explicitly states that for the four notable models (GPT‚Äë3, OPT‚Äë175B, GPT‚Äë4, Gemini Ultra) R&D staff costs with equity comprise 29% to 49% of the total amortized cost, which is the requested percentage range."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,TRUE,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Increasing the number of shards still tends to increase the energy costs of inference per response most overall,"The cited passage explicitly states that as GPU shards increase, the energy cost per response for LLaMA‚Äë65B also increases, confirming the true statement."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),CTCF,is_blank,is_blank,is_blank,"""Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.""",The quoted sentence from doc kim2025#0000 explicitly names the Compute Time Calibration Function (CTCF) as the function proposed to adjust for theoretical‚Äëactual GPU performance discrepancies to improve instance selection accuracy.
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search""","The context states that 53% of the 100 analyzed news articles used the contested 3‚ÄØWh/10√ó‚ÄëGoogle‚Äësearch estimate, which directly answers the question."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",4.49 zettaFLOPs,4.49,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,"""The 16B model with 245.37B tokens, the 51B model with 39.64B tokens, and the 101B model with 26.54B tokens.""; ""The total cost of FLM-101B is computed as 52.76 zettaFLOPs.""","The total training cost of FLM‚Äë101B is 52.76‚ÄØzettaFLOPs and the 101B stage processes 26.54‚ÄØB tokens out of a total of 311.55‚ÄØB tokens for all stages. Assuming FLOPs scale linearly with tokens, the 101B stage accounts for 26.54/311.55‚ÄØ‚âà‚ÄØ0.085 of the total cost, i.e., 0.085‚ÄØ√ó‚ÄØ52.76‚ÄØ‚âà‚ÄØ4.49‚ÄØzettaFLOPs."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Open‚Äësource (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This does not mandate the disclosure of energy consumption.","The EU AI Act, as interpreted in the cited document, exempts open‚Äësource general‚Äëpurpose AI models from reporting energy consumption to authorities, so the statement that they must report is false."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4]. ... However, it does not mandate the disclosure of energy consumption."", ""Open-source general‚Äëpurpose AI models are largely excluded from transparency requirements unless they present a systemic risk""","The cited passage from ebert2024 states that OS GPAI models are exempt from transparency, including energy‚Äëconsumption reporting, unless they pose a systemic risk, directly supporting the claim that they are fully exempt in that circumstance."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, M√©lange achieved cost reductions in what percentage range compared to single-GPU baselines?",15-77%,"[15, 77]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"""In Figs. 11a and 11d, M√©lange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).""","The document explicitly states that for the short‚Äëcontext Arena dataset at a 120‚ÄØms SLO, M√©lange achieves a cost reduction ranging from 15‚ÄØ% to 77‚ÄØ% compared with single‚ÄëGPU baselines. The quoted sentence directly provides the percentage range, confirming the answer."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26,26,metric tons of CO2e,li2025a,https://arxiv.org/pdf/2309.03852,Table 3: net tCO2e 26 for FLM-101B,"The table in the provided context lists the net carbon emissions for FLM-101B as 26 metric tons of CO2 equivalent, which is the total estimated pre‚Äëtraining emissions."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,~14.8,14.8,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Table 4: Energy consumption (mean ¬± std dev) per model across three prompt sizes (Wh). GPT-4.1 nano 0.827 ¬± 0.094 (10k input-1.5k output). o3 12.222 ¬± 1.082 (10k input-1.5k output).,The long‚Äëprompt energy for o3 (12.222‚ÄØWh) divided by that for GPT‚Äë4.1 nano (0.827‚ÄØWh) yields a factor of approximately 14.8. The table entries directly provide the required values for the calculation.
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a;wu2021b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2108.06738,"""In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide"" (Doc [luccioni2025a])","The cited sentence in Doc [luccioni2025a] states that these four companies accounted for nearly 30% of all corporate PPA purchases in 2020, confirming the percentage."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon package,CodeCarbon,is_blank,is_blank,is_blank,"""we measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking"" (Doc [morrison2025#0019])
""assess their impact on the hardware [...] using the CodeCarbon tool"" (Doc [rubei2025#0002])
""used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference"" (Doc [luccioni2024#0015])","All three cited documents explicitly state that the CodeCarbon package/tool was used to measure energy consumption during inference runs, confirming it as the software package employed."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"""the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.""","The passage explicitly states that the total public health cost of U.S. data centers in 2023 was about $6.7‚ÄØbillion, which is the value requested."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",$11.06 per hour,11.06,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr,"Table¬†1 in Chen et al. lists the H100 price per chip as $11.06 per hour, which directly answers the question."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Backblaze B2,Backblaze B2,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4].""",The passage explicitly states that Backblaze B2 was used as the independent S3 storage provider to shard and stream datasets for spot VMs that can terminate at any time.
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Senator Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""","The context explicitly states that Senator Edward J. Markey introduced the AI Environmental Impacts Act bill on 1 February 2024, identifying him as the bill‚Äôs introducer."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.0022 kL,0.0022,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"""Mining 1 kg of rare earth materials consumes about 11 kL of water ... Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU."", ""0.1% rare earth metal by mass""","The document states that mining rare earth materials for an H100 GPU, which contains 0.1% rare earth metal by mass, results in an additional 2.2 liters of water consumed. Converting 2.2 liters to kiloliters gives 0.0022 kL, which is the estimated water consumption per GPU."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"""We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.""","The cited sentence states that the 4‚Äëbit quantization was performed via Ollama, an open‚Äësource tool that enables local inference and privacy‚Äëcentric deployment, directly answering the question."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 NVIDIA V100 32GB GPUs,2,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Table II lists the bare minimum hardware required: ""13B 2 64"" indicating 2 V100 32GB GPUs are needed.  The text also states, ""The 13B model was run on two GPUs"".",The table and accompanying description explicitly state that LLaMA‚Äë13B requires two V100 32GB GPUs for inference without any compression or quantization. This is the bare minimum hardware specified in the document.
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,is_blank,is_blank,"""the health costs are unevenly distributed across counties and communities, particularly affecting low-income counties that could experience approximately 200x per-household health costs than others.""","The quoted passage explicitly states that public health costs are unevenly distributed, disproving the claim that they are evenly distributed across U.S. communities."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, which is about four times the five‚Äëcars estimate",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the ‚Äúfive cars‚Äù number""","The quoted sentence states the Gemma pre‚Äëtraining emissions as 1247.61‚ÄØtCO2e and explicitly notes it is over four times the baseline five‚Äëcars estimate, providing both the absolute figure and the comparative multiplier."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Figure 2: As a result of Moore‚Äôs law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The cited figure explicitly states that GPU theoretical performance per watt doubles every 3-4 years, which directly supports the truth of the statement."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,5 years,5,years,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""although GPUs can theoretically last about five years""","The document explicitly states that GPUs can theoretically last about five years, which is the estimated average GPU lifetime before retirement in AI data centers in 2024."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,"""fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.""","The context explicitly states that fine‚Äëtuning a sparse Mixtral model on 2‚ÄØmillion queries with an NVIDIA H100 GPU costs $3460, which is the net cost requested."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50‚Äì70%,"[50, 70]",%,is_blank,is_blank,"""GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50‚Äì70% of the total provisioned power in the datacenter [52‚Äì54, 58].""","The quoted sentence from Chung et al. states that GPUs account for 50‚Äì70% of the total provisioned power in a typical datacenter, directly answering the question."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied passages explicitly state that the relationship between runtime and energy consumption was nearly linear, so the answer cannot be determined from the given documents."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""AWS can lower its customers‚Äô workload carbon footprints by up to 96% compared to on‚Äëpremises computing workloads in North America when the electricity AWS uses is matched with 100% renewable energy.""","The quoted passage explicitly states that moving workloads to AWS in North America can reduce customers‚Äô carbon footprints by up to 96%, which is the figure requested."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96,96,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,We conduct training on a cluster containing 12 nodes and 96 H100s.,"The context explicitly states that the training was performed on a cluster comprising 12 nodes with a total of 96 H100 GPUs, yielding the count of 96 GPUs used for training."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context states the U.S. national average water consumption for electricity generation is 3.1‚ÄØL per kWh, directly answering the question."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,336 hours,336,hours,strubell2019,https://arxiv.org/pdf/1906.02243,"""Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).""","The document states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for a duration of 2 weeks, which equals 336 hours. This directly answers the question about training time on 3 GTX 1080 Ti GPUs."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,"""Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021"" (Doc [dodge2022]) and ""PUE of hyperscalar datacenters, such as Google‚Äôs, has improved from 1.21 (2008) to 1.10 (2021)"" (Doc [wu2021b])","Both documents report a PUE of 1.10 for Google‚Äôs hyperscale data centers in 2021, confirming the value."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""A single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh)""","The context states that a single short GPT‚Äë4o query consumes 0.42‚ÄØWh, which is the requested energy consumption."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"""When actively training, the average GPU power is over 600W, over 85% of an H100‚Äôs maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.""",The cited figure description states that during active training the average GPU power exceeds 600‚ÄØW. Therefore the average GPU power for a single node while actively training is over 600‚ÄØW.
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts contain a figure for the average global CO2e emissions per human life per year, so the answer cannot be determined from the given documents."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""AI‚Äôs expanding operational footprint also contributes to electronic waste (e‚Äëwaste), which is now the fastest‚Äëgrowing segment of solid waste worldwide, reaching 62 million tonnes in 2022.""","The 2025 paper states the global e‚Äëwaste generation was 62¬†million tonnes in 2022, which directly answers the question."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,is_blank,is_blank,"""training the GPT-3 language model in Microsoft‚Äôs state‚Äëof‚Äëthe‚Äëart U.S. data centers can directly evaporate 700,000 liters of clean freshwater,""","The abstract of Li‚ÄØet‚ÄØal. explicitly states that GPT‚Äë3 training in Microsoft‚Äôs U.S. data centers can directly evaporate 700,000‚ÄØL of clean freshwater, which directly answers the question."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",True,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Reporting the computational price tag of Ô¨Ånding, training, and running models is a key Green AI practice""","The provided document explicitly states that Green AI includes reporting the financial cost (price tag) of developing, training, and running models, confirming that the statement is true."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20√ó,20,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Facebook‚Äôs recommendation and ranking model sizes have increased by 20 times during the same time period [11].""","The quoted sentence directly states that between 2019 and 2021, Facebook‚Äôs recommendation and ranking model sizes grew 20‚Äëfold, which is the numeric answer requested."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""current averages of less than 3 years for cell phones [Cordella et al., 2020]""","The provided context states that smartphones have current average lifetimes of less than 3 years, which aligns with the claim and indicates a contribution to e‚Äëwaste concerns."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,Approximately 1.25√ó speedup,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The passage states that for the 13B model the inference throughput on A100 GPUs is about 1.25 times that of V100 GPUs, indicating a 1.25√ó speedup."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""o3-mini achieved the highest cross-efficiency score (0.884)""","The passage explicitly states that o3-mini had the highest cross-efficiency score of 0.884, indicating it ranked highest in the eco-efficiency DEA analysis."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.""","The passage explicitly states that the FLM‚Äë101B model‚Äôs context window size is 2,048 tokens, which is the requested value."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,TRUE,1,is_blank,is_blank,is_blank,"""CV‚Äôs per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)""","The cited passage reports that for CV models the per-GPU speedup with T4 GPUs is almost linear, confirming the statement."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,patterson2021,https://arxiv.org/pdf/2104.10350,"Processor   Average (Watts)  StDev   %  DNNs   used   to   calculate   average   power
TPU   v2   221   5%   Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19]
V100   GPU   325   2%   Transformer (Big), GPT-3 [Sut21]",The provided table lists the average system power per processor: 221‚ÄØW for TPU‚ÄØv2 and 325‚ÄØW for V100 GPU. Subtracting 221‚ÄØW from 325‚ÄØW gives a difference of 104‚ÄØW.
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million packages,150,million packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We delivered 150 million packages via EVs.""","The document states that in Europe Amazon delivered 150 million packages via electric vehicles in 2023, which directly answers the question."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;""","The quoted passage states that for DenseNet 201 in the West US region, the Flexible Start optimization can achieve up to an 80% reduction in CO‚ÇÇ emissions."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 ‚Äì 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,"""A recent study suggests that the global AI could consume 85 ‚Äì 134 TWh of electricity in 2027""","The provided passage explicitly states that a recent study projects global AI electricity consumption to be between 85 and 134 terawatt‚Äëhours in 2027, which directly answers the question."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,is_blank,is_blank,"""Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. ‚Ä¶ The workload evaluates a total of 3000 requests."", ""Online inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. The workload evaluates a total of 3000 requests."",","Each request processes 128 input + 512 output = 640 tokens. With 3000 requests, the total tokens processed are 3000 √ó 640 = 1,920,000 tokens."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61‚Äì76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61‚Äì76% and 2‚Äì7% respectively.""","The cited passage from Cottier et al. (2025) states that, when equity is excluded, computing hardware accounts for 61‚Äì76% of the total amortized cost for the four key models. This directly provides the requested percentage range."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,RTX A6000,RTX A6000,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Based on the Nvidia datasheet for the RTX A6000 GPU, we utilize consider FLOPS HW of 309.7 TFLOPS and a 300W TDP power draw; and estimate theoretical inference FLOPs with the DeepSpeed profiler.","The context specifies that the RTX A6000 provides 309.7‚ÄØTFLOPS with a 300W TDP, indicating a high performance‚Äëper‚Äëwatt ratio. No other GPU architecture is presented for single‚Äëtoken masked language modeling, so the RTX A6000 is the most energy‚Äëefficient architecture described."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,CO2e = 0.954pt,"The Strubell2019 context states that the EPA‚Äôs average CO‚ÇÇ produced for U.S. power consumption is 0.954 lbs per kWh, which directly answers the question."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration. The Act also neglects to address indirect emissions from AI applications (e.g., those used for oil and gas exploration).","The cited passages from ebert2024 explicitly state that the AI Act does not require providers to disclose GHG emissions of AI applications, such as those used in oil and gas exploration, thereby making the statement false."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,TRUE,1,is_blank,is_blank,is_blank,"""LLM decoding, the dominant operation for LLM serving, is memory‚Äëintensive and does not fully utilize the GPU‚Äôs compute resources... Diffusion models, on the other hand, consume nearly the maximum power of the GPU... LLM decoding is characterized by low compute‚Äëintensity, meaning that the number of arithmetic operations per byte of memory loaded is low ... This leads to the GPU‚Äôs computation throughput being bottlenecked by VRAM bandwidth and results in the GPU‚Äôs computation units being underutilized, leading to low power draw."", ""LLM‚Äôs power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model‚Äôs power consumption is close to the maximum."", ""LLM decoding is less compute‚Äëintensive and bottlenecked by VRAM bandwidth."", ""Diffusion models consume nearly the maximum power of the GPU when batch size is not small.""","The cited passages explicitly state that LLM decoding uses far less compute power, is memory‚Äëbound, and therefore draws lower power than diffusion models, which approach the GPU‚Äôs maximum power. These statements directly support the claim that LLMs generally have lower power draw during inference because their decoding is less compute‚Äëintensive and bottlenecked by VRAM bandwidth."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about Yelp sentiment analysis benchmarks or a comparison between traditional models and large language models, so a confident answer cannot be derived."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450,1450,is_blank,is_blank,is_blank,"""text classification 0.002 0.001 ... image generation 2.907 3.31"" from Table 2, and the text stating the factor is over 1450.","The table gives mean energy per 1,000 queries: 0.002‚ÄØkWh for text classification and 2.907‚ÄØkWh for image generation. Dividing 2.907 by 0.002 yields ‚âà1453, which the text reports as a factor of over 1450, so the energy required for image generation exceeds that of text classification by roughly 1450 times."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any information about the number of kilometers of fiber optic cable installed globally for AI workloads in 2023, so a confident answer cannot be derived."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Amazon‚Äôs AWS only covered fifty percent of its power usage with renewable energy.""","Both provided passages state that AWS covered 50% of its power usage with renewable energy, which is the figure asked for in 2018."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,"Table¬†1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU‚Ä¶ The GPU alone accounts for 74% of the total energy consumption due to these components.","The table shows the GPU‚Äôs share of power draw as 74% of the total, directly answering the question."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2,2,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"""For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs""","The provided passage explicitly states that serving Llama2-70b in BF16 precision needs two NVIDIA A100-80GB GPUs, which directly answers the question."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244,244,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"United States 244 17,706","The table of Amazon Renewable Energy Projects lists 244 projects in the United States as of January 2024, which is the direct figure provided in the document."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",5.0 seconds,5.0,seconds,xia2024,https://arxiv.org/pdf/2408.04693,"Forward time for sparse Mixtral (batch size‚ÄØ1) is 4.0‚ÄØs and backward time is 1.0‚ÄØs (Table‚ÄØIII), optimizer negligible.","Table‚ÄØIII lists the execution time breakdown per stage. For sparse Mixtral with batch‚ÄØ1, forward =‚ÄØ4.0‚ÄØs, backward =‚ÄØ1.0‚ÄØs, and the optimizer stage is negligible, giving a total ‚âà‚ÄØ5.0‚ÄØs."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Energy reporting at the cumulative server level: Energy consumption should be reported at the cumulative server level (see also [4]).""","The excerpt explicitly states that energy consumption should be reported at the cumulative server level, which balances accuracy (captures total computation-related power usage) and feasibility (avoids overly complex or uncertain measurement levels)."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800M,800000000,USD,is_blank,is_blank,"""For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.""","The context explicitly states that the estimated upfront hardware acquisition cost for GPT‚Äë4 is $800‚ÄØmillion. The quote directly provides the figure, which is then expressed in the answer."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,False,0,is_blank,is_blank,is_blank,"""The umbrella term ‚ÄòSustainable AI‚Äô was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.""",The quoted passage shows that Sustainable AI was defined to include not only climate‚Äëpositive uses of AI but also the improvement of AI‚Äôs own environmental sustainability. Therefore the claim that it was proposed to only encompass climate‚Äëpositive applications is false.
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about McKinsey projections for data center electricity consumption in 2030.
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",25%,25,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""doubling the duration can lead to significant savings up to about 25%""","The context indicates that for the 6‚Äëbillion‚Äëparameter transformer, Pause and Resume with a doubled duration yields a maximum emissions reduction of approximately 25%, which is the value reported in the cited figure."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,"""reducing inference computation by about 70% compared to Llama2-7B.""","The JetMoE‚Äë8B architecture is reported to reduce inference computation by roughly 70% relative to the Llama2‚Äë7B model, as stated in the cited document."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google reports a 48% increase in GHG emissions since 2019""","The quoted sentence from Google‚Äôs 2024 environmental sustainability report explicitly states that GHG emissions increased by 48% since 2019, providing the required percentage increase."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5%,28.5,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).""","The quoted passage directly states that Facebook achieved a 28.5% reduction in operational energy footprint over the 2019‚Äë2021 two‚Äëyear period through iterative hardware‚Äësoftware optimization, which is the figure requested."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,is_blank,is_blank,"""GPT-3 was trained and deployed by OpenAI in Microsoft‚Äôs data centers, with an estimated training energy of 1287 MWh [29].""","The context explicitly states that the full GPT‚Äë3 model‚Äôs training consumed 1287‚ÄØMWh, which is the requested energy estimate."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping consumes less energy than Recomputation when the server is overloaded.,Swapping,is_blank,chung2025,https://arxiv.org/pdf/2505.06371,"""Swapping consistently consumes less energy."" (from Figure 8 description in Doc [chung2025#0079] and [chung2025]), ""This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.""","The cited passage directly compares the two mechanisms under overload and states that swapping consistently uses less energy, explaining the reason based on computation vs memory operations."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs,luccioni2025b;luccioni2023,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/2302.08476,"""quantified the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192]."" (Doc luccioni2025b)
""284,019 kg (626,155 lbs) of CO2e"" (Doc luccioni2023)","Both cited passages state that Strubell et al. estimated the training of BERT produced 626,155 lbs of CO‚ÇÇe, which is the requested carbon footprint."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.""",The quoted passage from the document explicitly states that the peak of direct environmental information disclosure occurred in 2022.
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g,0.32,g,is_blank,is_blank,"""for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of ùê∂ùëÇ2ùëíùëûper 1,000 queries""","The 2024 study explicitly states that bert-base-multilingual-uncased-sentiment emits 0.32 grams of CO‚ÇÇeq for every 1,000 text classification queries, which is the value requested."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a numeric value for the hectares of land occupied by new AI data centers in 2022, so the answer cannot be determined from the documents."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",26.1%,26.1,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""16.6% 31.9% 26.1% 23.5%"" (within the People Managers table in the 2023 Amazon Sustainability Report AppendixPeople section)","The table for People Managers shows the 2023 percentage of women as 26.1%, indicating that 26.1% of Amazon‚Äôs People Managers globally identified as women in 2023."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",TRUE,1,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""CV task is still at a granularity of 3.33 on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.""","The cited excerpts explicitly state that for high‚Äëgranularity CV models, intercontinental (C‚Äë8) training is only 7% slower than the fully local (A‚Äë8) run, confirming the statement as true."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,TABLE III COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLM S BEFORE AND AFTER OPTIMIZATION .  Llama 3.2 0.55 0.45 0.44 0.45 0.012  ...  After Optimization  Llama 3.2 0.57 0.48 0.47 0.48 0.005\nPhi 3.2 0.97 0.82 0.88 0.82 0.012  ...  After Optimization  Phi 3.2 1.00 0.84 0.91 0.84 0.007\nQwen 0.77 0.79 0.76 0.79 0.009  ...  After Optimization  Qwen 0.80 0.81 0.80 0.81 0.004\nMistral-small 0.70 0.67 0.65 0.67 0.020  ...  After Optimization  Mistral-small 0.73 0.70 0.69 0.70 0.015\nLlava-Llama 3 0.58 0.50 0.48 0.50 0.014  ...  After Optimization  Llava-Llama 3 0.61 0.54 0.51 0.54 0.006,"The table shows that for each of the five LLMs the accuracy (e.g., 0.45‚Üí0.48 for Llama‚ÄØ3.2) and the F1 score (e.g., 0.44‚Üí0.47 for Llama‚ÄØ3.2) increased after optimization. Since this improvement is observed for all models, the statement that accuracy and F1 scores always improved in the financial sentiment case study is supported."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the freshwater consumption of Google's DeepMind AlphaFold servers specifically in 2023, only overall data center consumption figures."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",5 days,5,days,morrison2025,https://arxiv.org/pdf/2503.05804,"""OLMo 60M‚Ä† 1.2 0.4 1 month 1.6 5 days""",The table in the OLMo paper lists the equivalent water usage for each model. For the 60M‚Äëparameter model the entry shows 5 days of water usage for one U.S. person.
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,$32.7,32.7,dollars,xia2024,https://arxiv.org/pdf/2408.04693,Table IV shows that for an NVIDIA A40 (48GB) GPU fine‚Äëtuning a sparse Mixtral on GSM8K costs $32.7 in total.,"The table lists the total cost for fine‚Äëtuning Mixtral on GSM8K with a sparse MoE setup on an A40 GPU as $32.7, which directly answers the question."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,False,0,is_blank,is_blank,is_blank,"""A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.""","The quoted sentence from the paper states that adding compute resources is a strategy to reduce cost, implying that it does not increase costs. Therefore, the claim that it can increase costs is false."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,is_blank,is_blank,"""MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning."" (Doc xia2024#0021)","The quoted passage explicitly states that the MoE layer is a prime target for optimization, confirming the claim that it is often targeted when improving LLM fine‚Äëtuning performance."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MWh,40,MWh,li2025a,https://arxiv.org/pdf/2309.03852,Energy (MkWh) 40,"The carbon footprint table for FLM-101B lists its energy consumption as 40‚ÄØMkWh, which is equivalent to 40‚ÄØMWh of total energy used during training."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",More than $20 billion,20,billion$,han2024,https://arxiv.org/pdf/2412.06288,"""the growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028, rivaling that of on-road emissions of California.""","The quoted sentence from the recent study states that the projected public health burden in 2028 could exceed $20‚ÄØbillion, satisfying the ‚Äòmore than‚Äô condition in the question."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.""","The cited passage explicitly states that GPU-level power monitoring is not recommended for overall AI energy reporting, contradicting the claim that it is the preferred method."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""The 7B model was run on a single GPU"" and Table II lists a bare minimum of 1 V100 32GB GPU for the LLaMA‚Äë7B model.",Both the narrative and Table II explicitly state that the LLaMA‚Äë7B requires only one V100‚Äë32GB GPU when no compression or quantization is applied. Thus the bare minimum number of such GPUs is 1.
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""training accounted for only half of the model‚Äôs overall emissions""","The quoted sentence from the 2023 article states that training comprised only half of BLOOM‚Äôs total emissions, which equals 50%."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.30 metric tons,8.3,metric tons,is_blank,is_blank,"""one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)""","The context directly states that an average U.S. home emits 8.30 metric tons of CO‚ÇÇ per year, providing the required estimate."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Table 3 shows GSM8k scores of 14.5 (LLaMA2), 17.3 (DeepseekMoE), 16.9 (Gemma) and 27.8 (JetMoE).","The table lists GSM8k performance for each model; the JetMoE column lists 27.8, which is the score for JetMoE-8B."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",16.7 requests,16.7,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,Table 1 row for Arizona: ‚Ä¶ 23.406 29.926 16.7,"The table lists the number of requests required to consume a 500‚ÄØml bottle of water for each location. For Arizona the value is 16.7, meaning roughly 16.7 user requests would consume a 500‚ÄØml bottle during GPT‚Äë3 training."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",32,32,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Table III shows that for the sparse Mixtral model (Mixtral-S) on the CS dataset the maximum batch size supported on an NVIDIA A40-48GB is ""Sparse(bsz=32)"".","The longest-running MoE layer corresponds to the maximum supported batch size for the sparse Mixtral model on an A40-48GB GPU, which Table‚ÄØIII indicates is 32 samples."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",Approximately 0.094 thousand grams per GPU hour,0.094,thousand grams,is_blank,is_blank,"""We find CO2 (grams) per GPU hour in the most efficient region to average 34, and in the least efficient region to average 128""","The passage reports 34‚ÄØg per GPU hour for the most efficient region and 128‚ÄØg for the least efficient region. The difference is 94‚ÄØg, which equals 0.094 thousand grams."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,4,4,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""A40 48GB 4 1.01 0.79 32.7""","The analytical model table shows the projected maximum batch size for Mixtral fine‚Äëtuning on an NVIDIA A40 (48‚ÄØGB) as 4 samples, which is the maximum batch size supported in the dense setup on the Hellaswag dataset."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a explicit numeric value for the total execution time of a sparse Mixtral model fine‚Äëtuned on an NVIDIA A40‚Äë48GB with batch size 10, so the answer cannot be determined confidently from the documents."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear numeric value for the ground truth maximum batch size of Mixtral fine‚Äëtuning on an NVIDIA A100‚Äë40GB GPU, nor a direct quote indicating that number."
