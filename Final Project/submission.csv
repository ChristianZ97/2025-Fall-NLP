id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4% increase,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: ""The total energy consumption of the US data centers increased by about 4% from 2010-2014, ...""","The question asks for the average increase in U.S. data center electricity consumption from 2010 to 2014. Doc [wu2021b] explicitly states that this period saw an approximate 4% increase, providing the needed value."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Amazon Solar Farm Maryland‚ÄìCPV Backbone will avoid more than 64,000 metric tons of CO2e each year‚Äîthe equivalent of taking more than 13,900 cars off the road.'","The context states that the solar farm‚Äôs avoided emissions equal taking more than 13,900 cars off the road, so the estimated number of cars is 13,900."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.'","The question asks for the training duration for GPT‚Äë3 on 10,000 V100 GPUs at 24.6‚ÄØTeraFLOPS/sec. Doc [patterson2021] explicitly states that this configuration requires approximately 14.8 days, so the answer is 14.8‚ÄØdays."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24 data centers,24,data centers,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: In 2023, AWS increased the number of data centers using recycled water for cooling from¬†20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The passage explicitly states that in 2023 AWS began using recycled water for cooling in 24 data centers, providing the count needed for the answer."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold improvement",6750,fold,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'apable of running at 5,000,000kHz ... This is a more than 6,750 fold improvement in processor clock speed ...'","The context states that a typical 2021 microprocessor runs at 5,000,000‚ÄØkHz compared to the Intel‚ÄØ4004‚Äôs 740‚ÄØkHz, giving a speed increase of 5,000,000/740 ‚âà 6,750√ó. The document explicitly cites a >6,750‚Äëfold improvement, which is used as the answer."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72% time-saving,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: are listed in Table 1. Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).",The document explicitly states that using the growth strategy saves 72% of the training time compared to a from-scratch approach for the 101B model.
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg CO2e per GPU,463,kg,is_blank,is_blank,"Doc [morrison2025#0049]: '3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.'","The question asks for embodied carbon emissions per GPU. Doc [morrison2025#0049] states that the embodied emissions are 3700 kg CO2eq for an 8‚ÄëGPU server node, which yields 463 kg CO2e per GPU. This directly answers the question with the specified unit."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give information about premature deaths in 2030 from scope‚Äë2 pollutants of U.S. data centers.
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [cottier2024#0044]: has the largest amortized hardware and energy cost, at $40M. Doc [li2025a]: trained with our growth strategy under a budget of $100K.",The amortized training cost for GPT‚Äë4 is $40M (Doc [cottier2024#0044]) and the total training budget for FLM‚Äë101B is $100K (Doc [li2025a]). Dividing 40M by 0.1M yields a factor of 400. Thus GPT‚Äë4‚Äôs amortized cost was 400 times greater than FLM‚Äë101B‚Äôs budget.
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 tCO2e,1.2,tCO2e,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: a single passenger round trip SF-NY is ~1.2t CO 2 e (Table 2).,"The context states that a single passenger round trip from San‚ÄØFrancisco to New‚ÄØYork emits about 1.2‚ÄØtCO2e, so that is the approximate value."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of  2014, is a long‚Äëterm investigation of the field of Artificial Intelligence (AI) ...'","The question asks for the launch year of the One Hundred Year Study on AI. Doc [stone2022] explicitly states that the study was launched in the fall of 2014, so the year is 2014."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context provides a total permitted NOx limit of 13,000 tons for all data center backup generators in Virginia, but it does not give a region‚Äëspecific figure for northern Virginia. Therefore the documents do not contain enough information to answer the question confidently."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.94,43.94,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: Results. On average, FLM-101B achieves a score of 43.94,","The passage from Doc [li2025a] explicitly states that FLM-101B‚Äôs final average performance score on the Open LLM Leaderboard is 43.94. No other document provides a different average for this model, so this value is used as the answer."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: The UN‚Äôs Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling","The question asks for the percentage of e‚Äëwaste formally collected and recycled according to the UN‚Äôs Global E‚ÄëWaste Monitor 2024. Doc [luccioni2025a] explicitly states that about 22% of e‚Äëwaste has been formally collected and recycled, which is the value used in the answer."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,Command-R Plus,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).'","The 2025 study documents that the maximum GPU energy consumption among the listed models is 3,426 Wh for Command‚ÄëR Plus, making it the highest."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,experts,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'set the same number of experts to 8 and top-k to 2 for every layer.',"The JetMoE-8B documentation states that each MoE layer uses 8 experts, as shown in the hyperparameter table and the accompanying text."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,Doc [morrison2025#0138]: '8 0.036 12.0 0.054 12.64 21.5 bil.',"The table in Doc [morrison2025#0138] lists the GPU Power Usage for Llama‚ÄØ3.2‚ÄØ1B at an 8‚ÄØreq/s frequency as 0.036‚ÄØkWh, which matches the scenario described (SGLang benchmarking on 2400 ShareGPT prompts)."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"Doc [dodge2022#0056]: 6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days.","The question asks for the estimated duration of a full training run. Doc [dodge2022#0056] explicitly states that a full training run would take 60 days, so 60 days is the answer."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,"The question requests the cost estimation for the English portion of FLM‚Äë101B. Doc [li2025a] explicitly states that the English portion requires 28.22 zettaFLOPs, so that is the answer."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.1%,0.1,%,is_blank,is_blank,Doc [patterson2021#0063]: 'design intuitive improved models with reduced communication and computational costs [Fed21]. The authors show large sparse models‚Äî1500B parameters but only 0.1% activated per token‚Äîcan deliver up to 7x increases in pre-training speed...',"The question asks for the activation percentage of the Switch Transformer‚Äôs 1500‚ÄØB parameters. Doc [patterson2021#0063] explicitly states that for the Switch Transformer only 0.1% of its 1500‚ÄØB parameters are activated per token, so the answer is 0.1%."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a#0002]: ... Jevons‚Äô Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The paper argues that technical efficiency gains may not reduce environmental harm because of the economic rebound effect known as Jevons‚Äô Paradox, which is explicitly cited in the cited passage."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention a hardware processor for an experimental setup of energy-efficient local inference in financial sentiment classification.
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The question asks for the total wall-clock time of FLM-101B when using a growth strategy. Doc [li2025a] explicitly states that the total time cost is 21.54 days, so that is the answer."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024#0029]: 'We sampled 88 models, some of which were trained or finetuned specifically for the tasks that we selected...'","The question asks for the number of models sampled and analyzed in the 2024 Power Hungry Processing study. Doc [luccioni2024#0029] explicitly states that 88 models were sampled, and Doc [luccioni2024] also references 88 models, confirming the answer."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).'","The question asserts that Red AI is on the decline, but Doc [schwartz2019] explicitly states that Red AI is on the rise, contradicting the claim. Therefore the correct answer is FALSE."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3√ó,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3√ó.'","The question asks for the factor by which the overall carbon footprint is reduced when GPU utilization reaches 80% for LM training. The cited passage directly states that increasing utilization to 80% decreases the overall carbon footprint by 3√ó, so the answer is a factor of 3."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.6 MWh,103.6,MWh,is_blank,is_blank,"Doc [dodge2022#0056]: 'The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8 * 13.8 MWh) ‚âà 103.6 MWh.' / Doc [dodge2022#0054]: 'we estimate a full training run would consume approximately 103,593 kWh.'","The question asks for the estimated energy for a full training run of a 6.1B parameter transformer. Doc [dodge2022#0056] gives the partial run energy (13.8 MWh for 13% of the run) and the full run estimate (~103.6 MWh). Doc [dodge2022#0054] provides a consistent figure of 103,593 kWh (‚âà103.6 MWh). Therefore the answer is 103.6‚ÄØMWh."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,1.2x to 4x,"[1.2, 4]",is_blank,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024#0030]: 'a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.'; Doc [cottier2024]: 'ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.'","The question asks for the factor by which total compute for model development exceeds the compute of the final training run. Both cited passages from Cottier et al. state the ratio ranges from 1.2x to 4x, so the answer is that total compute is 1.2 to 4 times larger."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 ‚Äì 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'More critically, the global AI demand is projected to account for 4.2 ‚Äì 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 ‚Äì 6 Denmark or half of the United Kingdom.'","The question asks for the projected water withdrawal by global AI in 2027. Doc [li2025b] explicitly states a range of 4.2 ‚Äì 6.6 billion cubic meters, which is taken as the answer."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'A vast majority of model experimentation (over tens of thousands of training workÔ¨Çows) utilizes GPUs at only 30-50%, leaving room for utilization and efÔ¨Åciency improvements.'","The question asks whether most experimentation workflows use GPUs at over 80% capacity. Doc [wu2021a] states that the vast majority use GPUs at only 30-50% capacity, which directly contradicts the claim, so the answer is FALSE."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not contain a specific numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU, so the question cannot be answered confidently."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021]'.","The question asks for the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic. Doc [wu2021b] explicitly states that emissions dropped by 6.4%, providing the percentage value and context for the reduction."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.' and 'these methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models'","The question asks for the name of a collaborative project that seeks a standardized method for comparing inference efficiency. Doc [luccioni2025c] explicitly names this project as ""AI Energy Score"" and describes its goal of establishing a unified approach to inference efficiency comparison, matching the requested description."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,69 years,69,years,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: ""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.""","The question asks for the approximate age of the AI field in 2025. Doc [stone2022] states that AI was officially born in 1956; 2025 minus 1956 equals 69 years, which is the age of the field."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'Then, we set the same number of experts to 8 and top-k to 2 for every layer.' 
Doc [shen2024]: 'Top-k 2'","The question asks for the number of experts activated per token in each layer of JetMoE-8B. The context states that the model uses top‚Äëk = 2 for every layer, so two experts are selected for activation."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,51.686 MWh,51.686,MWh,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'the energy usage for fine‚Äëtuning the Bloomz‚Äë7B required 7,571 kWh compared to 51,686 kWh for the entire training process,'","The question asks for the energy consumption of pre‚Äëtraining the BLOOM model. Doc [ebert2024] gives the pre‚Äëtraining energy as 51,686 kWh, which equals 51.686 MWh. This figure directly answers the question."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The claim references a 200,000x increase, but Doc [schwartz2019] reports a 300,000x increase over the same six‚Äëyear period, indicating the statement is not accurate."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS) training of a large Transformer model,Neural Architecture Search (NAS),is_blank,luccioni2023;luccioni2025c,https://arxiv.org/pdf/2302.08476;https://arxiv.org/pdf/2506.15572,"Doc [luccioni2023]: 'the emissions of training and fine‚Äëtuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.' 
Doc [luccioni2025c]: 'NAS training workload ... performed much less frequently than the average AI model training workload.'","The question asks for the specific AI process underlying the five‚Äëcars estimate. Both documents state that the estimate comes from a 2019 study that measured emissions from a large Transformer model trained with Neural Architecture Search (NAS), a process performed infrequently. Thus the answer is NAS training of a large Transformer model."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025;patterson2021,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2104.10350,"Doc [jegham2025]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity'","The question asks for the estimated electricity consumption in MWh for GPT‚Äë3 training. All cited documents report the same figure of 1,287‚ÄØMWh, so that value is provided as the answer."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The question asks for Amazon‚Äôs energy storage capacity in 2023. Doc [amazon2023] explicitly states that Amazon held 1.3 gigawatts (GW) of storage capacity in 2023, so that is the answer."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'ons of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.'","The question asks for the number of transatlantic flights that correspond to GPT‚Äë4o‚Äôs annual emissions. Doc [jegham2025] explicitly states the emissions are comparable to the cumulative emissions of approximately 272 transatlantic flights, so 272 is the correct answer."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide an energy savings figure of 44% for a TPOT of 77‚ÄØms, but do not give a specific percentage for a TPOT of 100‚ÄØms, so the requested value cannot be determined from the provided context."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the longest kernel execution time of the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU, so I cannot provide the requested microsecond value."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22 AWS data center regions,22,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023#0277]: '‚Ä¶ 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy‚Ä¶'; Doc [amazon2023]: '22 AWS data center regions is matched with renewable energy sources‚Äîan increase from 19 regions in 2022.',"The question asks how many AWS data center regions had 100% renewable electricity in 2023. Both Doc [amazon2023#0277] and Doc [amazon2023] state that 22 regions achieved this, so the answer is 22."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,Doc [stone2022]: 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.',"The question asks for the year the field was officially christened. Doc [stone2022] states the christening occurred in 1956, so that year is the answer."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,is_blank,is_blank,Doc [luccioni2025c#0096]: Llama 7B 63 Meta 356 14,"The table in Doc [luccioni2025c#0096] lists the GHG emissions for pre-training Llama 7B as 14‚ÄØtCO2e, which directly answers the question."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].,"The question asks for the global average PUE of AI‚Äëdedicated data centers in 2023. The context provides the only global PUE figure for 2023, stating it was 1.58, which serves as the answer."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B,2,B,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token'","The question asks how many JetMoE-8B parameters are active per token. The cited passage from Doc [shen2024] explicitly states that 2B (2 billion) parameters are activated for each input token, so the answer is 2B."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.'","The question asks for the percentage reduction in RM2 model size after 32‚Äëbit to 16‚Äëbit quantization. Doc [wu2021a] explicitly states a 15% reduction, which is the answer."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: ... approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity","The statement matches the figure reported in Doc [wu2021b], which says about 770‚ÄØmillion people lack stable electricity, so the claim is true."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear, numeric average WUE value for Google's AI-dedicated data centers in 2024 that can be confidently reported."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",True,1,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy'","The question asks whether sparsely activated DNNs consume less than one‚Äëtenth the energy of large dense DNNs while keeping accuracy.  The cited passage from Doc [patterson2021] states exactly that, so the statement is true."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules per token,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The question asks for the energy consumed per output token when the maximum generation length is 512 tokens. Doc [samsi2024] explicitly states that at this length it takes about 3‚Äì4 Joules per token, so that is the reported energy cost per token."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'sions arising from energy sources used to power model training and deployment, including servers and data center cooling.'","The statement claims operational impacts exclude cooling emissions, but Doc [morrison2025] explicitly states that operational emissions include GHG emissions from servers and data center cooling, so the statement is false."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: 'The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG‚Äôs exp',"The context indicates that the 5-10% emission reduction claim is based on BCG‚Äôs experience but lacks detailed, publicly available calculations or sound scientific grounding, so the claim is not supported and the answer is False."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)',"The question asks for the optimizer stage percentage in BlackMamba sparse fine‚Äëtuning on an NVIDIA A40-48GB GPU with batch size 1. Doc [xia2024] explicitly states that this stage accounts for up to 53% of the running time under those conditions, so 53% is the correct answer."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information specifying the percentage of AI inference workloads in Asia powered by coal in 2023.
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"14,380 vans",14380,vans,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.'; Doc [amazon2023]: 'Last Mile Electric Delivery Vehicles By Region 2022 2023 U.S. 2,600 11,800 Europe 1,220 3,000+ India 3,800 7,200+.'","The question asks for the total number of Amazon electric delivery vans added from 2022 to 2023. The US fleet increased from 2,600 to 11,800 vans (an addition of 9,200). Europe increased from 1,220 to at least 3,000 vans (at least 1,780 added). India increased from 3,800 to at least 7,200 vans (at least 3,400 added). Summing these additions gives at least 14,380 vans added across the three regions, which is the answer reported."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?",25000 USD,25000,USD,is_blank,is_blank,"Doc [schwartz2019#0023]: 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.'","The question asks for the estimated training cost of Grover. Doc [schwartz2019#0023] explicitly states that training Grover on 256 TPU chips for two weeks cost $25,000 USD, so that is the answer."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'the average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].',"The question asks for the average global PUE in 2023, and Doc [ebert2024] explicitly states that the average global PUE that year was 1.58. Therefore the answer is 1.58."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific energy consumption value for the o3 model when processing a long prompt in watt-hours.
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,Doc [luccioni2024]: 'Flan-T5-xxl 11B 11.48 0.083',"The table in Doc [luccioni2024] gives 0.083‚ÄØkWh per 1,000 inferences for Flan‚ÄëT5‚Äëxxl. Dividing by 1,000 yields 0.000083‚ÄØkWh per query. Multiplying by 1‚ÄØbillion queries gives 83,000‚ÄØkWh, which is 83‚ÄØMWh."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry.'","The question asks how many members were in the inaugural 2015 Study Panel. Doc [stone2022] explicitly states the panel was a seventeen-member group, so the answer is 17."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the distance from the Earth to the Sun, so the answer cannot be determined from the given context."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear statement of the number of wind turbines Microsoft directly contracted to power Azure AI clusters in 2023.
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.'","The question asks for the share of a client device‚Äôs total carbon footprint attributable to manufacturing. Doc [wu2021a] explicitly states that manufacturing carbon cost accounts for 74% of the total footprint of client devices, so the answer is 74%."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Gemini Ultra has the highest fraction of R&D staff cost at 49%','Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.'","The question asks for the percentage of total development cost attributed to R&D staff (including equity) for Gemini Ultra. Doc [cottier2024] explicitly states that Gemini Ultra‚Äôs R&D staff cost fraction is 49%, which directly answers the question."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9√ó,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9√ó increase in AI training infrastructure capacity over the 1.5 years.',"The question asks for the factor by which AI training infrastructure capacity grew over the 1.5‚Äëyear period. The quoted passage from Doc [wu2021a] states a 2.9√ó increase, which directly answers the question."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about the metric tons of CO2 emitted by OpenAI's API requests in January 2024.
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Doc [chung2025#0062]: 'Generally, models with more parameters consume more energy, but this is not always the case.'","The question asserts that more parameters always mean higher inference energy. Doc [chung2025#0062] explicitly states that while larger models often consume more energy, there are cases‚Äîsuch as Phi‚Äë3 Mini vs Small‚Äîwhere a larger model can use less energy. Thus the statement is false."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,is_blank,is_blank,"Doc [fernandez2025#0045]: 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, imp' and Doc [fernandez2025#0046]: 'by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).'","The question asks for the batching strategy that reduces idle GPU time by replacing finished requests with new ones. Both Doc [fernandez2025#0045] and Doc [fernandez2025#0046] explicitly state that this strategy is called continuous batching, which matches the description."
q080,True or False: The AlphaGo program defeated the human Go champion.,True,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to...'","The question asks whether AlphaGo defeated the human Go champion. Doc [stone2022] explicitly states that AlphaGo beat the human Go champion in a five‚Äëgame match, confirming the statement as true."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e, which is over 40 times the five cars estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.'","The context states that Meta‚Äôs Llama‚ÄØ3 family emitted 11,390‚ÄØtCO2e, and explicitly notes this is more than 40 times the five‚Äëcars lifetime emissions estimate, providing the requested comparison."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: esearch on the fairness, security , privacy , and societal impacts of  AI systems. Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The question asks whether the Study Panel is concerned that AI is an imminent threat. The passage from Doc [stone2022] explicitly states the panel found no cause for concern, so the statement is false."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide the maximum batch size for BlackMamba sparse on GSM8K dataset with an A40 GPU.
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: 'The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].',"The question asks for the gross carbon intensity of energy for the U.S. average mix in 2021. Doc [patterson2021] explicitly states the value as 0.429 kg CO2e per kWh, so that is the answer."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems","The question asks for the term used to describe an expanded form of transparency that includes socio-technical aspects and environmental footprint. Doc [luccioni2025b] explicitly defines this as ""social transparency"", so that is the answer."
q093,How many parameters does the largest T5 model have?,11B parameters,11000000000,parameters,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'The largest size has 11B parameters,'","The question asks for the parameter count of the largest T5 model. Doc [patterson2021] explicitly states that the largest T5 has 11B parameters, so that is the answer."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'The entire alignment process takes 60 H100 GPU hours.',"The question asks for the total H100 GPU hours used for alignment, which includes both dSFT and dDPO fine-tuning. Doc [shen2024] explicitly states that the entire alignment process requires 60 H100 GPU hours, providing the required numeric value and unit."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention classification experiments on German public administration texts or specify a model using sentence embeddings that achieved the highest accuracy.
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,is_blank,is_blank,Doc [luccioni2025b#0134]: 'We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist) ‚Äì ...',"The passage from Doc [luccioni2025b#0134] explicitly states that researchers do not believe a universal, one‚Äësize‚Äëfits‚Äëall approach can be developed, so the claim is false."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: h GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The question asks for the GPU energy usage range for 1,000 inference queries. Doc [luccioni2025c] explicitly states that the range is 0.06‚ÄØWh to 3,426‚ÄØWh, so that is the answer."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,is_blank,is_blank,"Doc [chen2024#0003]: 'develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.'","The question asks for the LLM inference system name developed by Chen et al. that uses model-attention disaggregation. All cited passages explicitly name the system as ""Lamina"", confirming the answer."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,is_blank,is_blank,Doc [erben2023#0021]: 'Hivemind [39] is a PyTorch-based [32] framework ...',"The question asks for the decentralized PyTorch-based framework used for distributed spot instance training. All three referenced passages identify Hivemind as a PyTorch-based, decentralized framework that enables this type of training across clouds and continents, so Hivemind is the correct answer."
q094,What is the total number of parameters in the JetMoE-8B model?,8B parameters,8,B,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'LLaMA2 DeepseekMoE Gemma JetMoE
# Total Params 7B 16B 2B 8B'","The table in Doc [shen2024] lists JetMoE‚Äôs total parameter count as 8B, which directly answers the question about the total number of parameters in the JetMoE-8B model."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"Doc [li2025b]: 'Importantly, the company‚Äôs data center water consumption increased by ‚àº20% from 2021 to 2022 and by ‚àº17% from 2022 to 2023 [4]'; Doc [luccioni2025a]: 'Google observed a 20% uptick in the same period'","The question asks for the percentage increase in Google's data center water consumption from 2021 to 2022. Both Doc [li2025b] and Doc [luccioni2025a] state a 20% increase in that period, so the answer is 20%."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams of CO2eq",1594,grams,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams ofùê∂ùëÇ2ùëíùëû for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51], whereas the least carbon-intensive text generation mod","The question asks for the CO2eq emissions per 1,000 inferences of stable-diffusion-xl-base-1.0. The passage in Doc [luccioni2024] explicitly states that this model emits 1,594 grams of CO2eq per 1,000 inferences, so that is the answer."
q096,What is the name of the emissions metric defined as 'CO‚ÇÇ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: 'Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed',"The question asks for the name of the metric defined as CO‚ÇÇ emissions per unit of electricity consumed. Doc [khan2025] provides the definition and explicitly names the metric as ""Carbon Intensity."""
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810√ó,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer‚Äëbased universal translation model by 810√ó.'","The question asks for the reduction factor relative to a CPU baseline. Doc [wu2021a] explicitly states that full‚Äëstack optimization reduces the operational carbon footprint by 810√ó, so that is the answer."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13 InferSave-2nd g6.xlarge 60 415.04 2.344 Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699
Doc [kim2025#0111]: Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about","The Max-Performance policy chose g6e.xlarge costing $2.699, while InferSave chose g4dn.xlarge costing $2.13. The percentage increase is (2.699‚Äì2.13)/2.13√ó100% ‚âà 26.7%, so the Max-Performance instance was about 26.7% more expensive."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3.7,million GPUs,is_blank,is_blank,Doc [luccioni2025a#0081]: 'NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023)',"The 2025 paper (Doc [luccioni2025a#0081]) explicitly states that NVIDIA shipped 3.7 million data‚Äëcenter GPUs in 2024, providing the numeric value and unit needed for the answer."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,is_blank,is_blank,"Doc [cottier2024#0049]: 'Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.'","The question asks for the average share of total amortized hardware and energy cost that belongs to AI accelerator chips. Both Doc [cottier2024#0049] and Doc [cottier2024#0050] explicitly state that this share is 44%, so the answer is 44%."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The question asks for the acronym of the Finnish project that proposes integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI. Doc [luccioni2025b] explicitly names the project as ETAIROS, so the answer is ETAIROS."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.64,0.64,is_blank,is_blank,is_blank,Doc [erben2023#0088]: '‚Ä¶for NLP compared to the A-4 runs (Figure 7a).‚Ä¶ 36% slower for NLP‚Ä¶',"The question asks for the fraction of local throughput achieved for NLP when training was spread across four continents (C-4) instead of remaining local (A-4). Doc [erben2023#0088] states NLP throughput is 36% slower than the local A-4 run, meaning it achieves 64% (1‚Äë0.36) of the local throughput. Thus the fraction is 0.64."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: '101 4 4 12 192 2160 165 52.88%',"The question asks for the FLOPs utilization percentage in the final growth stage of FLM‚Äë101B. The table in Doc [li2025a] lists the utilization for each stage, showing 52.88% for the 101B stage, which is the final growth stage."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5 billion liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 3.5B Liters of water returned to communities from replenishment projects in 2023, with additional volume contracted and r","The question asks for the volume of water returned from Amazon‚Äôs replenishment projects in 2023. Doc [amazon2023] states that 3.5B Liters (3.5‚ÄØbillion liters) were returned, so the answer is 3.5‚ÄØbillion liters."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,"Doc [wu2021a]: 'Achieving a Power Usage Effectiveness (PUE) of about 1.10,'
Doc [wu2021b]: 'PUE of Facebook datacenters is 1.10 (2020)'","The question asks for Facebook‚Äôs data‚Äëcenter PUE. Both Doc [wu2021a] and Doc [wu2021b] explicitly state that Facebook‚Äôs data centers have a PUE of approximately 1.10, so the answer is 1.10."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: 'finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device',"The question asks how many physical print books equal the CO2 emitted by one Kindle. Doc [luccioni2025a] states that 115 books produce the same amount of CO2, so the answer is 115 books."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Doc [rubei2025]: 'Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.',"The question asks whether custom tags with zero-shot, one-shot, and few-shots reduce energy consumption. The cited passage from Doc [rubei2025] explicitly states that custom tags reduce energy consumption across those techniques, so the answer is True."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024#0003]: 'per-household health burden could be 200x more than that in less-impacted communities.',"The question asks for the factor by which the per-household health burden could exceed that in less-impacted communities. Three passages state a 200‚Äëfold increase, so the factor is 200."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the DS Llama 70B inference energy consumption on the FKTG dataset.
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.'","The question asks for the estimated amortized training cost of GPT‚Äë4. The cited passage in Doc [cottier2024] explicitly states that GPT‚Äë4‚Äôs amortized hardware and energy cost is $40‚ÄØmillion, which is the value used in the answer."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 ¬µg/m¬≥,9,¬µg/m¬≥,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'the EPA‚Äôs recently tightened standard for PM2.5 sets an annual average limit of 9¬µg/m 3, considerably higher than t'","The question asks for the EPA‚Äôs recently tightened primary standard for the annual average limit of PM2.5. Doc [han2024] explicitly states that this limit is 9¬µg/m¬≥, providing the required value and unit."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,luccioni2025a;jegham2025,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2505.09598,"Doc [luccioni2025a]: ""...due to effects such as Jevons Paradox.""; Doc [jegham2025]: ""...aligned with the Jevons Paradox...""; Doc [luccioni2025b#0124]: ""This is often referred to as Jevons paradox.""","The question asks for the phenomenon where technological progress improves efficiency but leads to higher overall consumption. All referenced documents explicitly name this effect as the Jevons Paradox, confirming it as the correct answer."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement of which West Virginia county is projected to have the highest per-household health cost for 2030, so I cannot answer confidently."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 lbs CO2e",36156,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'American life, avg, 1 year 36,156'","The table in Doc [strubell2019] lists consumption CO2e in pounds and records the average American life in one year as 36,156 lbs, which is the requested estimate."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,M USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: training costs. We find that the most expensive publicly-announced training runs to date are OpenAI‚Äôs GPT-4 at $40M and Google‚Äôs Gemini Ultra at $30M.,"The passage from Doc [cottier2024] explicitly states that Google‚Äôs Gemini Ultra has an estimated amortized training cost of $30M, which directly answers the question."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",6.1 billion parameters,6100000000.0,parameters,is_blank,is_blank,Doc [dodge2022#0056]: '6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s.',"The 2022 Dodge et al. paper describes a 6.1‚Äëbillion‚Äëparameter Transformer model; the question asks for the total parameter count, so the answer is 6.1‚ÄØbillion parameters."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,is_blank,is_blank,"Doc [luccioni2024#0084]: 'Training energy (kWh) 51,686 ... Finetuning energy (kWh) 7,571 ...'","The Power Hungry Processing study lists the training energy for BLOOMz-7B as 51,686‚ÄØkWh and the fine‚Äëtuning energy as 7,571‚ÄØkWh. Adding these values gives a combined energy cost of 59,257‚ÄØkWh."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: nference energy (kWh) task mean std text classification 0.002 0.001 extractive QA 0.003 0.001 masked language modeling 0.003 0.001 token classification 0.004 0.002 image classification 0.007 0.001 object detection 0.038 0.02 text generation 0.047 0.03 summarization 0.049 0.01 image captioning 0.063 0.02 image generation 2.907 3.31 Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","The question asks for the average energy consumption for 1,000 image generation inferences. Table 2 in the 2024 study lists the mean energy for image generation as 2.907 kWh, which is the value used for the answer."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,is_blank,is_blank,Doc [luccioni2024#0109]: 'we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.',"The question asks for the total energy used for all experimentation and evaluation in the 2024 study. Doc [luccioni2024#0109] (and similarly Doc [luccioni2024#0037]) provides the explicit figure of 754.66 kWh, which is the answer."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592'","The cost‚Äëparity column in Table 5 of Doc [luccioni2024] lists 592,570,000 inferences for BLOOMz‚Äë7B, indicating that this many inferences are needed for deployment energy to equal the initial training and fine‚Äëtuning energy cost."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9).'  
Doc [ebert2024]: 'Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors... no detailed reporting on mitigation efforts concerning environmental risks is currently required.'","The AI Act requires risk assessment for GPAI models with systemic risk, but the documents state that environmental risks are not explicitly required to be included, making the statement false."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: n scaling up FLM-101B is the growth strategy. Specifically, we train three models, with 16B, 51B, and 101B parameters, respectively","The question asks for the total number of parameters in the final FLM‚Äë101B model. Doc [li2025a] explicitly states that the final model contains 101B parameters, so the answer is 101‚ÄØB."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention the dataset name for the German nuclear waste site objection texts classified in the experiments.
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84% of LLM token usage,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed'","The question asks for the percentage of token usage through non-disclosed models. Doc [luccioni2025c] states that 84% of token usage was through models with no disclosure, directly providing the answer."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.96√ó,0.96,is_blank,is_blank,is_blank,Doc [khan2025#0049]: tral-small 0.73 0.70 0.69 0.70 0.015,"The table in Doc [khan2025#0049] lists Mistral‚Äësmall‚Äôs baseline emissions as 0.73 and its emissions after optimization as 0.70. Dividing 0.70 by 0.73 gives a multiplier of approximately 0.96, indicating the emissions were reduced to about 96% of the original value."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: 'Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.',"The question asks for the cost savings achieved when mixing A100 and A10G GPUs compared to an A100-only strategy. Doc [griggs2024] explicitly states that in the scenario of 2 A100s and 1 A10G, the cost saving is 24% over A100-only, so 24% is the answer."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide GPT-3 training energy (232‚ÄØMWh) but do not give a clear numeric training energy for Meena, so the ratio cannot be determined from the supplied context."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,3 passengers,3,passengers,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: e   of   84.5%   seat   occupancy,   yielding   1.2t   of   CO 2 e   per   passenger   round   trip.   Thus,   the   CO 2 e   equivalent   of   NAS   is   ~3   passengers   taking   a   round   trip   between   San   Francisco   and   New   York.","The document states that each passenger round‚Äëtrip SF‚ÄëNY emits about 1.2‚ÄØtCO2e, and 3 such trips would emit roughly 3.6‚ÄØtCO2e, which is close to the 3.2‚ÄØtCO2e actual emissions of the Evolved Transformer NAS. Therefore, the emissions are equivalent to about 3 passengers on that route."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'Model Size V100 32GB A100 80GB
Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128'","The table in Doc [samsi2024] lists the bare minimum GPU counts for each LLaMA size. For the 13B model, it shows a single A100‚Äë80GB GPU is sufficient, so the minimum number required is one."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons of CO2,"[21, 78]",tCO2,is_blank,is_blank,"Doc [dodge2022#0092]: 'If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2.'; Doc [dodge2022#0069]: 'If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it w...'","The question asks for the estimated range of CO‚ÇÇ emissions for a full training run of a 6.1‚ÄØbillion‚Äëparameter transformer. Both Doc¬†[dodge2022#0092] and Doc¬†[dodge2022#0069] state that a complete run would emit 21 to 78 metric tons of CO‚ÇÇ, which directly provides the required range."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a numeric estimate of the total carbon emissions avoided by pruning and quantizing large language models in 2023.
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: 'most carbon foot print analyses gather the information manually by writing to authors.',"The provided passage states that most analyses gather data manually by contacting authors, contradicting the claim that they gather information automatically. Therefore the statement is false."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA A100 80GB GPU,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'Model Size V100 32GB A100 80GB ... 7B 1 64 1 64',"The table in Doc [samsi2024] lists the bare minimum hardware requirements for each LLaMA variant. For the 7B model the entry shows 1 A100 80GB GPU, indicating that a single GPU is sufficient for inference without compression or quantization."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",1.9 √ó 10^9 inferences,1900000000.0,inferences,is_blank,is_blank,"Doc [dodge2022#0054]: 'we estimate a full training run would consume approximately 103,593 kWh.'; Doc [luccioni2024#0085]: 'Inference energy (kWh) 5.4 √ó 10‚àí5'.","The full training energy for the 6.1B model is 103,593 kWh (Doc [dodge2022#0054]). The per‚Äëinference energy for BLOOMz‚Äë7B is 5.4√ó10‚Åª‚Åµ kWh (Doc [luccioni2024#0085]). Dividing the training energy by the per‚Äëinference energy yields about 1.9√ó10‚Åπ inferences, which is the approximate number required to match the training energy cost."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",4.63 $/hr,4.63,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,"Doc [chen2024]: Table 1: H100, H20, and TPU v6e specifications. ... Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The price per hour for an NVIDIA H20 is explicitly listed by Chen et al. (2025) as $4.63/hr in the table of chip prices, which is the value requested."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024]: 'This is equivalent to approximately 44% of the data centers‚Äô total electricity cost.',"The question asks for the percentage of electricity cost represented by the public health cost in 2023 using the average attribution method. The context states that the public health cost is about 44% of the data center electricity cost, so the answer is 44%."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: 'United Kingdom 36 901',"The table of Amazon Renewable Energy Projects announced as of January 2024 lists 36 projects in the United Kingdom, so the answer is 36."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25T tokens,1250000000000,tokens,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.'","The question asks for the number of tokens used to pre‚Äëtrain JetMoE-8B. Doc [shen2024] explicitly states that the model was trained on 1.25 trillion (1.25T) tokens, so the answer is 1.25T tokens."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'Water withdrawal:It refers to freshwater taken from the ground or surface water sources, either tem-porarily or permanently, and then used for agricultural, industrial, or municipal uses'","The question asks for the term that describes freshwater drawn from ground or surface sources. Doc [li2025b] explicitly defines this as ""Water withdrawal,"" which matches the description and is the correct term."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,is_blank,is_blank,Doc [li2025b#0034]: 'Apple reports that its supply chain accounts for 99% of its total water footprint [23]'.,"The question asks for the percentage of Apple‚Äôs total water footprint that comes from its supply chain. Doc [li2025b#0034] explicitly states that Apple‚Äôs supply chain accounts for 99% of its total water footprint, so the answer is 99‚ÄØ%."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,answers,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'Luccioni and Hernandez‚ÄëGarcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, ...'","The question asks how many answers were collected after contacting over 500 authors. Doc [luccioni2025b] explicitly states that 95 answers were obtained, so 95 is the correct count."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,times,luccioni2025a;luccioni2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2504.00797,Doc [luccioni2025a]: 'could add up to 640 percent more carbon emissions compared to the company‚Äôs carbon removal targets for the year',"The question asks how many times more emissions the deal could add relative to Microsoft‚Äôs yearly carbon removal targets. The context states the deal could add up to 640‚ÄØpercent more emissions, which equals 6.4 times the target."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",$3.33 per H100 GPU-hour,3.33,$/h,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'less than$0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.'","The JetMoE project budget is reported as less than $0.1‚ÄØmillion and the training used 30,000 H100 GPU‚Äëhours. Dividing $100,000 by 30,000 hours gives an approximate cost of about $3.33 per H100 GPU‚Äëhour, which is the requested estimate."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific percentage for Altoona, Iowa."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,every five years,every five years,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'As its core activity , the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'","The question asks how often the Standing Committee forms a Study Panel. The passage from Doc [stone2022] states that it does so every five years, so the answer is ""every five years."""
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Doc [khan2025]: 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.'","The question asks whether sustainable deployment techniques for LLMs demonstrated up to a 45% reduction in carbon emissions after quantization. Doc [khan2025] explicitly states that quantization can reduce emissions by up to 45%, confirming the statement as true."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not contain a specific numeric value for the total execution time (in seconds) of a sparse BlackMamba model fine-tuned on a NVIDIA A40-48GB with batch size 84.
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].'","The question asks for the average number of connected devices per U.S. household in 2021. Doc [wu2021b] states that the average household has 25 connected devices, providing the numeric value and unit needed for the answer."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"Doc [erben2023#0015]: 'we introduce thegranularity metric, the ratio of calculation to communicati' and Doc [erben2023]: 'Granularity is important to evaluate scalability. We found that the ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability.'","The question seeks the metric used to evaluate the computation‚Äëto‚Äëcommunication time ratio for geo‚Äëdistributed training. Both Doc [erben2023#0015] and Doc [erben2023] state that the introduced metric is the granularity metric, defined as the ratio of calculation to communication time. Therefore the answer is ""granularity metric""."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8‚Äì3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)","The question asks for the range of energy consumption in MWh for pre‚Äëtraining LLMs. Doc [luccioni2025c] explicitly states the range as 0.8‚ÄØMWh to 3,500‚ÄØMWh, which is directly quoted above."
q168,The 2024 Griggs et al. paper reports that M√©lange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: 'Compared to using only a single GPU type, M√©lange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.'","The question asks for the maximum percentage reduction in deployment costs for conversational chat settings. The cited passage from Doc [griggs2024] states that M√©lange can reduce costs by up to 77% in conversational settings, which directly answers the question."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4 A100 GPUs,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.'","The question asks for the bare minimum number of A100 80GB GPUs for LLaMA‚Äë65B inference. Doc [samsi2024] explicitly states that 4 such GPUs are required, providing the required number."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10‚Äì50 completions,"[10, 50]",completions,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"Doc [li2025b]: 'GPT-3 needs to ‚Äúdrink‚Äù (i.e., consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses.'; Doc [luccioni2025a]: 'one paper suggesting that 10‚Äì50 queries on GPT-3 consumes around half a liter of water.'","The documents state that a 500‚ÄØmL bottle of water supports about 10 to 50 medium‚Äëlength GPT‚Äë3 completions. Thus, with the water from one bottle, you can produce between 10 and 50 such completions."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'IBM‚Äôs Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.'","The passage from Doc [stone2022] explicitly states that IBM‚Äôs Watson program beat human contenders in Jeopardy, so the claim that it did NOT beat them is false."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10‚Äì50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,"Doc [luccioni2025a]: '10‚Äì50 queries on GPT-3 consumes around half a liter of water.'  Doc [li2025b]: 'GPT-3 needs to ‚Äúdrink‚Äù (i.e., consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses.'","The study in Doc [luccioni2025a] and Doc [li2025b] both state that 10‚Äì50 queries to GPT-3 consume roughly half a liter of water, so the answer is a range of 10 to 50 queries."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80‚Äì90%,"[80, 90]",%,is_blank,is_blank,Doc [patterson2021#0020]: 'NVIDIA estimated that 80‚Äì90% of the ML workload is inference processing [Leo19]'.,"The question asks for the percentage of ML workload estimated to be inference processing by NVIDIA in 2019. Doc [patterson2021#0020] explicitly states that NVIDIA estimated 80‚Äì90% of the ML workload is inference, so the answer is that range."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [han2024#0002]: 'training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.'","The question asks for the number of LA‚ÄëNYC round trips equivalent to the air pollutants from training a Llama‚Äë3.1 scale model. Both Doc [han2024#0002] and Doc [luccioni2025c] state that the emissions are equivalent to more than 10,000 round trips, so the answer is 10,000+ round trips."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.516,7.516,$/h,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: 'resulting in a normalized price of (4.69/2.29) √ó 3.67 = $7.516 for H100.',"The question asks for the normalized on‚Äëdemand hourly price of an H100 GPU in Griggs et al. (2024). Doc [griggs2024] explicitly states that the normalized price is $7.516, which is the answer provided."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",754.6 tCO2e,754.6,tCO2e,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'In total, for all of model experimentation and evaluation, we used a total of 754.6'","The question asks for the total CO2‚Äëequivalent emissions of the Power Hungry Processing study. Doc [luccioni2024] states that the entire study produced 754.6, and in the context of CO2‚Äëequivalent emissions this is interpreted as 754.6‚ÄØtCO2e."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",49.8%,49.8,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: 'Other Gender Men Women 49.7%50.0% 49.8%49.9%',The table in Doc [amazon2023] lists the percentage of Amazon workforce in the U.S. that identified as men as 49.8%.
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, ... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.'","The paper reports that after peaking in 2022, the trend of direct environmental disclosure actually reversed and decreased, so it did not continue to increase as the question states."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",1000√ó larger,1000,is_blank,is_blank,is_blank,"Doc [wu2021a#0013]: 'with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000√ó larger in size.'","The question asks how much larger the model must be to raise BLEU from 5 to 40. Doc [wu2021a#0013] explicitly states that a 1000√ó larger model is required, so the answer is a 1000‚Äëfold increase."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",7.22 $/h,7.22,$/h,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.'","The context states that 2 A100 GPUs cost over $5,200 per month. Dividing $5,200 by 30 days √ó 24 hours (720 hours) gives about $7.22 per hour, which is the estimated hourly running cost."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,is_blank,is_blank,Doc [jegham2025#0071]: 'GPT-4o consumes around 2.875 Wh while GPT-4o mini‚Äôs consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.',"The quote shows GPT-4o mini consumes more energy per query (3.098‚ÄØWh) than GPT‚Äë4o (2.875‚ÄØWh), so the claim that it consumes less energy is false."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,is_blank,is_blank,"Doc [chung2025#0055]: 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU ‚Äì or any computing device ‚Äì to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst‚Äëcase overestimation of energy consumption.'","The question asks whether estimating GPU energy consumption based on TDP is reliable and accurate. Doc [chung2025#0055] states that TDP‚Äëbased estimates are nearly always an overestimation, indicating they are not reliable or accurate, so the correct answer is False."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents provide the CO2 emissions for training with NAS but do not include an emissions-to-driving-distance ratio needed to calculate an equivalent miles distance.
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide water usage figures for GPT-3 training and GPT-4o inference, but none contain a numeric value for the water used in cooling during OpenAI‚Äôs GPT‚Äë4 training run."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: '30,000 H100 GPU hours.'","The question asks for the number of H100 GPU hours used to pre-train JetMoE-8B. Doc [shen2024] explicitly states that JetMoE-8B was trained with 30,000 H100 GPU hours, which directly answers the query."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1000000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.' Doc [cottier2024#0065]: 'the amortized cost of frontier training runs will exceed one billion dollars by 2027.'","The question asks for the cost threshold that the largest training runs will exceed by 2027. Both cited passages explicitly state that the cost will surpass one billion dollars (USD) by that year, so the answer is 1‚ÄØbillion dollars."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a specific numeric value for AlexNet‚Äôs top-1 ImageNet accuracy, so the answer cannot be determined from the context."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: 'OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].',"The question asks for the total floating point operations reported by OpenAI. Doc [patterson2021] explicitly states that OpenAI published this value as 3.14E+23 FLOPs, which is why the answer is 3.14E+23 FLOPs."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 √ó80G) servers.',"The context states that each of the 24 servers contains 8 A800 GPUs. Multiplying 24 by 8 yields 192 GPUs in total, which is the requested number."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'FAIR‚Äôs RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.'","The question asks for the number of GPU hours used to train FAIR's RoBERTa on 160GB of text. Doc [schwartz2019] explicitly states that the training required around 25,000 GPU hours, so that is the answer."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024#0083]: 'the amount of energy required per inference varies from 5.4√ó 10‚àí5 for the smallest model, BLOOMz-560M to 1.0 √ó 10‚àí4 kWh for the biggest one' and Doc [luccioni2024]: 'Inference energy (kWh) 1.0 √ó 10‚àí4 7.3 √ó 10‚àí5 6.2 √ó 10‚àí5 5.4 √ó 10‚àí5'","The inference energy per inference for BLOOMz-7B is 1.0√ó10‚Åª‚Å¥ kWh. Each of the 606,096 downloads triggers 1,000,000 inferences, so total inferences = 606,096√ó1,000,000 = 6.06096√ó10¬π¬π. Multiplying by 1.0√ó10‚Åª‚Å¥ kWh gives 60,609,600 kWh, which converts to 60,609.6 MWh."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 GPUs,8,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'At a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.'","The question asks for the bare minimum number of NVIDIA V100 32GB GPUs needed for LLaMA‚Äë65B inference. The cited passage from Doc [samsi2024] explicitly states that 8 V100 GPUs are required, so the answer is 8 GPUs."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",14.9 queries/sec,14.9,queries/sec,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 14.9 Dense(bsz=1) Dense(bsz=6) Sparse(bsz=1) Sparse(bsz=6) Sparse(bsz=20) Blackmamba-MATH0 5 10 15 20 2.2 5.3 2.2 6.5 11.6 Dense(bsz=1) Dense(bsz=2) Sparse(bsz=1) Sparse(bsz=2) Sparse(bsz=8),"The figure in Doc [xia2024] lists the ground truth throughput for Mixtral-CS dense at batch size‚ÄØ1 as 14.9‚ÄØqueries per second, directly answering the question."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide emissions for NAS (284‚ÄØtCO2e or 626,155‚ÄØlbs) and compare it to five car lifetimes, but they do not give an equivalent in average American lifetimes."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: ""This implementation of the model uses Pytorch and the FairScale [20] library to enable model sharding across multiple GPUs and nodes.""","The question asks which framework was used to deploy the large language model across multiple GPUs and nodes. The context from Doc [samsi2024] explicitly states that the FairScale library is used for model sharding across GPUs and nodes, making it the correct answer."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 tCO2e",47400,tCO2e,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'These on-site solar energy systems are estimated to generate 123,000‚ÄØMWh annually‚Äîand avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.'","The question asks for the annual CO2e avoided by Amazon‚Äôs on-site solar systems. The passage in Doc [amazon2023] states that these systems avoid roughly 47,400 metric tons of CO2e per year, directly answering the query with the specified unit."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: datacenter   PUE   online   every   quarter .   The   PUE   for   the   Iowa   datacenter   where   we   ran   Evolved   Transformer   is   1.11,"The question asks for the PUE of Google's Iowa datacenter during the Evolved Transformer run. Doc [patterson2021] explicitly states that the PUE for that datacenter is 1.11, so that value is reported as the answer."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681 MT-Bench,6.681,MT-Bench,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'JetMoE-8B-chat 6.681 Llama-2-13b-chat 6.650',"The table in Doc [shen2024] lists the MT‚ÄëBench scores after alignment, showing JetMoE‚Äë8B‚Äëchat scoring 6.681, which is higher than Llama‚Äë2‚Äë13b‚Äëchat‚Äôs 6.650, answering the question."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period'","The 2025 paper in Doc [luccioni2025a] explicitly states that Microsoft‚Äôs global water consumption rose 34% from 2021 to 2022, which is the reported percentage increase."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,homes,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025#0002]: 'Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes,'","The question asks for the number of U.S. homes equivalent to the annual electricity use of 700‚ÄØmillion daily GPT‚Äë4o queries. Doc [jegham2025#0002] explicitly states this equivalence as 35,000 U.S. homes, and Doc [jegham2025] confirms the same figure. Therefore the answer is 35,000 homes."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: ""n from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].""","The question asks for the total estimated number of GPT-4o queries in 2025. Doc [jegham2025] explicitly states that the total is approximately 772‚ÄØbillion queries, so that is the answer."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not mention Yelp sentiment analysis benchmarks or a comparison between traditional models and large language models for that task, so I cannot determine if the statement is true or false."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",16.07 zettaFLOPs,16.07,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'For the 101B model, 26.54B tokens were processed.'\nDoc [li2025a]: 'The 101B model has 101B parameters.'\nDoc [cottier2024]: 'compute = 6√ó parameters √ó tokens.'\nDoc [li2025a]: 'single‚ÄëGPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%.'","The question asks for the total computational work of the final 101B training stage. Using the compute formula from Doc [cottier2024] (6√óparameters√ótokens), the 101B model (101B parameters) trained on 26.54B tokens requires 6√ó101√ó26.54‚âà1.607√ó10^22 FLOPs, which is 16.07 zettaFLOPs. The throughput data in Doc [li2025a] confirms the high FLOP rates, supporting this estimate. Thus the total work is 16.07 zettaFLOPs."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312 GB,5.312,GB,is_blank,is_blank,"Doc [kim2025#0019]: 'When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.'","The question asks for the KV Cache size for OPT-2.7B at batch size 32. Doc [kim2025#0019] states the cache expands to 5.312GB, so that is the answer."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: The US national datacenter average in 2018 was 1.58, which is the value [Str19] used ; In 2020, it was 1.59.","The question asks for the US national data center average PUE in 2020. The passage from Doc [patterson2021] explicitly states that the US average was 1.59 in 2020, so that is the answer."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29%‚Äì49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.'","The question asks for the percentage range of R&D staff costs (including equity) in the total amortized cost for the four models studied. Doc [cottier2024] explicitly states that this range is 29% to 49%, so that is the answer."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: '53% of articles cite the figure of 3 Wh per ChatGPT query or claim it con',"The analysis of 100 news articles reported that 53% of them cited the contested 3‚ÄØWh (or 10‚Äëtimes‚Äëmore‚Äëthan‚ÄëGoogle‚Äësearch) estimate, so the answer is 53%."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,is_blank,is_blank,Doc [luccioni2024#0034]: 'used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference',"The context explicitly states that the CodeCarbon package was employed to measure energy consumption during inference runs, providing the required software package name."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The question asks for the function name that improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The context explicitly names this function as the Compute Time Calibration Function (CTCF), which is cited in the passage from Doc [kim2025]."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not contain a clear figure for gallons of water consumed per ChatGPT user session in 2023.
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,TRUE,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 65B Figures 8 and 9 show energy metrics in terms of responses from the 65B model. Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.","The cited passage explicitly states that increasing the number of GPU shards increases the energy cost per response for LLaMA‚Äë65B, so the correct answer is TRUE."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The context does not provide a specific count of AI training runs conducted on renewable-only power in 2022.
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The cited passage explicitly states that open‚Äësource general‚Äëpurpose AI models are excluded from transparency requirements‚Äîsuch as energy‚Äëconsumption reporting‚Äîunless they pose a systemic risk, so the claim is true."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a;wu2021b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2108.06738,"Doc [luccioni2025a]: 'In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide'","The question asks for the share of PPAs held by the four companies in 2020. Both Doc [luccioni2025a] and Doc [wu2021b] state that they accounted for about 30% of all corporate PPAs worldwide, so the answer is 30%."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: TruthfulQA 38.8 36.1 33.1 41.7
WinoGrande 74.0 73.7 66.3 70.2
GSM8k 14.5 17.3 16.9 27.8
OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0
MBPP (Pass@1) 20.8 34.0 28.0 34.2
HumanEval (Pass@1) 12.8 25.0 24.4 14.6
All Avg. 45.5 47.3 43.2 47.6
Table 3: OpenLLM leaderboard and code benchmarks results from four different models.","The table lists the OpenLLM Leaderboard average scores for four models. The highest average, 53.0, corresponds to JetMoE-8B, as the text states JetMoE-8B achieves the best scores overall. Thus the final average score for JetMoE-8B on the OpenLLM Leaderboard is 53.0."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: '...the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.'","The passage from Doc [han2024] explicitly states that the total public health cost of U.S. data centers in 2023 is about $6.7‚ÄØbillion, which directly answers the question."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].'","The provided documents state that open‚Äësource general‚Äëpurpose AI models are largely excluded from transparency (and thus energy‚Äëreporting) obligations under current EU rules, meaning they are not required to report energy consumption to authorities. Therefore the statement is false."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit information about the energy consumption factor when the Llama‚ÄØ3.1‚ÄØ70B model is deployed on two nodes versus one node, so I cannot answer the question with confidence."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"Doc [khan2025#0033]: We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The question asks for the open-source tool used for 4‚Äëbit quantization and local deployment. Doc [khan2025#0033] explicitly names Ollama as the tool applied for quantization and local inference, providing the required answer."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 tCO2e,26,tCO2e,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'net tCO2e 552 380 271 257 291 26',The table in Doc [li2025a] lists the net carbon emissions for each model; the last column corresponds to FLM‚Äë101B and shows a net emission of 26 metric tons of CO2e.
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,SpotLake,SpotLake,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,Doc [erben2023]: 'SpotLake: Diverse Spot Instance Dataset Archive Service.',"The question asks for the storage service that shards and streams datasets for spot VMs. The context cites SpotLake as a diverse spot instance dataset archive service, indicating it is the storage service used for this purpose. "
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, M√©lange achieved cost reductions in what percentage range compared to single-GPU baselines?",13-51%,"[13, 51]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: In Figs. 11c and 11f, M√©lange achieves 13-51% cost reduction (120ms SLO)","The question asks for the cost‚Äëreduction range for short‚Äëcontext Arena workloads at a 120ms SLO. Doc [griggs2024] provides the exact range of 13‚Äë51% for this scenario, so that is the answer."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'Figure 2: As a result of Moore‚Äôs law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].'","The statement claims that GPU theoretical performance per watt doubles approximately every 3‚Äë4 years. Doc [wu2021b] explicitly states that GPU theoretical performance per watt doubles every 3‚Äë4 years, confirming the claim as true."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'the AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024',"The question asks for the U.S. Senator who introduced the AI Environmental Impacts Act bill in February 2024. Doc [ebert2024] explicitly states that Senator Edward J. Markey introduced the bill on 1 Feb 2024, providing the name needed for the answer."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Chen et al. (2025) or quote a price per hour for an NVIDIA H100, so the answer cannot be determined from the given context."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,0.54,0.54,is_blank,is_blank,is_blank,Doc [jegham2025#0068]: '0.443 Wh for long prompts ... GPT-4.1 nano remains among the most efficient proprietary models at 0.827 Wh ...',"The context gives 0.443‚ÄØWh for o3 and 0.827‚ÄØWh for GPT‚Äë4.1 nano on a long prompt; dividing 0.443 by 0.827 yields approximately 0.54, so o3 consumes about 0.54 times the energy of GPT‚Äë4.1 nano."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,False,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.'","The passages state that public health impacts are highly unevenly distributed, with disadvantaged communities experiencing much higher per‚Äëhousehold costs, contradicting the claim of even distribution. Thus the statement is false."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,4 years,4,years,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'me a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq...'","The question asks for the estimated average GPU lifetime before retirement in AI data centers. Doc [morrison2025] explicitly states a 4‚Äëyear lifespan for GPUs, which directly provides the required estimate."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 V100 GPUs,2,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128',"The table in Doc [samsi2024] lists the bare minimum GPU count for each LLaMA variant; for the 13B model it specifies 2 GPUs of 32‚ÄØGB V100, so the bare minimum number is two."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, which is over 4√ó the ‚Äòfive cars‚Äô estimate.",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the ‚Äúfive cars‚Äù number.'","The question asks for the reported GHG emissions from Gemma pre‚Äëtraining and how it compares to the five cars estimate.  Doc [luccioni2025c] explicitly states that Gemma emitted 1247.61‚ÄØtCO2e and that this is over four times the five cars estimate, so the answer reflects that figure and comparison."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50‚Äì70%,"[50, 70]",%,is_blank,is_blank,"Doc [chung2025#0014]: 'GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50‚Äì70% of the total provisioned power in the datacenter [52‚Äì54, 58].'","The question asks for the typical percentage of total provisioned power that GPUs consume in a data center. Doc [chung2025#0014] explicitly states that GPUs account for 50‚Äì70% of that provisioned power, so the answer is the 50‚Äì70% range with unit %."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.'","The passage in Doc [li2025b] explicitly states that the estimated U.S. national average water consumption for electricity generation is 3.1‚ÄØL/kWh, which directly answers the question."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,336 hours,336,hours,strubell2019,https://arxiv.org/pdf/1906.02243,Doc [strubell2019]: Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).,"The question asks for the training duration of ELMo on 3 NVIDIA GTX 1080 Ti GPUs. The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours, and the GPU model is effectively the same as the 1080 Ti. Thus the training time is 336 hours."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.0022 kL,0.0022,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.'","The context states that mining rare earth materials and wafer production together consume 2.2 liters of water per H100 GPU. Converting 2.2‚ÄØL to kiloliters gives 0.0022‚ÄØkL, which is the estimated water consumption per GPU."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: '... cost of $3460.'
Doc [xia2024#0093]: '... net cost of $3460.'","The question asks for the net cost of fine‚Äëtuning a sparse Mixtral model on 2M queries using an NVIDIA H100 GPU. Both Doc [xia2024] and Doc [xia2024#0093] explicitly state that this cost is $3460, so the answer is $3460."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: lobal Cloud Infrastructure. The AWS Global Cloud Infrastructure is built on AWS‚Äôs own custom hardware and optimized for workloads run by AWS customers. Research shows that in North America, AWS can lower its customers‚Äô workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy‚Äîa goal that Amazon, including AWS, achieved in 2023.","The question asks for the typical percent reduction in carbon footprint when moving workloads to AWS in North America. Doc [amazon2023] explicitly states that AWS can lower customers‚Äô workload carbon footprints by up to 96% under the specified conditions, so the answer is 96%."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,Doc [dodge2022]: 'Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021.',"The question asks for the reported PUE of Google's hyperscale data centers in 2021. Both Doc [dodge2022] and Doc [wu2021b] state that the PUE was 1.10 in that year, so 1.10 is the correct value."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 lbs",11023,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'Human life, avg, 1 year 11,023'","The question asks for the CO‚ÇÇe emitted per average human life in one year globally. Doc [strubell2019] explicitly lists the figure as 11,023 pounds of CO‚ÇÇe, which directly answers the question."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25√ó,1.25,times,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.'","The document states that for LLaMA‚Äë13B, the A100 provides a 1.25‚Äëtimes increase in inference throughput compared to the V100, indicating a speedup of roughly 1.25√ó."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave‚Äôs top choice.","The context states that the Max‚ÄëPerformance instance g6e.xlarge costs about 280% more than InferSave's top choice for the 400‚ÄØTPS SLO, so the answer is 280‚ÄØ%."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,GPUs,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'We conduct training on a cluster containing 12 nodes and 96 H100s.',"The context states that JetMoE‚Äë8B was trained on a cluster of 12 nodes with a total of 96 H100 GPUs, so the total number of GPUs used for training is 96."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: When actively training, the average GPU power is over 600W, over 85% of an H100‚Äôs maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.",The question asks for the average GPU power during the first 300 logging steps while actively training. Doc [morrison2025] states that the average GPU power is over 600W in that period.
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,is_blank,is_blank,"Doc [jegham2025#0079]: 'A single short GPT-4o query consumes 0.42 Wh (¬±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.'","The question asks for the energy consumption of a single short GPT‚Äë4o query. Doc [jegham2025#0079] explicitly states that a short query consumes 0.42 Wh, and Doc [jegham2025#0077] confirms the same estimate. Thus the answer is 0.42 Wh."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'AI contributes to the ballooning issue of electronic waste. AI‚Äôs expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'","The question asks for the total e‚Äëwaste generated worldwide in 2022 as reported in a 2025 paper. Doc [luccioni2025a] states this figure explicitly as 62 million tonnes, so that is the answer."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: ning the GPT-3 language model in Microsoft‚Äôs state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The passage from Doc [li2025b] explicitly states that the training of GPT-3 in Microsoft‚Äôs U.S. data centers directly evaporates 700,000 liters of clean freshwater, which is the requested value."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents explicitly state which GPU architecture is most energy-efficient for models generating only a single classification token, so I cannot answer confidently."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20√ó,20,times,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'Facebook‚Äôs recommendation and ranking model sizes have increased by 20 times during the same time period [11].',"The question asks for the magnitude of the increase in Facebook‚Äôs recommendation and ranking model sizes between 2019 and 2021. Doc [wu2021a] explicitly states that these model sizes increased by 20 times, so the answer is 20√ó (20 times)."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'As shown in Figure 8, OpenAI‚Äôs reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic‚Äôs Claude 3.7 Sonnet (0.825)...'","The passage from Doc [jegham2025] states that the model o3-mini has the highest cross‚Äëefficiency score of 0.884, indicating it ranked highest in the eco‚Äëefficiency analysis using DEA."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.'","The context window size is explicitly stated as 2,048 tokens in Doc [li2025a], which directly answers the question."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",True,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: Reporting the computational price tag of Ô¨Ånding, training, and running models is a key Green AI practice (see Equation 1).","The question asks whether Green AI includes reporting financial costs of model development. Doc [schwartz2019] explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice, confirming the statement is true."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a numeric total execution time in seconds for a sparse Mixtral model with batch size 1 on an NVIDIA A40-48 GB GPU.
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].'","The statement claims that smartphones currently average lifetimes of less than 3 years. Doc [wu2021b] explicitly states that the current average for cell phones is less than 3 years, directly supporting the True claim."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'Processor   Average (Watts)  StDev   %  DNNs   used to calculate average power   TPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   Search   [So19]   TPU   v3   283   10%   T5,   Meena,   Gshard,   Switch   Transformer   P100   GPU   271   11%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   Search   [So19]   V100   GPU   325   2%   Transforme'",The table shows TPU v2 averages 221‚ÄØW and V100 GPU averages 325‚ÄØW. Subtracting 221‚ÄØW from 325‚ÄØW yields a difference of 104‚ÄØWatts.
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 ‚Äì 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,Doc [li2025b]: 'a recent study suggests that the global AI could consume 85 ‚Äì 134 TWh of electricity in 2027',"The question asks for the projected electricity consumption range for global AI in 2027. Doc [li2025b] explicitly states a range of 85‚Äì134‚ÄØTWh, so that is the answer."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,True,1,is_blank,is_blank,is_blank,"Doc [erben2023#0076]: 'CV‚Äôs per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)'","The question asks whether intra‚Äëzone scaling with T4 GPUs yielded nearly linear per‚ÄëGPU speedup for CV models. Doc [erben2023#0076] states that CV‚Äôs per‚ÄëGPU speedup is almost linear, providing the supporting numeric evidence, so the correct answer is True."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a clear statement indicating that the relationship between runtime and energy consumption was found to be nearly linear.
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61‚Äì76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'However, if we exclude equity the fraction for R&D staff drops to 19‚Äì33%, and the fractions of computing hardware costs and energy rise to 61‚Äì76% and 2‚Äì7% respectively.'","The question asks for the range of computing‚Äëhardware share when equity is excluded. Doc [cottier2024] explicitly gives that share as 61‚Äì76%, so that is the answer."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",FALSE,0,is_blank,is_blank,is_blank,"Doc [khan2025#0046]: 'On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade‚Äëoff between energy efficiency and overall predictive performance.'  Doc [khan2025#0053]: 'Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.'","The question asks whether accuracy and F1 scores always improved after optimization. Both cited documents report that these metrics actually declined slightly after optimization, showing that they did not improve. Therefore the statement is false."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million packages,150,million packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: Europe ‚Ä¢ We delivered 150 million packages via EVs.,"The document states that in 2023 Amazon delivered 150 million packages via EVs in Europe, which directly answers the question about the number of packages in millions."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,is_blank,is_blank,"Doc [kim2025#0089]: time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario... The workload evaluates a total of 3000 requests.","The online inference workload processes 128 input + 512 output = 640 tokens per request, and there are 3000 requests, giving 640√ó3000 = 1,920,000 tokens in total."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Yelp sentiment analysis benchmarks or compare traditional models to large language models for that task, so the answer cannot be determined from the given context."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,Doc [strubell2019]: 'CO2e = 0.954pt',"The question asks for the EPA‚Äôs average CO2 per kWh in pounds. Doc [strubell2019] states that the EPA provides an average of 0.954‚ÄØlbs per kWh, which is used to convert power to CO2 emissions. This directly answers the question with the required unit."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Amazon AWS renewable energy coverage for the year 2018, so the answer cannot be determined from the context."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244 projects,244,projects,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: United States 244 17,706","The table of Amazon Renewable Energy Projects announced as of January 2024 lists 244 projects in the United States, so 244 is the count of US projects."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: The AI Act fails to address indirect greenhouse gas emissions from AI applications. 
Doc [ebert2024]: omits indirect emissions from AI applica- tions (e.g., those us","The question asks whether the AI Act requires providers to disclose GHG emissions of AI applications like oil and gas exploration. The cited passages from Doc [ebert2024] state that the Act does not address or omit indirect emissions from such applications, indicating no such mandate. Therefore the answer is FALSE."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"Doc [dodge2022#0081]: 'For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;'","The passage explicitly states that for DenseNet 201, a short job, the Flexible Start optimization can yield up to 80% reduction in CO2 emissions in the West US region."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,TRUE,1,is_blank,is_blank,is_blank,"Doc [chung2025#0064]: 'It can be seen that the LLM‚Äôs power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model‚Äôs power consumption is close to the maximum. This is because LLM decoding is characterized bylow compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU‚Äôs computation throughput being bottlenecked by VRAM bandwidth and results in the GPU‚Äôs computation units being unde', Doc [chung2025#0192]: 'Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.'","The documents state that LLM inference draws lower power because decoding is low compute‚Äëintensity and VRAM‚Äëbound, whereas diffusion models draw near maximum GPU power and are more compute‚Äëintensive. Thus the statement is true."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",over 1450 times,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 kWh per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9 kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The passage states that image generation consumes 2.9 kWh versus 0.002 kWh for text classification, yielding a factor of over 1450. Therefore the energy required for image generation exceeds that for text classification by more than 1450 times."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,Cumulative server level,Cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level.' 
Doc [ebert2024#0188]: 'Cumulative server energy reporting: Require energy consumption to be measured and reported at the cumulative server level.'","The question asks for the measurement level recommended for reporting AI energy consumption. Both Doc [ebert2024] and Doc [ebert2024#0188] explicitly state that the recommended level is the cumulative server level, balancing accuracy and feasibility."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the amount of fiber optic cable installed globally to support AI workloads in 2023, so the answer cannot be determined from the given context."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping,Swapping,is_blank,is_blank,is_blank,"Doc [chung2025#0177]: It can be seen that when the server is overloaded, Swapping consistently consumes less energy.","The question asks which preemption mechanism consumes less energy when an LLM inference server is overloaded. Doc [chung2025#0177] explicitly states that Swapping consistently consumes less energy under overload, so the answer is Swapping."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: ... Google reports a 48% increase in GHG emissions since 2019 ...,"The question asks for the percentage increase in GHG emissions reported by Google in its 2024 environmental report. Doc [luccioni2025a] explicitly states that Google reports a 48% increase in GHG emissions since 2019, which directly answers the question."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,800 million USD,800000000,USD,is_blank,is_blank,"Doc [cottier2024#0047]: 'we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.'","The question asks for the estimated upfront hardware acquisition cost to train GPT-4. Doc [cottier2024#0047] explicitly states that acquiring the hardware used for GPT-4 training costs $800M, which is the required value."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,GPUs,is_blank,is_blank,"Doc [griggs2024#0005]: 'For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs'","The question asks how many NVIDIA A100‚Äë80GB GPUs are needed to serve Llama2‚Äë70b at BF16 precision. Doc [griggs2024#0005] explicitly states that 2 such GPUs are required, so the answer is 2 GPUs."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,"Doc [dodge2022]: RAM0 DRAM1 Total Watts 187.1 22.9 9.3 23.0 9.3 251.6 Fraction 74% 9% 4% 9% 4% 100% Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption du","The question asks for the GPU‚Äôs share of total electricity consumption during a BERT‚Äëbase training experiment. Doc [dodge2022] explicitly states that the GPU accounts for 74% of the total energy consumption, providing the required percentage."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",26.1%,26.1,%,is_blank,is_blank,Doc [amazon2023#0963]: '16.6% 31.9%26.1%23.5%',"The table in Doc [amazon2023#0963] lists People Managers gender percentages, and the 2023 value for women is 26.1%."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5% operational energy footprint reduction,28.5,%,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).,"The question asks for the overall operational energy footprint reduction achieved at Facebook from 2019 to 2021 through iterative hardware‚Äësoftware optimization.  Doc [wu2021a] explicitly states a 28.5% reduction over the two‚Äëyear period, which directly answers the query."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific McKinsey projection for the percentage of U.S. national electricity consumption that data centers are expected to account for in 2030.
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'The umbrella term ‚ÄòSustainable AI‚Äô was initially proposed by van Wynsberghe as a Ô¨Åeld of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.'","The statement claims the term was proposed to only encompass using AI in climate-positive applications. The cited passage shows it also includes improving AI‚Äôs environmental sustainability, so the claim is false."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28 samples,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 1200 5 10 15 20 25 30 35 40 A100-40GB A100-80GB A40 H100 bsz=28 bsz=35 Projected GPU capacity Ground Truth Projection Max batch size GPU DRAM capacity Fig. 13. Projected maximum batch size of Mixtral for different GPUs.,"The figure caption in Doc [xia2024] shows the ground truth batch size for Mixtral on an NVIDIA A100-40GB GPU as 28 samples, which directly answers the question."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,about 70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.'","The passage explicitly states that JetMoE-8B reduces inference computation by about 70% relative to Llama2-7B, which directly answers the question."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: ""The seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].""","The question asks for the BERT training carbon footprint in pounds CO2e from the 2019 study. Doc [luccioni2025b] provides the exact figure of 626,155 pounds CO2e, so that is the answer."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,is_blank,is_blank,"Doc [li2025b#0046]: 'with an estimated training energy of 1287 MWh' . Doc [jegham2025#0005]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh)' .","Both documents state that the full GPT-3 model required 1,287 MWh of training energy, so the estimated training energy is 1287‚ÄØMWh."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",25%,25,%,is_blank,is_blank,Doc [dodge2022#0092]: 'doubling the duration can lead to significant savings up to about 25%.',"The question asks for the maximum potential emissions saving when using the Pause and Resume optimization for a 6B parameter transformer. Doc [dodge2022#0092] states that for the 6B transformer training run, doubling the duration can lead to savings up to about 25%, which directly answers the question."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g CO2eq,0.32,g CO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.'","The question asks for the emissions of the BERT-based model bert‚Äëbase‚Äëmultilingual‚Äëuncased‚Äësentiment per 1,000 text classification queries. Doc [luccioni2024] explicitly states that this model emits 0.32‚ÄØg CO2eq for every 1,000 queries, which directly answers the question."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",True,1,is_blank,is_blank,is_blank,"Doc [erben2023#0093]: 'rity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.'","The question asks whether intercontinental training for high‚Äëgranularity CV models slowed performance by only 7% relative to local training. Both Doc [erben2023#0093] and Doc [erben2023#0092] state that CV performance was only 7% slower in intercontinental setups, so the statement is true."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so the answer cannot be determined from the available context."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.'","The question asks for the year when direct environmental disclosures peaked. The passage from Doc [luccioni2025c] explicitly states the peak occurred in 2022, so 2022 is the correct answer."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MWh,40,MWh,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: Energy (MkWh) 1171 1066 3179 444 688 40,"The carbon footprint analysis table in Doc [li2025a] lists the total energy consumed for each model. For FLM‚Äë101B, the Energy (MkWh) entry is 40, indicating 40 MWh of energy were used during training."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",7 to 26 thousand grams,"[7, 26]",thousand grams,is_blank,is_blank,"Doc [dodge2022#0062]: '7k grams vs. 26k grams, for the most efficient vs. least efficient regions.'","The experiment reports emissions of 7k grams in the most efficient region and 26k grams in the least efficient region, giving a range of 7 to 26 thousand grams."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 'Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.'","The passage from Doc [xia2024] explicitly states that the MoE layer is the costliest component and a prime target for optimization, supporting the claim that it is often targeted when improving LLM fine-tuning performance."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about the hectares of land occupied by new AI data centers globally in 2022.
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024#0100]: 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.'
Doc [xia2024]: 'optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.'","The documents state that adding compute resources to accelerate the MoE layers reduces cost, not increases it. Therefore the claim that it can increase costs is false."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",more than $20 billion,20,billion $,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024#0002]: '... up to more than $20 billion in 2028.',"The question asks for the projected public health burden value. Doc [han2024#0002] explicitly states that the burden could reach more than $20‚ÄØbillion, which matches the answer."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.',"The statement claims GPU-level monitoring is recommended, but Doc [ebert2024] explicitly states that GPU-level or component-based tracking is discouraged and recommends cumulative server-level measurement instead, so the claim is false."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: 'finding that training accounted for only half of the model‚Äôs overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.'","The 2023 article by Luccioni et al. reports that training contributed only half of BLOOM‚Äôs total emissions, which translates to 50‚ÄØ% of the overall carbon footprint."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,32.7 USD,32.7,USD,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: TABLE IV ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL ... A40 48GB 4 1.01 0.79 32.7,"The table in Doc [xia2024] lists the estimated total cost for fine‚Äëtuning Mixtral on the GSM8K dataset with sparse MoE on an NVIDIA A40‚Äë48GB GPU as $32.7, which directly answers the question."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10‚Äì50 requests,"[10, 50]",requests,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'GPT-3 needs to ‚Äúdrink‚Äù (i.e., consume) a 500ml bottle of water for roughly 10 ‚Äì 50 medium-length responses, depending on when and where it is deployed.'",The context states that a 500‚ÄØml bottle of water is consumed by GPT‚Äë3 during approximately 10 to 50 medium‚Äëlength responses. Treating each response as one user request gives a range of 10‚Äì50 requests to consume a 500‚ÄØml bottle.
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 tCO2,8.3,tCO2,is_blank,is_blank,"Doc [dodge2022#0065]: 'one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)'","The question asks for the estimated CO2 emission for one year of average US home energy use. Both Doc [dodge2022#0065] and Doc [dodge2022#0069] state that this value is 8.3 metric tons, so the answer is 8.3 tCO2."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'GSM8k 14.5 17.3 16.9 27.8',"The table in Doc [shen2024] lists scores for multiple models across tasks. Column four, corresponding to JetMoE-8B, shows a GSM8k score of 27.8, which is higher than the other models and matches the description that JetMoE-8B performs best except on WinoGrande."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64,"The table in Doc [samsi2024] lists the bare minimum GPU count for each LLaMA size; for the 7B model the count for a V100 32GB GPU is 1, so the bare minimum number of V100 GPUs required is one."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a clear numeric value for the total execution time of a sparse Mixtral model fine-tuned on an NVIDIA A40-48GB GPU with batch size 10, so the answer cannot be derived with confidence."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a specific water usage figure for an OLMo 60M model trained on 1.7 to 5.6 trillion tokens, so the answer cannot be determined with confidence."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,28 samples,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'A40 ... bsz=28',"The figure in Doc [xia2024] shows the projected maximum batch size of Mixtral for different GPUs, and for the NVIDIA A40 (48‚ÄØGB) it lists bsz=28, indicating the maximum dense batch size on Hellaswag with that GPU."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",1 sample,1,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: ... longest-running MoE layer. The batch size of the longest-running MoE layer was 1 sample.,"The context states that for the sparse Mixtral model fine-tuned on an NVIDIA A40‚Äë48GB, the longest-running MoE layer had a batch size of 1 sample."
