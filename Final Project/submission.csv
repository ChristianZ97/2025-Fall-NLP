id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time‑saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The passage explicitly states that the growth strategy saved 72% of the training time for the 101B model, so that is the required percentage. The quoted sentence provides the exact figure and context."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 t CO2e,1.2,t,patterson2021,https://arxiv.org/pdf/2104.10350,"""single passenger round trip SF-NY is ~1.2t CO2e""","The passage from Patterson 2021 explicitly states that a single passenger round trip between San Francisco and New York emits approximately 1.2 metric‑tonnes of CO₂e, providing the required value."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,Request freq. (req / s) GPU Power Usage (kWh) … Llama 3.2 1B ∞ 0.003 … 8 0.036 … 1 0.160,The table in Morrison et al. (2025) lists GPU power usage for Llama 3.2 1B at different request rates; at 8 requests per second the value is 0.036 kWh.
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,percent,wu2021b,https://arxiv.org/pdf/2108.06738,"""total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05""","The provided passage states that U.S. data center energy consumption rose by about 4% between 2010 and 2014, which is the average increase requested."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.""","The 2023 Amazon Sustainability Report states that the Maryland‑CPV Backbone solar farm would avoid over 64,000 metric tons of CO₂e, which is equivalent to taking more than 13,900 cars off the road."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg CO2eq per GPU,463,kg CO2eq,is_blank,is_blank,"""or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.""","The documents note that NVIDIA does not release embodied carbon data, so the estimate from Luccioni et al. (3700 kg CO₂eq per 8‑GPU server node) is used, which equates to 463 kg CO₂eq per individual GPU."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24,24,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore."",","The quoted passage explicitly states that in 2023 AWS expanded the use of recycled water for cooling to 24 data centers, which directly answers the question."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"""The total permitted site-level annual emission limits are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.""","The context states that for permits issued between January 26, 2023 and December 1, 2024, the total permitted annual emission limit for NOx from data center backup generators in Virginia (which includes northern Virginia) is about 13,000 tons. This directly answers the question."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context gives premature death estimates for 2028 (approximately 1,300) but does not provide any figure or projection for 2030. Therefore, the documents do not contain enough information to answer the question with confidence."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021,https://arxiv.org/pdf/2104.10350,"""It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.""","The context from Patterson 2021 states that training GPT‑3 with 10,000 V100 GPUs operating at 24.6 TFLOPS/sec requires approximately 14.8 days, which matches the table entry for 10,000 chips showing a training time of 14.8 days."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""more than 6,750 fold improvement in processor clock speed""","The cited passage states that typical 2021 microprocessors run at 5,000,000 kHz versus the 740 kHz of the 1971 Intel 4004, yielding a speed increase of more than 6,750×."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B … under a budget of $100K"" (Doc li2025a)
""GPT-4 … largest amortized hardware and energy cost, at $40M"" (Doc cottier2024#0044)","The FLM-101B was trained with a total budget of $100,000, while the amortized training cost of GPT-4 is reported as $40 million. Dividing 40 M by 0.1 M yields a factor of 400, indicating GPT-4’s amortized cost was 400 times greater than FLM-101B’s training budget."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"""a full training run would take 60 days""","The context explicitly states that training the 6.1 billion‑parameter model for 13% of its run took 8 days, and extrapolating to full completion yields an estimated 60‑day training period."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.0,43.0,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B 28.22 43""","The table for the Open LLM Leaderboard lists the average score for FLM-101B as 43, which is the final average performance score across the four tasks (ARC, HellaSwag, MMLU, TruthfulQA)."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation...""","The context explicitly states that the study was launched in the fall of 2014, providing the precise year of launch."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling""",The quoted passage from luccioni2025a directly states that 22% of e-waste is formally collected and recycled according to the UN’s Global E-Waste Monitor 2024.
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,models,luccioni2024,https://arxiv.org/pdf/2311.16863,"""we sampled 88 models""","The Power Hungry Processing (2024) study explicitly states that 88 different machine learning models were sampled and analyzed, as shown in the cited document."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts per MoE layer,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,is_blank,"The JetMoE‑8B hyperparameter table specifies that each MoE layer contains 8 experts, and the text explicitly states that all layers use 8 experts with a top‑k of 2."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3×,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.""","The context from Wu et al. (2021) explicitly states that increasing GPU utilization to 80% reduces the overall carbon footprint by a factor of 3, which directly answers the question."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,3426,Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)""","The appendix of the 2025 study lists GPU energy consumption for various models. Command‑R Plus has the highest reported value of 3,426 Wh for 1,000 queries, as indicated in the quoted passage."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,"The context explicitly states that the English portion of the FLM‑101B model requires 28.22 zettaFLOPs for training, as part of the total 52.76 zettaFLOPs cost."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time‑saving...""","The document explicitly states that the total wall‑clock time for training FLM‑101B using the growth strategy is 21.54 days, so that is the required answer."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).""","The cited passage from Schwartz 2019 states that Red AI is on the rise, contradicting the claim that it is declining, so the statement is false."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",0.5 seconds,0.5,seconds,xia2024,https://arxiv.org/pdf/2408.04693,Dense(bsz=30) 0.5,The plot in the xia2024 context lists execution times for various batch sizes; for a dense BlackMamba model with batch size 30 on an NVIDIA A40‑48 GB GPU the corresponding time shown is 0.5 seconds. This directly provides the total execution time requested.
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,Between 1.2× and 4× (median about 2.2×),"[1.2, 4]",x,cottier2024,https://arxiv.org/pdf/2405.21015,"""a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.""  ""The ratio ranges from 1.2x to 4x, with a median of 2.2x.""","The cited passage states that the total compute used over the entire model development process is 1.2‑to‑4 times the compute required for just the final training run, with a median estimate of roughly 2.2×, directly answering the question."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 – 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"""the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027""","The cited passage from Li et al. (2025) explicitly states that global AI is projected to withdraw 4.2 to 6.6 billion cubic meters of water by 2027, which directly answers the question."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.1%,0.1,%,is_blank,is_blank,"""large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre‑training speed with the same computational resources.""","The document explicitly states that the 1500‑billion‑parameter Switch Transformer activates 0.1% of its parameters per token, which directly answers the question."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%"", ""A significant portion of machine learning model experimentation utilizes GPUs at only 30-50%""","The provided passages from the Wu 2021 paper state that most experimentation uses GPUs at 30‑50% capacity, not over 80%, so the claim is false."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.""","The cited passage states that efficiency gains can lead to higher overall consumption through Jevons’ Paradox, the economic principle that explains why technical efficiency may not yield net environmental benefits."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025;patterson2021,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2104.10350,"""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity"" (jegham2025); ""estimated training energy of 1287 MWh"" (li2025b#0046); ""energy consumption is 1287 MWh"" (patterson2021).","All three documents explicitly state that GPT‑3’s training consumed 1,287 MWh of electricity, confirming the answer."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.6 MWh,103.6,MWh,is_blank,is_blank,"""The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/..."" (dodge2022#0056). ""We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh."" (dodge2022#0054).","The first passage reports 13.8 MWh for 13 % of the full run, implying a full run would consume roughly 13.8 MWh ÷ 0.13 ≈ 106 MWh. The second passage gives a direct estimate of 103,593 kWh, which is 103.6 MWh, consistent with the scaling. Thus the estimated total energy consumption for a full 6.1‑billion‑parameter transformer training run is about 103.6 MWh."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about an experimental setup for energy‑efficient local inference in financial sentiment classification, so the specific hardware processor cannot be determined with confidence."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks""; ""AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models""","The excerpts from doc luccioni2025c explicitly name the collaborative project as ""AI Energy Score"" and state that it aims to provide a standardized method for comparing inference efficiency across AI models."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a clear numeric value for the execution time of the longest kernel in the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40‑48 GB GPU. Hence the answer cannot be determined with confidence from the documents.
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not contain a clear numeric value for the energy consumption in MWh for pre‑training the BLOOM model, so a confident answer cannot be derived from the documents."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts per token per layer,2,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"""we set the same number of experts to 8 and top-k to 2 for every layer."" (Doc [shen2024])","The JetMoE‑8B hyperparameters specify that each layer uses 8 experts, but only the top‑2 experts are selected for activation (top‑k=2) for each token. This is explicitly stated in the document and confirms the answer."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22,22,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy"" (Doc [amazon2023#0277])","The cited passage states that 22 AWS data center regions achieved 100% renewable energy matching in 2023, which directly answers the question."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.""",The document explicitly states that in 2023 Amazon held 1.3 gigawatts of energy storage capacity.
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,"2,300 transatlantic flights",2300,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""cumulative emissions from approximately 2,300 transatlantic flights between Boston and London.""","The Jegham 2025 document explicitly compares GPT‑4o’s annual carbon emissions to the cumulative emissions of about 2,300 transatlantic flights, providing the numeric count needed for the answer."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The context reports a 300,000× increase, not the 200,000× stated in the question, so the statement is false."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",44%,44,percent,chung2025,https://arxiv.org/pdf/2505.06371,"""reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.""","The cited passage from Chung et al. states that targeting a TPOT of 100 ms yields a 44 % lower energy consumption per generation compared to the latency‑minimizing configuration, directly answering the question."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,69 years,69,years,stone2022,https://arxiv.org/pdf/2211.06318,"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.""",The context states AI was officially founded in 1956. Counting from 1956 to 2025 gives an approximate age of 69 years for the field.
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].""","The passage from Doc [wu2021b] explicitly states that global carbon emissions fell by 6.4% in 2020, which is the reported drop during the COVID‑19 pandemic. The quoted sentence provides the precise figure and confirms the percentage decrease."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Training a large Transformer model with Neural Architecture Search (NAS).,Neural Architecture Search (NAS),is_blank,luccioni2025c;luccioni2023,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2302.08476,"""the emissions of training and fine‑tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO₂, similar to the lifetime emissions of five US cars"" (Doc luccioni2025c) and ""the NAS training workload represents a large‑scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload"" (Doc luccioni2023).","The 2019 Strubell et al. estimate of five cars’ CO₂ is derived from the training of a large Transformer model using Neural Architecture Search (NAS), an AI process that is infrequently performed compared to typical model training. The quoted passages directly state that NAS training produced the five‑car equivalent emissions and that this workload is rarely run."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].""","The passage from doc wu2021b states that about 770 million people lack stable electricity access, directly supporting the claim that approximately 770 million people worldwide still lack such access. Therefore the statement is true."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.""","The context explicitly states that the field was officially christened in 1956 during the Dartmouth workshop, providing the date required for the answer."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages contain a specific percentage of AI inference workloads in Asia powered by coal in 2023, so the answer cannot be determined from the supplied documents."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific value for the global average PUE of AI‑dedicated data centers in 2023. The closest figure given is the overall average PUE for all data centers (1.58), but no separate measurement for AI‑dedicated facilities is reported. Hence we cannot answer the question with confidence."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.""","The quoted passage from Morrison 2025 explicitly states that operational impacts encompass GHG emissions from servers and data center cooling, contradicting the claim that they are excluded. Therefore the statement is false."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,is_blank,is_blank,"""Llama 7B 63 Meta 356 14""","The table in doc luccioni2025c#0096 lists the pre‑training GHG emissions for the Llama 7B model as 14 tCO2e, which directly answers the question."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,2.195 L/kWh,2.195,L/kWh,is_blank,is_blank,"""total water usage effectiveness (WUE) of 1.29 liters per kWh"" and ""total WUE of 3.1 liters per kWh"" for the Jupiter and Augusta clusters respectively.","The two Google AI‑dedicated clusters, Jupiter and Augusta, have reported WUE values of 1.29 L/kWh and 3.1 L/kWh. Averaging these two figures gives (1.29+3.1)/2 = 2.195 L/kWh, which is the average WUE for Google’s AI‑dedicated data centers in 2024."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear, explicit numeric value for the energy consumption (in Wh) of the o3 model for a long prompt. Without a definitive statement or value, it is not possible to answer the question with confidence."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules per token,"[3, 4]",J,is_blank,is_blank,"For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The quoted sentence directly states that at a maximum generation length of 512 tokens, the LLaMA‑65B consumes roughly 3‑4 Joules per decoded token, giving the required energy per token value."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",25000,USD,is_blank,is_blank,"""Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.""","The document explicitly states that Grover, trained on 256 TPU chips for two weeks, incurred an estimated cost of $25,000, which directly answers the question."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.""","The context explicitly states that quantizing RM2 from 32-bit to 16-bit reduces its model size by 15%, providing the required percentage."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B,2,B,shen2024,https://arxiv.org/pdf/2404.07413,"""JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token,""","The cited passage explicitly states that out of JetMoE-8B’s 8 billion total parameters, only 2 billion are activated per input token, confirming the answer."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,percent,xia2024,https://arxiv.org/pdf/2408.04693,"""up to 53% when conducting sparse fine‑tuning with batch size = 1""","The context explicitly states that in BlackMamba sparse fine‑tuning on an NVIDIA A40‑48GB GPU, the optimizer stage accounts for up to 53% of the total running time when the batch size is 1."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,The average data center PUE in 2023 was 1.58 globally[74],"The context from Doc [ebert2024] explicitly states that the global average PUE in 2023 was 1.58. This value is dimensionless, so no unit is provided."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",True,1,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy""","The cited statement from the Patterson 2021 paper directly asserts that sparsely activated DNNs use less than one-tenth the energy of comparable dense models while maintaining accuracy, confirming the claim as true."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s exp""","The context indicates that the 5‑10 % estimate lacks clear, publicly available calculations and does not rest on sound scientific grounding, so the claim is not supported as stated."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17,17,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The seventeen-member Study Panel""","The context explicitly states that the inaugural 2015 Study Panel was a seventeen‑member panel, so the number of members is 17."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Flan‑T5‑xxl 11B 11.48 0.083"" from Table 3, showing 0.083 kWh per 1,000 queries.","The table gives 0.083 kWh for 1,000 queries. Dividing by 1,000 gives 0.000083 kWh per query. For 1 billion queries, 1 000 000 000 × 0.000083 kWh = 83,000 kWh, which is 83 MWh."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9×,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.""",The context explicitly states that the explosive growth in AI use cases led to a 2.9‑fold increase in AI training infrastructure capacity over the 1.5‑year period from 2019 to 2021. This factor directly answers the question.
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.""","The quoted passage directly states that the Study Panel found no cause for concern that AI poses an imminent threat, contradicting the claim that they are concerned about such a threat."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.""","The quoted passage directly states that manufacturing accounts for 74% of a client device’s total carbon footprint, providing the required percentage."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"""Gemini Ultra has the highest fraction of R&D staff cost at 49%""","The context states that Gemini Ultra’s R&D staff cost (including equity) accounts for 49% of the total amortized model development cost, which directly answers the question."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi‑3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.","The cited passage explicitly states that models with more parameters do not always consume more energy during inference, contradicting the claim that it always does."
q080,True or False: The AlphaGo program defeated the human Go champion.,TRUE,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""AlphaGo program that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.""","The cited passage from Doc stone2022 explicitly states that AlphaGo defeated the human Go champion, confirming the statement as true."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Meta’s Llama 3 family of models emitted 11,390 tCO₂e in pre‑training, which is more than 40 × the ‘five cars’ estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Meta reports that their Llama 3 family of models emitted 11,390 tons CO₂e or over 40× the ‘five cars’ estimate.","The context states that Meta’s pre‑training emissions for the Llama 3 family were 11,390 tCO₂e, and that this figure is over 40 times the lifetime emissions of five cars, directly answering the question."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement indicating the number of wind turbines that Microsoft directly contracted to power Azure AI clusters in 2023. Therefore, the answer cannot be determined with confidence from the given documents."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement of the maximum batch size for BlackMamba sparse fine‑tuning on the GSM8K dataset with an NVIDIA A40 GPU. The tables given list values for CS and MATH datasets but not for GSM8K, so the requested number cannot be determined from the documents."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear numeric value for the Earth–Sun distance, so a confident answer cannot be derived from the documents."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"""The entire alignment process takes 60 H100 GPU hours.""",The document states that the combined dSFT and dDPO fine‑tuning (the entire alignment process) required 60 H100 GPU hours.
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous Batching,Continuous Batching,is_blank,is_blank,is_blank,"""Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).""","The context states that continuous batching reduces idle GPU time by dynamically replacing completed requests with new ones, directly supporting the answer. "
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit statement or estimate of the CO2 emissions from OpenAI’s API requests in January 2024, so the answer cannot be determined from the documents alone."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams",1594,grams,luccioni2024,https://arxiv.org/pdf/2311.16863,"""the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences""","The provided passage directly states that the stable‑diffusion‑xl‑base‑1.0 model emits 1,594 g CO2eq per 1,000 inferences, which is the required value."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)""","The 2025 study’s appendix reports that across the listed models, GPU energy for 1,000 inference queries ranges from 0.06 Wh (bert-tiny) up to 3,426 Wh (Command‑R Plus)."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,is_blank,is_blank,"""Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.""",The context explicitly states that Hivemind is a PyTorch-based decentralized framework used for distributed spot instance training across clouds and continents.
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg of CO2e/KWh,0.429,kg CO2e/KWh,patterson2021,https://arxiv.org/pdf/2104.10350,"""The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO 2 e/KWh [USE21].""","The context from Patterson 2021 explicitly states that the U.S. average energy mix has a gross carbon intensity of 0.429 kg CO₂e per kWh, which directly answers the question."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about classification experiments on German public administration texts, sentence embeddings, or accuracy results for specific models. Therefore, I cannot identify which model achieved the highest accuracy."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems""","The context states that the expanded form of transparency proposed by Ehsan et al. is called ""social transparency"", which explicitly includes socio‑technical aspects and the societal/environmental footprint of AI systems."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,is_blank,is_blank,"""Lamina, an LLM inference system that incorporates model-attention disaggregation"" (Doc [chen2024#0003]), ""We develop and evaluate Lamina, a distributed heterogeneous LLM inference system with model-attention disaggregation"" (Doc [chen2024#0023]), ""We build Lamina, a distributed heterogeneous LLM decoding system that implements model-attention disaggregation"" (Doc [chen2024#0055])","The quoted passages explicitly name the system as Lamina and describe it as an LLM inference system that uses model-attention disaggregation, directly answering the question."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)"" (luccioni2025b#0134).  ""Given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines."" (luccioni2025b).","The excerpts explicitly state that researchers do not believe a universal, one‑size‑fits‑all approach to AI ethics and sustainability can be developed, contradicting the claim."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"""Total Price($) 100 TPS InferSave-1st g4dn.xlarge 2.13 Max-Perf.,InferSave(w/o KV) g6e.xlarge 2.699""",The table shows the total price for InferSave’s first choice (g4dn.xlarge) as $2.13 and for Max‑Performance (g6e.xlarge) as $2.699. The percentage increase is ((2.699-2.13)/2.13)*100 ≈ 26.7 %. This matches the question’s scenario for the 100 TPS offline workload.
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,is_blank,is_blank,"""Half of amortized hardware CapEx + energy cost is for AI accelerator chips. Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.""","The provided passage directly states that on average 44% of the amortized hardware CapEx plus energy cost is attributed to AI accelerator chips, supporting the answer."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"""the company’s data center water consumption increased by ∼20% from 2021 to 2022"" (Doc li2025b) and ""Google observed a 20% uptick in the same period"" (Doc luccioni2025a)","Both documents explicitly state that Google’s data center water consumption rose by about 20% between 2021 and 2022, which is the requested percentage increase."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,101 4 4 12 192 2160 165 52.88%,"The table for the 101B growth stage lists a FLOPs utilization of 52.88%, indicating the achieved utilization in the final stage of FLM‑101B training."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency""","The table in the Khan 2025 document lists a metric named ""Carbon Intensity"" with the definition ""CO₂ emissions per unit of electricity consumed"", matching the requested metric."
q093,How many parameters does the largest T5 model have?,11 billion parameters,11,B,patterson2021,https://arxiv.org/pdf/2104.10350,"The largest size has 11B parameters, and training used 86 MWh.","The passage explicitly states that the largest T5 model contains 11 billion parameters, which directly answers the question."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5 billion liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""3.5B Liters of water returned to communities from replenishment projects in 2023""","The report explicitly states that 3.5 billion liters of water were returned to communities through replenishment projects in 2023, which is the value requested."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810×,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.""","The cited passage explicitly states that full‑stack optimization—including platform‑level caching, GPU acceleration, and algorithmic changes—reduces the operational carbon footprint by a factor of 810 compared to a CPU server baseline."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",TRUE,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.,"The quoted passage directly states that custom tags reduce energy consumption for zero-shot, one-shot, and few-shot source code completion, confirming the claim is true."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3.7,million GPUs,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023)""","The 2025 paper (Doc luccioni2025a) explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, providing the numeric figure requested."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.64,0.64,is_blank,is_blank,is_blank,"""36% slower for NLP compared to the A-4 runs""","The document reports that when training NLP models across four continents (C-4) the throughput is 36% slower than the fully local run (A-4). Therefore, the fraction of local throughput achieved is 1 – 0.36 = 0.64, or 64 % of the local throughput."
q094,What is the total number of parameters in the JetMoE-8B model?,8B,8,B,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token","The context explicitly states the total number of parameters is 8B, as shown in the description and the table of JetMoE-8B model specifications."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40,USD,cottier2024,https://arxiv.org/pdf/2405.21015,The most expensive publicly announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.,The cited passage explicitly states that the amortized hardware and energy cost for GPT‑4 is $40 million. This numeric figure directly answers the question about the estimated amortized training cost for GPT‑4.
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 µg/m³,9,µg/m³,han2024,https://arxiv.org/pdf/2412.06288,"""set an annual average limit of 9µg/m 3""",The EPA’s tightened primary standard for the annual average PM2.5 concentration is stated as 9 µg/m³ in the provided document excerpts.
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,USD,cottier2024,https://arxiv.org/pdf/2405.21015,We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.,"The context explicitly states that Gemini Ultra’s estimated amortized training cost is $30M, which is the value required for the answer."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided context passages contain any information about the energy consumption of the DS Llama 70B model on the FKTG dataset.
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems""","The quoted sentence from Doc luccioni2025b explicitly names the Finnish project as ""ETAIROS"", confirming the acronym requested."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e‑readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].""","The quoted LCA explicitly states that 115 physical print books produce the same CO2 emissions as one Amazon Kindle e‑reader, providing a direct numeric equivalence."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,"Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient…; The PUE of Facebook datacenters is 1.10 (2020).","Both documents explicitly state that Facebook’s data centers have a PUE of 1.10, so the answer is 1.10."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",6.1 billion parameters,6.1,billion,is_blank,is_blank,"""over 6.1 billion parameters""","The 2022 Dodge et al. paper states that the large language model they studied comprised over 6.1 billion parameters, which is the total parameter count reported."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,image generation 2.907 3.31,"Table 2 lists the mean energy consumption per 1,000 inferences for each task; the row for image generation shows a mean of 2.907 kWh, which is the requested average."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 lbs",36156,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Table 1 in Strubell et al. lists ""American life, avg, 1 year 36,156"" as the estimated CO2e in pounds.","The table explicitly states that the average American life emits 36,156 pounds of CO2e per year, which directly answers the question."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200 times,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""per-household health burden could be 200x more than that in less-impacted communities."" (Doc [han2024#0003]) ; ""per-household impacts potentially up to 200 times higher than in less-affected areas."" (Doc [han2024])","The cited passages state that disadvantaged communities can experience a per‑household health burden from air pollutants that is roughly 200 times higher than in less‑impacted communities, supporting the factor of 200."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""the Act’s provisions lack sufficient emphasis on environmental factors.""","The context states that while the AI Act mandates risk assessments for GPAI models with systemic risk, it does not require those assessments to include environmental risks; rather, it notes a lack of emphasis on environmental factors. Therefore, the claim that the Act requires environmental risks to be included is false."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,luccioni2025b;luccioni2025a;jegham2025,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2505.09598,"""This phenomenon is referred to as Jevons paradox, which observes that when technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use"" (Doc [luccioni2025b#0124]). ""As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox"" (Doc [jegham2025]).","The cited passages explicitly name Jevons Paradox as the phenomenon where efficiency gains lead to higher usage and overall resource consumption, matching the question description."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,ebert2024,https://arxiv.org/pdf/2410.06681,"""Training energy (kWh) 51,686  …  Finetuning energy (kWh) 7,571""","The Power Hungry Processing study reports that BLOOMz‑7B required 51,686 kWh for training and an additional 7,571 kWh for fine‑tuning. Adding these two values gives the combined energy cost of 59,257 kWh."
q125,What is the total number of parameters in the final FLM-101B model?,101 billion,101,billion,li2025a,https://arxiv.org/pdf/2309.03852,Params 175B 280B 540B 130B 70B 101B,"The table in doc li2025a lists the parameters for FLM-101B as 101 billion, confirming the total number of parameters in the final model."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,is_blank,is_blank,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy""",The quotation explicitly states the total energy consumed for all experimentation and evaluation in the 2024 study as 754.66 kWh.
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not contain any mention of a dataset name for German nuclear waste site objection texts, so the answer cannot be determined from the documents."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,approximately 3 passengers,3,passengers,patterson2021,https://arxiv.org/pdf/2104.10350,"Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that a single passenger round trip SF‑NY emits about 1.2 tCO₂e, and the Evolved Transformer NAS emits 3.2 tCO₂e; dividing 3.2 by 1.2 yields roughly 3 passengers, which is explicitly mentioned in the document."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific figure for freshwater consumption by Meta’s Llama 3 inference serving clusters in 2024, only general data center water usage statistics. Since the required number is not present, we cannot answer with confidence."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","204,592,592 inferences",204592592,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","Table 5 lists the number of inferences required for each BLOOMz model to reach the training‑plus‑fine‑tuning energy cost. For BLOOMz‑7B the parity value is 204,592,592 inferences, which is the required number for deployment energy to equal the initial training and fine‑tuning energy."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.015,0.015,is_blank,is_blank,is_blank,"""tral-small 0.73 0.70 0.69 0.70 0.015""","The table for Mistral‑small lists the emissions multiplier after optimization as 0.015, showing a 98.5 % reduction relative to the baseline. The quoted row directly provides the multiplier value."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents contain a figure or statement indicating the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. Therefore the answer cannot be derived with confidence from the provided information.
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear numeric energy consumption for a single Meena training run that can be compared to the 1,287 MWh reported for GPT‑3. Therefore the ratio cannot be determined from the given information."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,is_blank,is_blank,"""If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO₂ (depending on the region it w…""","The documents state that a full training run of the 6.1 billion‑parameter transformer would emit between 21 and 78 metric tons of CO₂, giving the required range."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain a clear numeric value for the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. Therefore, a confident answer cannot be derived from the documents."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed""","The context states that, based on May 2025 OpenRouter data, 84 % of token usage occurred through models that did not disclose environmental impact, directly supporting the answer."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",$4.63/hr,4.63,USD/hr,chen2024,https://arxiv.org/pdf/2405.01814,"Table 1: H100, H20, and TPU v6e specifications. Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The table from Chen et al. (2025) lists the price per chip for the NVIDIA H20 as $4.63 per hour, directly answering the question."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,"""the U.S. data centers have already resulted in a total public health cost of about $6.7 billion ... This is equivalent to approximately 44% of the data centers’ total electricity cost.""","The passage states that the 2023 public health cost equals about 44% of the total electricity cost, giving the required percentage."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","Marshall County, West Virginia",Marshall County,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""WV Marshall 1083.8(831.2, 1336.5) 0.77""","The table lists per‑household health costs for West Virginia counties; Marshall County has the highest value (1083.8), exceeding other WV counties such as Taylor (1052.5), Brooke (918.8), and Jackson (871.9)."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only,"The quoted passage directly states a 24% cost saving when the hybrid 2 A100 + 1 A10G configuration is used compared to an A100‑only strategy, providing the required percentage."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""most carbon foot print analyses gather the information manually by writing to authors.""","The cited passage states that most carbon‑footprint analyses are performed manually by contacting authors, indicating that automatic collection without author contact is not the norm, so the claim is false."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64""","The table in the paper lists the bare minimum GPU count for each LLaMA variant. For the 7B model on A100 80GB GPUs, the required count is 1, indicating that a single GPU is sufficient for inference without compression or quantization."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,13B 2 64 1 64,"The baseline table (Table II) lists the bare minimum hardware for each LLaMA size. For the 13B model, the entry for A100 80 GB shows a count of 1, indicating that only one A100 GPU is required when no compression or quantization is applied."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25 trillion tokens,1250000000000,tokens,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context explicitly states that JetMoE-8B was trained on 1.25 trillion (1.25T) tokens, which is the number of tokens used for pre‑training."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,United Kingdom 36 901,The renewable energy projects table lists 36 projects announced in the United Kingdom as of January 2024.
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,is_blank,is_blank,Apple reports that its supply chain accounts for 99% of its total water footprint [23].,"The context directly states that Apple’s supply chain accounts for 99% of the company’s total water footprint, giving the required percentage."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Approximately $3.33 per H100 GPU-hour,3.33,$/hour,shen2024,https://arxiv.org/pdf/2404.07413,"""JetMoE-8B, a new LLM trained with less than$0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.""","The training budget of <$100,000 divided by 30,000 GPU hours yields an approximate cost of <$3.33 per H100 GPU-hour, so the estimate is about $3.33 per hour."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain a specific percentage of health cost relative to electricity cost for training a Llama-3.1 model in Altoona, Iowa, so the answer cannot be determined with confidence."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,is_blank,is_blank,"""Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization""","The quoted passage from the context states that sustainable deployment techniques, specifically quantization, can achieve up to a 45% reduction in carbon emissions, confirming the statement as true."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,is_blank,is_blank,"""we introduce thegranularity metric, the ratio of calculation to communication time""","The cited passage defines the granularity metric as the ratio of calculation to communication time, making it the metric introduced for assessing scalability in geo‑distributed training."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,answers,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""were only able to collect 95 answers""","The cited document states that after contacting over 500 authors, the researchers were only able to collect 95 answers, which directly answers the question."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"""As its core activity , the Standing Committee that oversees the One Hundred Y ear Study forms a Study Panel every five years to assess the current state of AI.""","The document states that the Standing Committee forms a Study Panel every five years, so the frequency is five years."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses.""","The quoted sentence from Doc li2025b defines the term ""Water withdrawal"" as freshwater extracted from ground or surface sources, either temporarily or permanently, for various uses, which directly answers the question."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]""","The context from doc wu2021b states that in 2021 the average U.S. household had 25 connected devices, which directly answers the question."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year""","The document states the deal could add up to 640 percent more emissions, which is 6.4 times the company’s yearly carbon removal target."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific total execution time for a sparse BlackMamba model fine‑tuned on an NVIDIA A40‑48GB with a batch size of 84, so the answer cannot be derived with confidence."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8–3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)""","The quoted passage from the publicly available study directly states the lowest and highest energy consumptions reported for pre‑training LLMs, giving a clear range of 0.8 to 3,500 MWh."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,percent,griggs2024,https://arxiv.org/pdf/2404.14527,"""Mélange reduces deployment costs by up to 77% in conversational settings""","The cited passage from the Griggs et al. 2024 paper explicitly states that Mélange can reduce deployment costs by up to 77% in conversational chat settings, which directly answers the question."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].  GPT-3 needs to “drink” a 500ml bottle of water for roughly 10 – 50 medium-length responses,"Both documents state that approximately 10 to 50 queries to GPT‑3 consume about 0.5 L of water, so the answer is a range of 10–50 queries."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80–90%,"[80, 90]",%,is_blank,is_blank,"For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].",The quoted sentence from Doc patterson2021#0020 explicitly states that NVIDIA estimated the inference portion of the ML workload to be between 80 % and 90 % in 2019.
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000",10000,round trips,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.""","Both cited documents state that training a Llama‑3.1‑scale model emits air pollutants equal to over 10,000 LA‑NYC round trips, directly answering the question."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10-50,"[10, 50]",completions,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed."",","The context states that one 500 mL bottle of water allows GPT‑3 to generate about 10 to 50 medium‑length completions, so the range of possible completions is 10–50."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,JetMoE 8B-chat 6.681 Llama-2-13b-chat 6.650,"The MT‑Bench table in the Shen 2024 document lists JetMoE‑8B‑chat with a score of 6.681, which is higher than the 6.650 score for Llama‑2‑13b‑chat after alignment, confirming the model’s superior performance."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024;rubei2025,https://arxiv.org/pdf/2310.03003;https://arxiv.org/pdf/2501.05899,"""8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.""",Both documents state that the bare‑minimum configuration for running LLaMA‑65B without compression or quantization is 4 A100 GPUs with 80 GB of memory each. The quoted sentence directly provides the required number of GPUs.
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",25.6%,25.6,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon Representation by the Numbers 36.8%29.8% 8.8% 25.6% 37.4% 43.1%8.9%9.7%""","The table in the Amazon 2023 report lists the percentages for global and U.S. workforce gender distribution. The second group of three percentages corresponds to the U.S. workforce, where 25.6% represents men. Hence, 25.6% of Amazon’s U.S. workforce identified as men in 2023."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011""","The quoted passage explicitly states that Watson beat human contenders, contradicting the claim that it did NOT beat them, so the statement is false."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement of the total CO2 equivalent emissions for the entire 'Power Hungry Processing' 2024 study, nor does it provide the necessary conversion factors to compute it. Therefore the answer cannot be determined with confidence from the given documents."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","103,593 kWh for the full training run of the 6.1‑B parameter model; approximately 2.0 billion inferences of BLOOMz‑7B are required to match that energy cost.","[103593, 2000000000]",kWh,is_blank,is_blank,"""The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately 103,593 kWh."" (dodge2022#0056) – and from luccioni2024#0085: ""Inference energy (kWh) for BLOOMz‑7B: 5.4 × 10−5 kWh per inference.""","The first quote provides the full‑run training energy of 103,593 kWh. Dividing this by the per‑inference energy of 5.4 × 10−5 kWh yields roughly 2.0 billion inferences needed to match the training energy cost."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,FALSE,0,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"""Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time."" (Doc Chung 2025#0055).  ""The TDP-based approach is often used in practice when estimating the carbon emissions of AI model training and it remains a fair approximation of the actual energy consumption of many hardware models."" (Doc Luccioni 2023).","The first quote shows that TDP-based estimates overestimate real consumption, while the second indicates it is only a rough approximation. Together they demonstrate that TDP is not a reliable or accurate method for estimating GPU energy use."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes."" and ""GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s."",","The provided passages state that GPT‑4o mini uses more energy per query—20% more on long queries and 3.098 Wh versus 2.875 Wh for GPT‑4o—showing that the mini variant consumes more, not less, energy."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.516 per hour,7.516,$/h,griggs2024,https://arxiv.org/pdf/2404.14527,"(4.69/2.29) × 3.67 = $7.516 for H100.  In Table 1, the On‑demand Price ($/h) for H100 is listed as 7.5164.","The text explicitly states the normalized on‑demand hourly price for the H100 GPU is $7.516, and the table confirms the value (7.5164). Thus the answer is the hourly price of $7.516 per hour."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","1,000× larger",1000,is_blank,is_blank,is_blank,"""to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.""","The document explicitly states that achieving a BLEU score rise from 5 to 40 for GPT‑3 translation requires a model 1,000 times larger, so the answer is a factor of 1,000."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give the CO2 emission for training a Transformer model with neural architecture search (626,155 lbs) but do not provide the emissions‑to‑driving‑distance ratio needed to calculate the equivalent driving distance in miles. Therefore, the answer cannot be derived with confidence from the available information."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE‑8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maximum learning rate of 5e-4 and a batch size of 4M t… using 1.25T tokens from carefully mixed open‑source corpora and 30,000 H100 GPU hours.","The JetMoE‑8B pre‑training was explicitly described as using 30,000 H100 GPU hours, so that is the number of GPU hours consumed."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Approximately $7.22 per hour,7.22,$,griggs2024,https://arxiv.org/pdf/2404.14527,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.""","The context states a monthly on‑demand cost of over $5,200 for the two A100 GPUs. Dividing this monthly cost by the number of hours in 30 days (30×24=720) yields an hourly cost of about $5,200/720 ≈ $7.22, which is the estimated cost per hour to run the model."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include any explicit figure for the amount of water used for cooling during OpenAI’s GPT‑4 training run, so the answer cannot be determined from the documents given."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].,"The document explicitly states that OpenAI reported 3.14 × 10²³ floating‑point operations for GPT‑3 training, which is the requested total number."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","$1,000,000,000",1000000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027""","The quoted passage from the cited document explicitly states that, under the current growth trend, the most expensive training runs will exceed one billion dollars by 2027, which is the cost threshold requested."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, ... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The 2025 paper explicitly states that after the 2022 peak, the trend of direct environmental disclosure reversed and declined, not increased."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8,8,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""8 V100 GPUs each with 32 GB of RAM"" (rubei2025#0055) and ""at a minimum, 8 V100 GPUs each with 32 GB of RAM"" (samsi2024).","Both documents state that the bare minimum hardware required to run LLaMA‑65B inference without compression or quantization is 8 NVIDIA V100 GPUs, each with 32 GB of memory. Hence the minimal number is 8."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.""","The document states that FLM-101B was trained on 24 servers, each containing 8 A800 GPUs. Multiplying 24 servers by 8 GPUs per server gives a total of 192 GPUs."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied passages provide a numeric estimate of the gallons of water consumed per ChatGPT user session for the year 2023.
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,This implementation of the model uses Pytorch and the FairScale [20] library to enable model sharding across multiple GPUs and nodes.,"The context states that the LLaMA model implementation relies on FairScale to shard the model across several GPUs and nodes, indicating that FairScale is the framework used for deployment across multiple GPUs and nodes."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied passages mention AlexNet and its top-1 accuracy on ImageNet but do not provide a numeric value. Therefore the documents do not contain enough information to answer the question with confidence.
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"""requiring around 25,000 GPU hours to train.""","The context from Schwartz 2019 explicitly states that FAIR’s RoBERTa, trained on 160 GB of text, required approximately 25,000 GPU hours, which directly answers the question."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emissions estimate for NAS training (284 metric tons or 626,155 lbs) and its equivalence to five U.S. cars’ lifetimes, but they do not give a direct total CO2 emission for an average American lifetime. Without that figure, the second part of the question cannot be answered confidently from the supplied context."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons",47400,metric tons,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""...and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.""","The documents state that Amazon’s on‑site solar energy systems avoid approximately 47,400 metric tons of CO₂e annually when compared with nonrenewable electricity sources. The figure is directly quoted in the sustainability report passages provided."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,homes,is_blank,is_blank,"Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes","The passage explicitly states that 700 million daily GPT‑4o queries amount to an annual electricity use equivalent to 35,000 U.S. residential households, providing the required numeric answer."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Inference energy (kWh) 1.0 × 10−4 for BLOOMz-7B (Table 5) and the model was downloaded 606,096 times as of Nov 2023 (Doc [luccioni2024] – download count).","The inference energy per inference is 1.0×10⁻⁴ kWh. One million inferences consume 1.0×10⁻⁴ kWh×1,000,000 = 100 kWh per download. Multiplying by 606,096 downloads gives 60,609,600 kWh, which is 60,609.6 MWh."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Approximately 16 zettaFLOPs,16.1,zettaFLOPs,li2025a;cottier2024,https://arxiv.org/pdf/2309.03852;https://arxiv.org/pdf/2405.21015,"""The 101B stage processed 26.54B tokens"" (Doc li2025a) and ""compute = 6× parameters × tokens"" (Doc cottier2024).","The 101B model has 101B parameters. Using the formula compute = 6 × params × tokens, the total FLOPs equal 6 × 101 × 26.54 ≈ 1.607×10^4 (B^2). Converting 10^18 FLOPs to 1 zettaFLOP gives ≈16.1 zettaFLOPs for the final stage."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit comparison of traditional models and large language models on Yelp sentiment analysis benchmarks, so a confident answer cannot be derived from the documents."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",14.9,14.9,queries/sec,is_blank,is_blank,"""14.9 Dense(bsz=1) Dense(bsz=6) Sparse(bsz=1) Sparse(bsz=6)"" from Doc [xia2024]","The figure and accompanying text report a ground‑truth throughput of 14.9 queries per second for the dense Mixtral‑CS model running on an A100‑40GB GPU with a batch size of 1. The quoted text directly gives this value, indicating the exact throughput for the requested configuration."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons""",The quoted sentence from the 2025 paper explicitly states that Microsoft’s global water consumption rose by 34% from 2021 to 2022.
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion GPT-4o queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"""yielding a total of approximately 772 billion GPT-4o queries in 2025, ...""",The cited passage from the Jegham 2025 study explicitly states that the total estimated number of GPT‑4o queries for 2025 is about 772 billion.
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"The US national datacenter average in 2018 was 1.58, which is the value used ; In 2020, it was 1.59.","The passage from Patterson2021 explicitly states that the US national datacenter average PUE in 2020 was 1.59, providing the required numeric answer."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,is_blank,is_blank,"When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The provided passage explicitly states that for the OPT-2.7B model at batch size 32 the KV Cache size is 5.312GB, so that is the answer."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear statement of the number of AI training runs conducted globally on renewable-only power in 2022.
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29%–49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,R&D staff costs including equity are between 29% and 49% of the total amortized cost.,"The quoted passage from Cottier et al. states that for the four models (GPT‑3, OPT‑175B, GPT‑4, Gemini Ultra) the R&D staff costs (including equity) comprise 29–49% of the total amortized cost, which directly provides the requested percentage range."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].""","The passage states that open‑source general‑purpose AI models are exempt from transparency requirements unless they pose systemic risk, which directly supports the statement that they are fully exempt unless a systemic risk exists."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),CTCF,is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"""InferSave proposes the Compute Time Calibration Function (CTCF) to accurately determine the optimal instance based on user requirements.""",The quoted sentence directly names the Compute Time Calibration Function (CTCF) as the function proposed to adjust theoretical GPU performance for accurate instance selection.
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,%,shen2024,https://arxiv.org/pdf/2404.07413,"""OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0""","The fourth value in the OpenLLM Leaderboard average scores corresponds to JetMoE‑8B, giving a final average score of 53.0."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.""",The quoted sentence from doc patterson2021 directly states that the Power Usage Effectiveness (PUE) of Google’s Iowa datacenter during the Evolved Transformer run was 1.11. This provides the required numeric value with no additional units.
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,53% of articles cite the figure of 3 Wh per ChatGPT query or claim it con,The analysis of 100 news articles found that 53% of them referenced the contested 3 Wh estimate (or the claim that a query uses 10 times more energy than a Google search).
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear statement of the factor by which energy consumption increased when deploying Llama 3.1 70B on two nodes versus one node.
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""We see that increasing the number of shards still tends to increase the energy costs of inference per response most overall...""","The cited passage from the Samsi et al. 2024 document explicitly states that adding more GPU shards increases the energy cost per response for the LLaMA‑65B model, supporting a True answer."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a;wu2021b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2108.06738,"""In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide"" (luccioni2025a). ""In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally"" (wu2021b).","Both documents state that in 2020 the four companies accounted for about 30% of all corporate PPAs, supporting the 30% answer."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The cited figure from Wu et al. (2021) explicitly states that GPU theoretical performance per watt doubles every 3–4 years, confirming the claim in the question."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,is_blank,is_blank,"""used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference""","The quoted sentence explicitly states that the Code Carbon package was employed to measure energy consumption during inference runs, providing direct evidence for the answer."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 metric tons of CO2e,26,tCO2e,li2025a,https://arxiv.org/pdf/2309.03852,net tCO2e 552 380 271 257 291 26,"The table in Doc [li2025a] lists the net CO2 emissions for FLM-101B as 26 metric tons of CO2 equivalent, which is the requested total estimated net carbon emissions for its pre‑training."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4].","The passage states that open‑source general‑purpose AI models are excluded from the transparency obligations, meaning they do not have to report energy consumption to authorities under current EU rules."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"We apply quantization through Ollama [19], an open‑source platform known for its support of edge computing principles and privacy‑centric deployments.","The passage explicitly states that 4‑bit quantization was performed using Ollama, an open‑source tool that also facilitates local deployment of large language models."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"""total public health cost of about $6.7 billion, or $47.5 per household, in 2023.""","The passage states that in 2023, the total public health cost of U.S. data centers was approximately $6.7 billion, based on the average attribution method used in the analysis."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.000022 kL,2.2e-05,kL,morrison2025;morrison2025,https://arxiv.org/pdf/2503.05804;https://arxiv.org/pdf/2503.05804,"""Mining 1 kg of rare earth materials consumes about 11 kL of water"" and ""one 12‑inch silicon wafer weighs 125 grams and produces about 63 H100s.""","From the context, 11 kL of water is required per kg of rare earth. A wafer of 125 g yields 63 H100s, giving ~2 g of material per H100. 0.1% of 2 g is 0.002 g (2×10⁻⁶ kg) of rare earth per GPU, so the water consumption is 11 kL × 2×10⁻⁶ = 2.2×10⁻⁵ kL. This matches the quoted data."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,is_blank,is_blank,"The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].","The supporting quote explicitly states that the public health impact of AI is highly unevenly distributed, contradicting the claim that it is evenly distributed across communities."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""",The quoted passage directly names Senator Edward J. Markey as the introducer of the bill in February 2024.
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts give energy consumption for GPT-4.1 nano (0.827 Wh) and for other models such as LLaMA-3.1‑8B (0.443 Wh), but they do not provide the energy consumption value for the o3 model. Without that numeric value, the factor cannot be computed with confidence."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,4 years,4,years,is_blank,is_blank,"""me a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq ...""","The Morrison 2025 study explicitly states that the assumed average lifespan of GPUs in AI data centers is 4 years, which directly answers the question."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,2,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"Model Size V100 32GB A100 80GB
Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64","The table lists the bare minimum hardware required for each LLaMA variant. For 13B, the count of V100 GPUs is 2, indicating that two 32‑GB V100 GPUs are needed for inference without compression or quantization."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context includes a figure reference (Fig. 4) for execution time breakdown of Mixtral, but it does not give a numeric value for the total execution time of a sparse Mixtral model with batch size 1 on an NVIDIA A40-48GB GPU. Without a concrete number in the text or figure description, the answer cannot be determined with confidence from the documents alone."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",9-77%,"[9, 77]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Mélange reduces costs by 9-77% for short-context tasks (interactive chats)""","The document states that for short-context tasks, which correspond to the Arena dataset, Mélange achieves a cost reduction ranging from 9% to 77% compared to single‑GPU baselines, directly answering the question."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement that the relationship between runtime and energy consumption was found to be nearly linear. Therefore, I cannot provide a confident answer based solely on the documents given."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The pre‑training of Google’s Gemma family emitted about 1,247.6 tCO₂e, which is roughly 4‑plus times the “five‑cars” benchmark (≈284 tCO₂e).",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the “five cars” number."" ""The Strubell et al. estimate of 284 metric tons CO₂e for a large Transformer model is similar to the lifetime emissions of five US cars.""","The quoted passage gives the Gemma pre‑training emissions (1247.61 tCO₂e) and states it is over four times the five‑cars estimate. The earlier Strubell figure of 284 tCO₂e represents the five‑cars benchmark, so Gemma’s emissions are about 4.4 times larger."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"""the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.""","The context explicitly states that the U.S. national average water consumption for electricity generation is 3.1 L/kWh, providing a clear numeric value for the requested metric."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,strubell2019,https://arxiv.org/pdf/1906.02243,"""Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).""","The document explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for a duration of 2 weeks, equivalent to 336 hours. Although the question refers to the GTX 1080 Ti, the same training configuration and duration are implied, as the document notes ELMo was trained on 3 GTX 1080 Ti GPUs in the same context."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",$11.06/hr,11.06,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,Table 1: H100 … Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr,"The table in Chen et al. lists the hourly price per H100 chip as $11.06/hr, which directly answers the question."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,"""Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021"" and ""has improved from 1.21 (2008) to 1.10 (2021)""",Both documents state that Google's hyperscale data centers reported a PUE of 1.10 in 2021.
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,shen2024,https://arxiv.org/pdf/2404.07413,"""Conduct training on a cluster containing 12 nodes and 96 H100s.""","The document states that JetMoE‑8B was trained on a cluster of 12 nodes with a total of 96 H100 GPUs, so the total number of GPUs used is 96."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 lbs",11023,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"""Human life, avg, 1 year 11,023""","The Strubell2019 table lists the estimated CO2e emissions for an average human life over one year as 11,023 pounds, which matches the question’s request for a global average."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50–70%,"[50, 70]",%,is_blank,is_blank,"GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].","The cited passage from Chung et al. (2025) explicitly states that GPUs account for 50–70% of the total provisioned power in a typical datacenter, providing the required percentage range."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,percent,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The cited Amazon 2023 report states that customers can expect up to a 96% reduction in carbon footprint when migrating workloads from on‑premises data centers to AWS in North America, providing the numeric value for the answer."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,"""our model predicted that fine‑tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460."" ""the most cost‑effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.""","Both documents explicitly state that the net cost for fine‑tuning a sparse Mixtral model on 2 M queries with an NVIDIA H100 GPU is $3460. The quoted sentences provide the precise figure and context, justifying the answer."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"""When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum."", from Figure 2 description in Morrison 2025","The context explicitly states that during active training the average GPU power exceeds 600 W. That value is quoted directly, so the answer is over 600 W, represented numerically as 600 W for the average power."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25 times,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The passage states that the 13B model’s inference throughput on A100 GPUs is 1.25 times higher than on V100 GPUs, indicating a 1.25× speedup."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)""","The document explicitly states that a single short GPT‑4o query consumes 0.42 Wh, which directly answers the question."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",True,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing transparency, price tags are baselines that other researchers could improve on.""","The passage states that Green AI practice includes reporting the financial cost (price tag) of developing, training, and running models, directly supporting the claim that Green AI involves providing the financial cost of these activities."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"""Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice."",","The context explicitly states that the Max-Performance instance costs about 280% more than InferSave’s top choice for the 400 TPS SLO, giving the requested percentage difference."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""AI’s expanding operational footprint also contributes to electronic waste (e‑waste), which is now the fastest‑growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.""","The 2025 paper (luccioni2025a) reports that worldwide e‑waste reached 62 million tonnes in 2022, which directly answers the question."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,SpotLake,SpotLake,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"[24] Sungjae Lee, Jaeil Hwang, and Kyungyong Lee. 2022. SpotLake: Diverse Spot Instance Dataset Archive Service.",The document cites SpotLake as a dataset archive service that can be used to shard and stream datasets for spot VMs that may terminate at any time.
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,patterson2021,https://arxiv.org/pdf/2104.10350,"""Processor Average (Watts) ... TPU v2 221 ... V100 GPU 325""",The table in the Patterson2021 document lists the average system power per processor as 221 W for TPU v2 and 325 W for the V100 GPU. Subtracting the two gives a difference of 104 W.
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20×,20,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021.""","The provided passage explicitly states the model size increase as a 20‑fold (20×) factor, which is the quantitative answer requested."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"""can directly evaporate 700,000 liters of clean freshwater""","The context from Li et al. (2025b) explicitly states that training GPT‑3 in Microsoft’s U.S. data centers directly evaporates 700,000 liters of clean freshwater, which is the value asked for."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""current averages of less than 3 years for cell phones [Cordella et al., 2020]""","The passage explicitly states that smartphone lifetimes average less than 3 years, supporting the true statement that this contributes to e‑waste concerns."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884)""","The excerpt from jegham2025 states that o3-mini had the highest cross‑efficiency score (0.884), indicating it ranked highest in the DEA‑based eco‑efficiency analysis."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,A10G,A10G,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"""lower-end GPUs are more cost-effective for small"" (from the analysis of Llama2‑7b on A10G and A100, showing that A10G performs best for small request sizes such as a single‑token classification output).",The study compares A10G and A100 GPUs and finds that for smaller request sizes—like those producing only a single classification token—the lower‑end A10G architecture delivers higher tokens‑per‑dollar and thus is the most energy‑efficient option.
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256.""","The context window size is explicitly stated as 2,048 tokens in the configuration description of FLM‑101B, providing a direct answer."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million,150,million,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We delivered 150 million packages via EVs.""","The Amazon 2023 Sustainability Report states that in Europe, Amazon delivered 150 million packages via electric delivery vehicles in 2023."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85-134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,"""a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027""","The quoted passage from Doc li2025b explicitly states the projected electricity consumption range for global AI in 2027 as 85 to 134 TWh, which directly supports the answer."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",FALSE,0,is_blank,is_blank,is_blank,"Doc [khan2025#0046]: ""metrics like accuracy and F1 score are slightly lower after optimization"". Doc [khan2025#0053]: ""Metrics such as F1 score and overall accuracy may decline slightly post-optimization""","Both cited documents report that accuracy and F1 scores either stay the same or decline after optimization, contradicting the claim that they always improved. Hence the statement is false."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,TRUE,1,is_blank,is_blank,is_blank,"""It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute‑intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low… Diffusion models, on the other hand, consume nearly the maximum power of the GPU…""","The quoted passages state that LLMs draw far less power than diffusion models and attribute this to LLM decoding being low compute‑intensity and VRAM‑bandwidth limited, directly supporting the statement that LLMs generally have lower power draw during inference."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61–76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.""","The cited passage states that, after excluding equity, computing hardware accounts for 61–76% of the total amortized cost for the four key models. This directly provides the required percentage range."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration"".  Additionally, ""The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration)"".","Both excerpts from ebert2024 state that the AI Act does not mandate disclosure of greenhouse gas emissions for AI applications such as oil and gas exploration, contradicting the claim that it does mandate such disclosure."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,is_blank,is_blank,"""The workload evaluates a total of 3000 requests."" and ""we use a pattern of 128 input tokens and a 512 output tokens.""","The online inference workload consists of 3000 requests, each with 128 input tokens and 512 output tokens, totaling 640 tokens per request. Multiplying 640 tokens by 3000 requests gives 1,920,000 tokens processed."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"""The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO 2 emissions: CO2e = 0.954pt""","The context from Strubell et al. explicitly states that the EPA’s average CO₂ emission factor for U.S. electricity is 0.954 pounds per kWh, which directly answers the question."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The quoted passage from the Dodge et al. 2022 paper states that for a short DenseNet 201 job in the West US region, the Flexible Start optimization can achieve up to an 80% reduction in CO2 emissions."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""text classification, with mean consumption of 0.002 kWh per 1,000 inferences, to the most energy‑intensive one, image generation, whose mean consumption is 2.9 kWh. This means that the different models examined in our study can vary by a factor of over 1450.""","The context gives the mean energy per 1,000 inferences for text classification (0.002 kWh) and image generation (2.9 kWh). Dividing 2.9 by 0.002 yields a factor of 1450, which matches the reported over‑1450 factor in the study."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Energy consumption should be reported at the cumulative server level""","The authors explicitly state that reporting energy consumption at the cumulative server level achieves a balance between accuracy (capturing total computation-related power usage) and feasibility, and this recommendation is repeated in multiple passages of the ebert2024 document."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""GPU alone accounts for 74% of the total energy consumption""","The provided experiment reports that in training a BERT‑base model on a single NVIDIA TITAN X GPU, the GPU alone consumed 74% of the total electricity usage, as stated in Table 1 of the cited study."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244,244,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Table in Doc amazon2023 shows ""United States 244 17,706"" indicating 244 renewable energy projects announced as of January 2024.","The table lists the United States with 244 projects, which is the number of Amazon Renewable Energy Projects announced in that country as of January 2024."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context documents mention Yelp sentiment analysis benchmarks or provide any comparison of accuracy between traditional models and large language models, so the statement cannot be verified."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Amazon’s AWS only covered ﬁfty percent of its power usage with renewable energy.,"The quoted sentence from Schwartz 2019 explicitly states that AWS covered 50% of its power usage with renewable energy, answering the 2018 percentage question."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context documents provide evidence of nearly linear per‑GPU speedup for CV models on T4 GPUs in transatlantic or intercontinental settings, but do not explicitly state that this holds for intra‑zone scaling. Therefore, the documents do not contain sufficient information to confirm the claim for intra‑zone scaling."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping,Swapping,is_blank,is_blank,is_blank,"""when the server is overloaded, Swapping consistently consumes less energy.""","The quoted passage from Chung 2025 (doc #0177) explicitly states that under overload conditions the Swapping preemption mechanism consumes less energy than Recomputation, making Swapping the consistently lower‑energy option."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.,The provided excerpts from the Shen 2024 report explicitly state that JetMoE-8B achieves approximately a 70% reduction in inference computation relative to the Llama2-7B model.
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google reports a 48% increase in GHG emissions since 2019""","The quoted sentence in the 2024 environmental report directly states that Google’s GHG emissions increased by 48% compared to 2019, providing the required percentage."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages contain any information about the quantity of fiber optic cable installed globally for AI workloads in 2023, so the answer cannot be determined from these documents."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a McKinsey projection for the percentage of U.S. national electricity consumption that data centers are expected to account for in 2030.
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800M,800,USD,is_blank,is_blank,"""we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.""",Both cited passages explicitly state that the upfront hardware acquisition cost for training GPT‑4 is estimated at $800 million.
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively."" (Fig. 13 shows bsz=28 for A100‑40GB)","The document states that for the NVIDIA A100‑40GB GPU the ground‑truth maximum batch size for fine‑tuning Mixtral is 28 samples, as confirmed by the figure labeling bsz=28 for that GPU."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.""","The quoted definition explicitly states that Sustainable AI includes both using AI in climate-positive applications and improving the environmental sustainability of AI itself, so it is not limited to just climate-positive applications."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,GPUs,is_blank,is_blank,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs""","The quoted sentence from the document explicitly states that two NVIDIA A100‑80GB GPUs are needed to serve Llama2‑70b at BF16 precision, giving the required number as 2."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,li2025b;jegham2025,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2505.09598,"""with an estimated training energy of 1287 MWh [29]."" (Doc li2025b#0046) and ""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity"" (Doc jegham2025).","Both documents explicitly state the estimated training energy for the full GPT‑3 model as 1,287 MWh, so the answer is 1287 MWh."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any explicit figure indicating the number of hectares of land occupied by new AI data centers globally in 2022, so a confident answer cannot be derived."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",25%,25,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""doubling the duration can lead to significant savings up to about 25%"" (Doc [dodge2022#0092])","The document states that with the Pause and Resume optimization, doubling the training duration for the 6B parameter transformer can achieve a maximum emissions saving of approximately 25%."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",26.1%,26.1,%,is_blank,is_blank,16.6% 31.9%26.1%23.5%,"The table for People Managers lists the percent of women for each year, with 26.1% corresponding to 2023."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2e emissions.""","The cited passage from the 2019 Strubell et al. study explicitly states that training BERT produced 626,155 pounds of CO2e, providing the required numeric value and unit."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g,0.32,g,luccioni2024,https://arxiv.org/pdf/2311.16863,"""bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries""","The context explicitly states that the BERT-based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 g CO2eq for every 1,000 text‑classification queries, so the answer is 0.32 g."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5%,28.5,is_blank,is_blank,is_blank,"""28.5% operational power footprint reduction over two years (Figure 8).""",The cited figure and text state that the iterative hardware‑software optimization led to a 28.5% reduction in operational energy footprint over the two‑year period from 2019 to 2021. This directly answers the question.
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The passage explicitly states that 2022 was the year when direct environmental disclosures peaked, and subsequent text notes a decline after that year."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",19,19,k grams,is_blank,is_blank,"""7k grams vs. 26k grams, for the most efficient vs. least efficient regions""","The document reports that the most efficient region emitted 7 k grams of CO₂ and the least efficient emitted 26 k grams, giving a difference of 19 k grams."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MWh,40,MWh,li2025a,https://arxiv.org/pdf/2309.03852,Energy (MkWh) 1171 1066 3179 444 688 40,"The table in document li2025a lists the total energy consumption for the FLM‑101B model as 40 megawatt‑hours, which is the value requested."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.,"The quoted passage states that the MoE layer is the most time‑consuming and is explicitly identified as a prime target for optimization in LLM fine‑tuning, confirming the statement as true."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include a specific water consumption figure for an OLMo 60M model trained on 1.7 to 5.6 trillion tokens, nor does it provide a direct conversion to days of equivalent water usage for a single US person. Therefore, the answer cannot be determined with confidence from the documents alone."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,is_blank,is_blank,"""A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.""","The cited passage states that adding compute resources reduces cost, contradicting the assertion that it can increase costs. Therefore the statement is false."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",True,1,is_blank,is_blank,is_blank,"""For CV, even distributing VMs over four continents only slows down performance by 7%"" (Doc [erben2023#0093]) and ""on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment"" (Doc [erben2023#0092])","Both documents report that intercontinental training for high‑granularity CV models incurs only a 7% performance slowdown compared to local training, directly supporting the statement."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so a confident answer cannot be derived from the documents."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""finding that training accounted for only half of the model’s overall emissions [121]""","The article states that training represented only half of BLOOM’s total emissions, i.e., 50% of the overall carbon footprint."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.,"The cited excerpt from the ebert2024 document explicitly states that GPU-level monitoring is not recommended for overall AI energy reporting, contradicting the claim that it is the preferred method."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,$32.7,32.7,$,xia2024,https://arxiv.org/pdf/2408.04693,"Table IV shows the estimated cost for fine‑tuning Mixtral on GSM8K with a sparse MoE setup on an NVIDIA A40‑48GB GPU: ""Cost ($) 32.7"".","The table lists the total cost for the A40 GPU as $32.7, which directly answers the question about the estimated total cost."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The supplied context contains only execution time breakdown figures and descriptions for various batch sizes but does not provide a specific numeric total execution time for a sparse Mixtral fine‑tuning run on an NVIDIA A40‑48GB with batch size 10. Therefore the answer cannot be determined from the documents alone.
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""The 7B model was run on a single GPU"" and Table II lists for V100 32GB: 7B – 1 GPU (max batch 64).","The document states that the 7B LLaMA model was run on a single V100 GPU, and Table II shows the bare minimum hardware for 7B as one 32‑GB V100, confirming that one GPU is sufficient for inference without compression or quantization."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,is_blank,is_blank,"""one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)""","Both Dodge 2022 documents state that the typical U.S. household emits about 8.3 metric tons of CO₂ annually from its combined electricity, natural gas, gasoline, and fuel oil use, which directly answers the question."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a value for the projected public health burden of U.S. data centers in 2030, so the answer cannot be determined with confidence."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",35 samples,35,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""our model predicts that the maximum batch sizes supported for fine‑tuning Mixtral will be 28 and 35, respectively."" (sparse Mixtral on NVIDIA A40‑48 GB supports batch size 35).","The analytical model reports that for a sparse Mixtral fine‑tuning on an NVIDIA A40‑48 GB GPU the maximum supported batch size is 35 samples. This batch size corresponds to the longest-running MoE layer because it is the largest batch size for which the model fits in memory, thereby maximizing the MoE execution time."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10–50 medium-length responses,"[10, 50]",requests,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.""","The document states that consuming a 500 ml bottle of water corresponds to about 10–50 medium‑length user requests when GPT‑3 is trained in a data center, covering all relevant locations including Arizona."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,is_blank,is_blank,GSM8k 14.5 17.3 16.9 27.8,"The GSM8k score column for JetMoE-8B is 27.8, the highest value in the GSM8k row of Table 3, indicating its performance on the GSM8k benchmark."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement of the maximum batch size for fine‑tuning Mixtral with a dense setup on the Hellaswag dataset using an NVIDIA A40 GPU with 48 GB memory. No numeric value is explicitly reported for this specific configuration, so the answer cannot be determined with confidence from the documents."
