id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about U.S. data center electricity consumption figures for 2010 or 2014, nor any statement of their change or average increase between those years. Therefore, the answer cannot be determined from the given documents."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% timeâ€‘saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""",The passage explicitly states that using the growth strategy saved 72% of the training time compared to a fromâ€‘scratch approach for the 101B model.
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The excerpt from the Han 2024 document explicitly states that the total permitted annual emission limits for nitrogen oxides from data center backup generators in northern Virginia are approximately 13,000 tons, directly answering the question."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,8 0.036 12.0 0.054 12.64 21.5 bil.,"The table for Llamaâ€¯3.2â€¯1B lists GPU Power Usage at an 8â€¯req/s request frequency as 0.036â€¯kWh, directly answering the question."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,morrison2025,https://arxiv.org/pdf/2503.05804,"""3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.""","The context from Morrison 2025 states that NVIDIAâ€™s embodied emissions are assumed equal to Luccioni et al. (2023), giving 3700â€¯kg CO2eq per 8x server node, which converts to 463â€¯kg CO2eq per GPUâ€”providing the requested estimate."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",44.41,44.41,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,Table 6: Performance of the three stages of FLM on Open LLM. Parameters 101B â€¦ Average 44.41,"The table reporting Open LLM Leaderboard results lists the overall average score for the 101B model as 44.41, which is the final average performance score mentioned in the provided context."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"more than 6,750 fold improvement",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""This is a more than 6,750 fold improvement in processor clock speed""","The context states that the Intel 4004 ran at 740â€¯kHz while typical 2021 microprocessors run at 5,000,000â€¯kHz, yielding a more than 6,750â€‘fold increase in clock speed."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021;luccioni2023,https://arxiv.org/pdf/2104.10350;https://arxiv.org/pdf/2302.08476,"""It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS."" (patterson2021) and ""total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs)"" (luccioni2023).","Both documents explicitly state that the training of GPTâ€‘3 with 10,000 V100 GPUs at 24.6â€¯TFLOPS/sec requires approximately 14.8â€¯days."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context provides estimates for premature deaths in 2028 (approximately 1,300) but does not contain any figures or projections for 2030. Therefore, the documents do not provide sufficient information to answer the question about 2030."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a numeric estimate of the total training budget for FLM-101B, so the factor comparing GPT-4â€™s amortized cost to that budget cannot be determined with certainty."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 tCO2e,1.2,tCO2e,patterson2021,https://arxiv.org/pdf/2104.10350,"""Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2 e per passenger round trip.""","The document calculates the roundâ€‘trip emissions for the whole aircraft as 180.4â€¯t and, using seat occupancy, divides that figure to obtain approximately 1.2â€¯tCO2e for a single passenger on the Sanâ€¯Franciscoâ€“Newâ€¯York route."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014","The context explicitly states that the study was launched in the fall of 2014, so the year of launch is 2014."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,4,4,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"In 2023, AWS increased the number of data centers using recycled water for cooling fromÂ 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The passage states that the number rose from 20 to 24 in 2023, indicating 4 new data centers began using recycled water for cooling that year."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,is_blank,is_blank,"""the equivalent of taking more than 13,900 cars off the road.""","The Amazon Sustainability Report 2023 states that the Amazon Solar Farm Marylandâ€“CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. Thus, the estimated number of cars that could be taken off the road is 13,900."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,approximately 60 days,60,days,is_blank,is_blank,"""This model was not trained to completion, but only until 13%; a full training run would take 60 days.""","The context states that the 6.1â€¯billion parameter model was trained for 13â€¯% of the total time, which corresponds to 8 days; extrapolating linearly yields a full run of 60 days, as explicitly mentioned in the passage."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.1%,0.1,%,patterson2021,https://arxiv.org/pdf/2104.10350,"""1500B parameters but only 0.1% activated per token"" and table entry ""Percent of model activated on every token 0.10%""","The documents state that the Switch Transformer has 1500â€¯billion parameters with only 0.1â€¯% activated per token, as shown both in the narrative sentence and the table. Hence the percentage activated per token is 0.1â€¯%."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"The UNâ€™s Global Eâ€‘Waste Monitor 2024 showed that about 22% of eâ€‘waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than eâ€‘waste recycling [10].","The context from DocÂ luccioni2025a explicitly states that 22% of eâ€‘waste is formally collected and recycled, directly answering the question."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,set the same number of experts to 8 and top-k to 2 for every layer,"The hyperparameter table lists Nexperts as 8, and the accompanying text confirms that each MoE layer uses 8 experts."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3Ã—,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3Ã—.""","The quoted passage states that raising GPU utilization to 80% reduces the overall carbon footprint by a factor of three, so the answer is 3."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,is_blank,is_blank,is_blank,"""we sampled 88 models""","The study explicitly states that 88 models were sampled and analyzed, indicating the total number of distinct machine learning models considered."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevonsâ€™ Paradox,Jevonsâ€™ Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Economists refer to such transformations as Jevonsâ€™ Paradox ...""",The paper cites Jevonsâ€™ Paradox as the economic principle explaining how efficiency gains can paradoxically increase overall consumption and thus negate environmental benefits.
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,51.686 MWh,51.686,MWh,is_blank,is_blank,"""Training energy (kWh) 51,686""","The context shows the preâ€‘training (training) energy for the BLOOMzâ€‘7B model as 51,686â€¯kWh. Converting from kilowattâ€‘hours to megawattâ€‘hours (divide by 1,000) yields 51.686â€¯MWh."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit information about which model has the highest GPU energy consumption for 1,000 queries in the appendix of a 2025 study, so the answer cannot be determined with confidence."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide a clear numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40-48 GB GPU, so the answer cannot be determined with confidence from the documents."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,"The quoted sentence explicitly states that the English portion of FLM-101B training required 28.22 zettaFLOPs, providing the required value."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,2.2x,2.2,is_blank,is_blank,is_blank,"""ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.""","The passage explicitly states that the total compute used during model development is about 2.2 times larger than the compute spent on the final training run alone, as reflected in the median ratio of 2.2x."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,dodge2022,https://arxiv.org/pdf/2206.05229,"Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8)âˆ—13.8 = 103.5 MWh, or 103,500 kWh â€” almost 2800 times more than training the BERT-small model!","The quoted passage explicitly calculates the full training energy as 103.5â€¯MWh, directly answering the question."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 â€“ 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"""global AI demand is projected to account for 4.2 â€“ 6.6 billion cubic meters of water withdrawal in 2027""","The document explicitly states that the global AI demand will result in 4.2 to 6.6â€¯billion cubic meters of water withdrawal by 2027, providing the numeric range and unit."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts do not contain any explicit mention of the hardware processor used for the experimental setup of energyâ€‘efficient local inference in financial sentiment classification, so the answer cannot be determined from the given documents."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Red AI is on the rise despite the well-known diminishing returns of increased cost""","The cited passage explicitly states that Red AI is on the rise, contradicting the claim that it is on the decline. Hence the statement is false."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 d""",The provided context explicitly states that the total wallâ€‘clock time for training the FLMâ€‘101B model using a growth strategy is 21.54 days. The quoted sentence directly supports the numeric answer and its unit.
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,is_blank,is_blank,"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity","The provided context from Doc [jegham2025#0002] explicitly states that GPTâ€‘3 training consumes 1,287â€¯MWh of electricity, which directly answers the question."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.""",The cited sentence explicitly states that the AI Energy Score is a collaborative project designed to provide a unified method for comparing inference efficiency across AI models.
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Fig. 10. A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The provided context states that most experimentation uses GPUs at 30â€‘50% capacity, not over 80%, so the claim is false."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,69 years,69,years,stone2022,https://arxiv.org/pdf/2211.06318,"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.""","The context states that AI was officially founded in 1956, so by 2025 it has been about 69 years."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22,22,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Amazonâ€™s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sourcesâ€”an increase from 19 regions in 2022.",The passage explicitly states that 22 AWS data center regions had 100% renewable energy matching in 2023.
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.",The hyperparameter description in the JetMoEâ€‘8B report explicitly states that each layer activates the topâ€‘2 experts (topâ€‘kâ€¯=â€¯2).
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural architecture search (NAS),Neural architecture search (NAS),is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study""","The 2019 Strubell et al. studyâ€™s â€˜five carsâ€™ estimate arises from the energy cost of performing neural architecture search, a specialized AI process that is rarely carried out."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the percentage of AI inference workloads in Asia that were powered by coal in 2023, so a confident answer cannot be derived."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.""","The context explicitly states Amazon holds 1.3 GW of energy storage capacity in 2023, so the answer is 1.3 GW."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,is_blank,is_blank,"""Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years."" ""The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].""","The provided documents report a 300,000x increase in compute from 2012 to 2018, not a 200,000x increase, so the statement is false."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",44%,44,%,chung2025,https://arxiv.org/pdf/2505.06371,"This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The quoted passage states that targeting an average TPOT of 100â€¯ms leads to a 44â€¯% reduction in energy consumption per generation compared to minimalâ€‘latency configuration, so the energy use decreases by 44â€¯%."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,is_blank,is_blank,"""the global carbon emissions for 2020 dropped by 6.4%""","The context explicitly states that global carbon emissions fell by 6.4% in 2020, attributing the decline to the COVID-19 pandemic. This quote directly supports the numeric answer of 6.4%."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,"2,300 transatlantic flights",2300,flights,is_blank,is_blank,"These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 2,300 transatlantic flights between Boston and London.","The context explicitly states that GPTâ€‘4oâ€™s projected annual carbon emissions (~138,125â€“163,441â€¯tCOâ‚‚e) are comparable to the emissions from about 2,300 transatlantic flights, providing the numeric answer."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.,"The context explicitly states that the AI field was christened in 1956 at the Dartmouth workshop, so the answer is 1956."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context references a figure that shows kernel execution times but does not provide explicit numeric values for the longest kernel of the MoE layer for a dense BlackMamba model with batch size 30. Therefore, the documents do not contain sufficient information to answer the question with confidence."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally[74]""","The context from ebert2024 states that the global average PUE for data centers in 2023 was 1.58, which is the value requested for AIâ€‘dedicated data centers."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,2.875 Wh,2.875,Wh,is_blank,is_blank,"""For instance GPT-4o consumes around 2.875 Wh while GPT-4o miniâ€™s consumption is slightly higher at 3.098 Wh...""","The provided context states that GPTâ€‘4o (the o3 model) requires about 2.875 Wh for a long prompt, which directly answers the question."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,is_blank,is_blank,Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed) â€¦ Llama 7B 63 Meta 356 14,"The table lists the GHG emissions for Llama 7B as 14â€¯tCO2e, which directly answers the question."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,is_blank,is_blank,"""including the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.""","The cited passage from Morrison et al. explicitly states that operational environmental impacts include GHG emissions from servers and data center cooling, which directly contradicts the claim that they do not include these emissions. Therefore the statement is false."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not include the maximum batch size for fineâ€‘tuning BlackMamba with sparse setup on the GSM8K dataset using an NVIDIA A40 GPU. Therefore the answer cannot be determined from the documents.
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",25000,USD,schwartz2019,https://arxiv.org/pdf/1907.10597,"Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context explicitly states that training Grover on 256 TPU chips for two weeks cost an estimated $25,000, providing the numeric answer and confirming the unit as USD."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",TRUE,1,is_blank,is_blank,is_blank,"Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The quoted sentence from the document explicitly states that sparsely activated DNNs consume less than oneâ€‘tenth the energy of dense DNNs while maintaining accuracy, confirming the claim."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].""","The cited sentence from Doc [ebert2024] directly provides the average global PUE value for 2023, which is 1.58. No other documents supply a conflicting 2023 figure, so this is the supported answer."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The document explicitly states that quantizing RM2 from 32-bit to 16-bit reduces its overall model size by 15%, providing the required percentage reduction."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCGâ€™s experienceâ€¦ Applying observations made from individual projects to the entire planetâ€™s GHG emissions lacks any scientific groundingâ€¦""","The context states that the 5â€‘10% reduction estimate originates from BCG reports whose calculations are not detailed and lack sound scientific grounding, so the claim is not supported by clear publicly available calculations."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied passages do not provide a specific average WUE figure for Googleâ€™s AIâ€‘dedicated data centers in 2024, nor do they distinguish Googleâ€™s value from Equinixâ€™s. Therefore the answer cannot be determined from the given documents."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules per token,"[3, 4]",Joules,samsi2024,https://arxiv.org/pdf/2310.03003,"For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The cited context explicitly states that at a maximum generation length of 512 tokens, the energy per decoded token is approximately 3 to 4 Joules, which is the value reported in the answer."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].""","The quoted passage directly states that about 770â€¯million people lack stable electricity, which matches the claim in the question, confirming it is true."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,is_blank,is_blank,The optimizer stage in BlackMamba fineâ€‘tuning takes a considerable portion of the running time (up to 53% when conducting sparse fineâ€‘tuning with batch size = 1).,"The provided sentence explicitly states that the optimizer stage accounts for 53% of the running time for BlackMamba sparse fineâ€‘tuning on an NVIDIA A40 GPU with batch size 1, which directly answers the question."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context contains information about Amazonâ€™s renewable energy projects and other unrelated topics, but it does not mention Microsoft, Azure AI clusters, or any wind turbine contracts by Microsoft in 2023. Therefore, there is insufficient information to answer the question."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B,2,parameters,is_blank,is_blank,JetMoE-8B has 8B parameters while only activating 2B for each input token,"The context explicitly states that JetMoE-8B activates 2B of its 8B parameters for each input token during inference, which directly answers the question."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The panel explicitly states there is no cause for concern that AI poses an imminent threat, contradicting the claim that they are concerned. Thus the statement is false."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,is_blank,is_blank,"""manufacturing carbon cost accounts for 74% of the total footprint""","Both documents state that the manufacturing carbon cost constitutes 74% of a client deviceâ€™s total carbon footprint, directly answering the question."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the distance between the Earth and the Sun, so a confident answer cannot be derived from them."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17,17,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AIâ€‘savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.""","The provided passage explicitly states that the inaugural Study Panel was a seventeen-member panel, giving the exact number of members."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain any information about CO2 emissions from OpenAI API requests in January 2024, so the question cannot be answered with confidence from these documents."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,is_blank,is_blank,"Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context from Cottier et al. states that Gemini Ultraâ€™s R&D staff cost fraction, including equity, is 49%, directly answering the question."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Generally, models with more parameters consume more energy, but this is not always the case.  Even though Small has nearly twice the parameters, the larger Small model can consume less energy than Mini.","The passage explicitly states that more parameters do not always lead to higher energy consumption during inference, disproving the claim that it always consumes more energy."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9Ã—,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9Ã— increase in AI training infrastructure capacity over the 1.5 years.,"The cited figure in the document explicitly states a 2.9Ã— increase in AI training infrastructure capacity over the 18â€‘month period, which directly answers the question."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Metaâ€™s Llamaâ€¯3 family emitted about 11,390â€¯tCOâ‚‚e during preâ€‘training, which is roughly 40â€¯Ã— higher than the â€œfive carsâ€ estimate (â‰ˆ284â€¯tCOâ‚‚e).",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Meta reports that their Llamaâ€¯3 family of models emitted 11,390 tons COâ‚‚e or over 40x the â€œfive carsâ€ estimate. The â€œfive carsâ€ estimate is 626,155â€¯pounds (284 metric tons) COâ‚‚â€‘equivalent GHG emissions.","The quoted passage gives the exact preâ€‘training emissions for Metaâ€™s Llamaâ€¯3 (11,390â€¯tCOâ‚‚e) and compares it to the fiveâ€‘cars benchmark (284â€¯tCOâ‚‚e), showing it is about 40 times larger."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,is_blank,is_blank,"Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context explicitly defines â€œContinuous batchingâ€ as the strategy that replaces finished requests with new ones to reduce idle GPU time, directly supporting the answer."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"14,380 Amazon electric delivery vans were added in total across 2022 and 2023.",14380,vans,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Region 2022 2023
U.S. 2,600 11,800
Europe 1,220 3,000+
India 3,800 7,200+","The table shows the number of electric delivery vans per region for 2022 and 2023. 2023 minus 2022 gives 11,800â€‘2,600â€¯=â€¯9,200 vans in the U.S., 3,000â€‘1,220â€¯â‰ˆâ€¯1,780 in Europe, and 7,200â€‘3,800â€¯=â€¯3,400 in India. Adding these increases yields a total of 9,200â€¯+â€¯1,780â€¯+â€¯3,400â€¯=â€¯14,380 vans added across the two years."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Flanâ€‘T5â€‘xxl at 0.083 kWh/1k queries"" and ""Google reported translating more than 100â€¯billion words per dayâ€¦yielding an estimate of 1â€¯billion queries to the model per day.""","The average energy cost is 0.083â€¯kWh per 1,000 queries, so per query it is 0.000083â€¯kWh. Multiplying by 1â€¯billion queries gives 83,000â€¯kWh, which equals 83â€¯MWh per day."
q080,True or False: The AlphaGo program defeated the human Go champion.,True,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The AlphaGo program that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game."" ""The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.""","Both quoted sentences explicitly state that AlphaGo defeated the human Go champion in a match, confirming the statement is true."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams of CO2eq per 1,000 inferences",1594,grams of CO2eq,is_blank,is_blank,"""the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams ofð¶O2eq for 1,000 inferences""","The quoted passage from the 2024 study states that stableâ€‘diffusionâ€‘xlâ€‘baseâ€‘1.0 emits 1,594â€¯grams of COâ‚‚eq per 1,000 inferences, providing the exact number requested."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"""The entire alignment process takes 60 H100 GPU hours.""","The context explicitly states that the full alignment, comprising both dSFT and dDPO fineâ€‘tuning, consumes 60 H100 GPU hours, directly providing the requested figure."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,is_blank,is_blank,The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO 2 e/KWh [USE21].,"The document states the gross carbon intensity of U.S. average mix energy as 0.429 kg CO2e per kWh, which is the value requested."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts contain no mention of classification experiments on German public administration texts or the performance of sentenceâ€‘embedding models on such data, so the question cannot be answered with confidence."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)""","The quoted passage explicitly states that researchers do not believe a universal, oneâ€‘sizeâ€‘fitsâ€‘all approach to AI ethics and sustainability can be developed, therefore the statement is false."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""As proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socioâ€‘technical aspects in the description and understanding of AI systems""","The context explicitly states that the term proposed to broaden transparency to include socioâ€‘technical aspects and environmental footprint is ""social transparency."""
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied context does not contain a numerical range of GPU energy usage for performing 1,000 inference queries from a 2025 studyâ€™s appendix, so the answer cannot be determined with confidence."
q093,How many parameters does the largest T5 model have?,11 billion parameters,11000000000,parameters,is_blank,is_blank,"""The largest size has 11B parameters,""","The passage from Patterson 2021 states that the largest T5 model contains 11â€¯B parameters, which directly answers the question."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"""with a total cost of $2.699, an increase of about 26.7%""","The Max-Performance policy chose g6e.xlarge costing $2.699, while InferSave chose g4dn.xlarge costing $2.13. The cost difference corresponds to an increase of about 26.7%, as stated in the document."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,is_blank,is_blank,"""the companyâ€™s data center water consumption increased by âˆ¼20% from 2021 to 2022"" (doc [li2025b#0002]) and ""Google observed a 20% uptick in the same period"" (doc [luccioni2025a#0010])","Both cited passages explicitly state that Googleâ€™s data center water consumption rose by about 20% between 2021 and 2022, providing a clear numeric answer."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5 billion liters,3500000000.0,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""3.5B Liters of water returned to communities from replenishment projects in 2023""","The document states that AWSâ€™s water replenishment portfolio returned 3.5â€¯billion liters to local communities in 2023, which directly answers the question."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,is_blank,is_blank,"""Hivemind [39] is a PyTorch-based [32] framework...runs in a decentralized fashion and can handle peers that drop out at any stage of the training.""","The cited passage identifies Hivemind as a decentralized, PyTorchâ€‘based framework designed for distributed training, enabling spot instance training across multiple clouds and continents."
q096,What is the name of the emissions metric defined as 'COâ‚‚ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency""","The context lists Carbon Intensity as the metric whose definition is ""CO2 emissions per unit of electricity consumed,"" so the metric name is Carbon Intensity."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.59,0.59,is_blank,is_blank,is_blank,"""resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).""","The context states that the NLP throughput when spread across four continents (Câ€‘8) is 41â€¯% lower than the local (Aâ€‘8) run, meaning it achieved 59â€¯% of the local throughput (1â€¯â€“â€¯0.41â€¯=â€¯0.59)."
q094,What is the total number of parameters in the JetMoE-8B model?,8B parameters,8,B,shen2024,https://arxiv.org/pdf/2404.07413,"Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token","The document explicitly states that JetMoE-8B has 8B parameters, which directly answers the question about the total number of parameters in the model."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",model-attention disaggregation,model-attention disaggregation,is_blank,chen2024,https://arxiv.org/pdf/2405.01814,"We introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model.","The quoted passage from the Chen et al. paper states that the system developed is called model-attention disaggregation, directly answering the question."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40,$,cottier2024,https://arxiv.org/pdf/2405.21015,"""most expensive publicly-announced training runs to date are OpenAIâ€™s GPT-4 at $40M""","The cited passage explicitly states the amortized training cost for GPTâ€‘4 as $40â€¯million, which directly answers the question."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",51.3%,51.3,%,li2025a,https://arxiv.org/pdf/2309.03852,"""The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%.""","The quoted sentence states that each training stage, including the final growth stage, achieved a FLOPs utilization of at least 51.3%, which is the value requested."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.,"The quoted sentence explicitly states that custom tags reduce energy consumption for zeroâ€‘shot, oneâ€‘shot, and fewâ€‘shot prompt techniques in source code completion, supporting the True answer."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810Ã—,810,is_blank,is_blank,is_blank,"""Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810Ã—.""","The document explicitly states that full-stack optimization reduces the operational carbon footprint by 810 times compared to a CPU baseline, supporting the answer."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebookâ€™s data centers are about 40% more efficient than small-scale, typical data centers.""","The cited passage from WuÂ 2021a explicitly states that Facebookâ€™s data centers achieve a PUE of about 1.10, which is the required value."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,is_blank,is_blank,"""the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems""","The context explicitly states that the Finnish ETAIROS project proposed the integration of ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems, making ETAIROS the required acronym."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""115 books would produce the same amount of CO2 as a single Amazon Kindle device""","The context explicitly states that a life cycle assessment found 115 print books generate the same CO2 as one Amazon Kindle, directly giving the required number."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 Âµg/mÂ³,9,Âµg/mÂ³,han2024,https://arxiv.org/pdf/2412.06288,"""EPAâ€™s recently tightened standard for PM2.5 sets an annual average limit of 9Âµg/m 3""","The provided passage explicitly states that the EPAâ€™s newly tightened primary standard for PM2.5 is an annual average limit of 9 Âµg/mÂ³, which directly answers the question."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,USD,cottier2024,https://arxiv.org/pdf/2405.21015,We find that the most expensive publiclyâ€‘announced training runs to date are OpenAIâ€™s GPTâ€‘4 at $40M and Googleâ€™s Gemini Ultra at $30M.,"The context explicitly states that Googleâ€™s Gemini Ultra had an estimated amortized training cost of $30â€¯million, which is used as the answer."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200Ã—,200,is_blank,is_blank,is_blank,"""the per-household health burden could be 200x more than that in less-impacted communities."" â€“ Doc [han2024#0001]
""per-household burdens exceeding those in other counties by more than 200-fold."" â€“ Doc [han2024#0010]","Both cited documents explicitly state that the per-household health burden in the most affected, economically-disadvantaged communities could exceed that in less-impacted communities by a factor of 200."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain any information about the energy consumption of a DS Llama 70B model on the FKTG dataset. Therefore, I cannot answer the question with confidence."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons paradox,Jevons paradox,is_blank,luccioni2025b;jegham2025,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/2505.09598,"This is often referred to as Jevons paradox, which observes that when technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use.","The context explicitly names Jevons paradox as the phenomenon where improved efficiency leads to higher usage and greater total resource consumption, supporting the answer."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages mention Dodge et al. (2022) only in the context of measuring electricity consumption and carbon emissions, and do not contain any information about the total number of parameters in the large language model they analyzed."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,is_blank,is_blank,is_blank,"""Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.""","The context explicitly states that, on average, 44% of the total amortized hardware and energy cost is attributed to AI accelerator chips, providing the required answer."
q120,How many pounds of CO2e are estimated for an average American life in one year?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain any estimate of the CO2e emissions for an average American life in one year, so a confident answer cannot be derived from the given documents."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,True,1,is_blank,is_blank,is_blank,"""For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks... Crucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5].""","The passage states that the AI Act requires risk assessment and mitigation for GPAI models with systemic risk and interprets the scope of risks to include environmental risks, thus supporting the statement that providers must conduct risk assessments that include environmental risks."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any explicit statement of the number of data center GPUs shipped by NVIDIA in 2024.
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"image generation 2.907 3.31
Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","The table lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences, which directly answers the question."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied documents do not contain any information about Mistralâ€‘smallâ€™s emission multiplier after optimization for the financial sentiment classification task, so the answer cannot be determined from the provided context."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, net tCO2e, and their formulas are the same as (Patterson et al. 2021). Model GPT-3 (Brown et al. 2020) Gopher (Rae et al. 2021) PaLM (Anil et al. 2023) GLM-130B (Zeng et al. 2023) Llama-2 (Touvron et al. 2023b)FLM-101B Params 175B 280B 540B 130B 70B 101B",The table explicitly lists the parameter count for FLM-101B as 101â€¯B.
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about 2030 county-level per-household health cost projections for West Virginia counties.
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about a dataset name for German nuclear waste site objection texts classified in experiments.
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Approximately 5.8 Meena training runs,5.75,runs,patterson2021,https://arxiv.org/pdf/2104.10350,"T5 training emissions are ~26%, Meena is 53%, GPT-3 is ~305% of such a round trip.","The relative emissions figure shows GPT-3 uses about 305% of the energy of a roundâ€‘trip flight, while Meena uses 53%.  Dividing 305 by 53 gives â‰ˆ5.75, so roughly 5.8 Meena runs equal one GPTâ€‘3 run."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,Approximately 3 passengers,3,passengers,is_blank,is_blank,"Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The passage explicitly states that the 3.2â€¯tCOâ‚‚e embodied in the Evolved Transformer NAS corresponds to about three passengers on a roundâ€‘trip flight between San Francisco and New York, providing the numeric equivalence requested."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64","The table lists the bare minimum GPU counts for each model. For LLaMAâ€‘13B the entry for A100 80GB shows a count of 1, indicating that only one A100 80GB GPU is required for inference without compression or quantization."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts contain any numeric information about freshwater consumption by Meta's Llamaâ€¯3 inference serving clusters in 2024, so the answer cannot be determined from the documents given."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.""",The provided passage explicitly states that 84â€¯% of token usage on OpenRouter in Mayâ€¯2025 occurred via models that did not disclose environmental impact. This figure directly answers the question.
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,is_blank,is_blank,"Training energy (kWh) 51,686  ...  Finetuning energy (kWh) 7,571","The study reports training energy of 51,686â€¯kWh and fineâ€‘tuning energy of 7,571â€¯kWh for BLOOMzâ€‘7B; adding these gives a combined cost of 59,257â€¯kWh."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,is_blank,is_blank,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of COâ‚‚e.""",The quoted sentence from the Power Hungry Processing study explicitly states the total energy consumption for all model experimentation and evaluation was 754.66 kWh.
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context documents do not contain any explicit numeric value for the total carbon emissions avoided by pruning and quantizing large language models in 2023, so a confident answer cannot be derived."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,is_blank,is_blank,"""Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.""","The context explicitly states that blending 2 A100s with 1 A10G yields a 24% cost saving compared to an A100â€‘only strategy, which is the required percentage."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Table 5 lists Training energy (kWh) 51,686, Finetuning energy (kWh) 7,571, Inference energy (kWh) 1.0 Ã— 10âˆ’4 and Cost parity (# inferences) 592,570,000 for BLOOMz-7B.","The table provides the perâ€‘inference energy and the total training + finetuning energy. Dividing the total energy (51,686 + 7,571 = 59,257â€¯kWh) by the perâ€‘inference energy (1.0â€¯Ã—â€¯10â»â´â€¯kWh) yields 592,570,000 inferences, which matches the listed costâ€‘parity figure."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information from Chen et al. (2025) about the hourly price of an NVIDIA H20, so the answer cannot be determined with confidence."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""In fact, most carbon foot print analyses gather the information manually by writing to authors.""","The context explicitly states that most analyses gather information manually by contacting authors, contradicting the claim that they gather information automatically without contact."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Table II: ""7B 1 64 1 64"" â€“ the bare minimum hardware required for a 7B LLaMA model lists 1 A100 80GB GPU.","The table lists the bare minimum GPU count for each model; for the 7B variant it shows 1 GPU for both V100 and A100, indicating that a single A100 80GB GPU is sufficient for inference without compression or quantization."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""Luccioni and Hernandezâ€‘Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers""","The cited passage explicitly states that only 95 responses were collected after contacting more than 500 authors, providing the required count."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",122%,122,%,han2024,https://arxiv.org/pdf/2412.06288,"Table 2: ""Altoona, IA ... 122% ...""","The table shows that for Altoona, IA, the health cost is 122% of the electricity cost, directly giving the required percentage."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,is_blank,is_blank,"""public health cost of U.S. data centers from 2019 to 2023 as a reference. Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centersâ€™ total electricity cost.""","The context explicitly states that in 2023 the public health cost equaled about 44% of the data centersâ€™ total electricity cost, which is the percentage requested."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,United Kingdom 36 901,"The table of Amazon Renewable Energy Projects announced as of Januaryâ€¯2024 lists 36 projects in the United Kingdom, indicating the number of UK projects announced to that date."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization""","The quoted sentence from the abstract explicitly states that the deployment techniques achieve up to a 45% reduction in carbon emissions after quantization, confirming the statement is true."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25T tokens,1250000000000,tokens,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The text explicitly states that JetMoE-8B was pre-trained on 1.25 trillion (1.25T) tokens, which is the required token count."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric range for the CO2 emissions of a complete training run of a 6.1â€¯billion parameter transformer model. They only state that a partial run exceeds 8.3â€¯metric tons and that a full run would be about an order of magnitude higher, which is insufficient to give a precise range."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Approximately $1 per NVIDIA H100 GPUâ€‘hour,1,USD/h,is_blank,is_blank,"""4789 239,942 $103kâ€“$350k $9870"" â€“ indicating 239,942 GPUâ€‘hours cost between $103,000 and $350,000","The table lists 239,942 GPUâ€‘hours for the JetMoE R&D effort with a cloud compute cost range of $103kâ€“$350k. Dividing the midpoint of this cost ($~$226,500) by the hours gives roughly $1 per GPUâ€‘hour. This aligns with the documented total budget and hours in the source."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation).","The definition in the context explicitly names â€˜Water withdrawalâ€™ as the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"""As its core activity , the Standing Committee that oversees the One Hundred Y ear Study forms a Study Panel every five years to assess the current state of AI.""","The quoted sentence directly states that the Standing Committee creates a new Study Panel every five years, establishing the interval of formation."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,li2025b,https://arxiv.org/pdf/2304.03271,Apple reports that its supply chain accounts for 99% of its total water footprint [23].,"The quoted statement from Doc [li2025b] directly states that Appleâ€™s supply chain accounts for 99% of its total water footprint, providing the required percentage."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,False,0,is_blank,is_blank,is_blank,"IBMâ€™s Watson program, which beat human contenders to win the Jeopardy challenge in 2011","The cited passage explicitly states that Watson beat human contenders, contradicting the claim that it did NOT beat them, thus the statement is false."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity,granularity,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""the ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability.""","The quoted passage explicitly names ""granularity"" as the metric used to assess the ratio of computation to communication time for distributed training across continents."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10â€“50 medium-length completions,"[10, 50]",completions,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT-3 needs to â€œdrinkâ€ (i.e., consume) a 500ml bottle of water for roughly 10 â€“ 50 medium-length responses, depending on when and where it is deployed.""",The document states that a 500â€¯mL bottle of water supports about 10 to 50 mediumâ€‘length GPTâ€‘3 responses. Thus the range of completions producible from one bottle is 10â€“50.
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10â€“50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,"""10â€“50 queries on GPTâ€‘3 consumes around half a liter of water"" and ""GPTâ€‘3 needs to â€œdrinkâ€ a 500ml bottle of water for roughly 10 â€“ 50 mediumâ€‘length responses"",""","Both documents state that between ten and fifty GPTâ€‘3 queries use about half a liter of water, so the answer is the range 10â€“50 queries."
q168,The 2024 Griggs et al. paper reports that MÃ©lange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Compared to using only a single GPU type, MÃ©lange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.""",The quoted passage from Griggs et al. (2024) explicitly states that the maximum cost reduction achieved by MÃ©lange in conversational chat settings is 77%.
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8 â€“ 3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The energy required to pre-train an LLM spans from as little as 0.8â€¯MWh (OLMoâ€¯20M) to 3,500â€¯MWh (LLaMaâ€¯4â€¯Scout).","The quoted sentence from DocÂ luccioni2025c provides the minimum and maximum publicly reported energy consumptions for preâ€‘training large language models, establishing the requested range."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,is_blank,is_blank,"JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Table 4: MT-Bench score comparison of various models
JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment","The table lists JetMoE-8B-chat with a score of 6.681, which is higher than Llama-2-13b-Chatâ€™s 6.650, indicating the score achieved after alignment."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"""every U.S. household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].""","The context explicitly states that the average U.S. household had 25 connected devices in 2021, citing Deloitte 2021. This directly answers the question with the numeric value 25 and the unit 'devices'."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",178.97 kg of CO2eq,178.97,kg,is_blank,is_blank,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of ð¶ð‘‚2ð‘’ð‘ž.""","The context explicitly states that the Power Hungry Processing study emitted 178.97â€¯kg of COâ‚‚ equivalent, which is the total emissions for the entire study."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",7.4 times more,7.4,is_blank,luccioni2025a;luccioni2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2504.00797,"""a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the companyâ€™s carbon removal targets for the year"" and ""a single contract to use AI to expand oil production â€œcould enable carbon emissions adding up to 640 percent of the companyâ€™s carbon removal targets""","The coalition estimate states 640â€¯% more emissions, which translates to a 7.4â€‘fold increase over the target (640â€¯% + 100â€¯% = 740â€¯% of target)."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",43.1%,43.1,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon Workforce (All Levels) 43.1%56.8%""",The table for Amazonâ€™s 2023 workforce shows that 43.1% of employees in the United States identified as men across all levels. This figure is taken directly from the row labeled â€˜Amazon Workforce (All Levels)â€™ in the provided document.
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,is_blank,is_blank,"Training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","All cited documents explicitly state that training a Llamaâ€‘3.1â€‘scale model emits enough air pollutants to equal more than 10,000 round trips from Los Angeles to New York City, providing the numerical basis for the answer."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,65B 8 64 4 128,"The table in the cited documents lists the bare minimum GPU counts for each model. For LLaMA-65B on A100 80GB GPUs, the count shown is 4, indicating that four A100 GPUs are the minimum required without compression or quantization."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","A full training run of the 6.1B model consumes about 103,500â€¯kWh. Matching that energy cost with BLOOMzâ€‘7B would require roughly 1.0â€¯billion inferences.","[103500, 1035000000]",is_blank,dodge2022;luccioni2024,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2311.16863,"From Dodge et al. 2022: â€˜... would be approximately (60/8)âˆ—13.8 = 103.5â€¯MWh, or 103,500â€¯kWh.â€™ From Luccioni et al. 2024: â€˜Inference energy (kWh) 1.0â€¯Ã—â€¯10âˆ’4 â€¦ Cost parity (# inferences) 592,570,000.â€™","The 6.1â€‘B model fullâ€‘training energy is 103,500â€¯kWh as cited by Dodge et al. The BLOOMzâ€‘7B inference energy is 1.0eâ€‘4â€¯kWh per inference; dividing 103,500â€¯kWh by 0.0001â€¯kWh/inference gives â‰ˆ1.035â€¯Ã—â€¯10^9 inferences, i.e., about one billion."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,For instance GPT-4o consumes around 2.875 Wh while GPT-4o miniâ€™s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.,"The quoted passage shows GPT-4o mini consumes 3.098 Wh per query, which is higher than GPT-4oâ€™s 2.875 Wh, so the claim that GPT-4o mini consumes less energy per query is false."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.516 per hour,7.516,$/h,is_blank,is_blank,"""resulting in a normalized price of (4.69/2.29) Ã— 3.67 = $7.516 for H100.""","The passage explicitly states that the normalized onâ€‘demand hourly price for an H100 GPU is $7.516, derived from the RunPod pricing and adjusted to match major cloud pricing structures."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain a specific numeric value for the amount of water used for cooling during OpenAIâ€™s GPTâ€‘4 training run, so the answer cannot be determined with confidence from the documents."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",About $7.22 per hour,7.22,USD/hr,griggs2024,https://arxiv.org/pdf/2404.14527,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.""","The context states a monthly cost of $5,200 for two A100 GPUs. Dividing $5,200 by 720 hours (30 days Ã— 24 hours) yields approximately $7.22 per hour. This matches the estimate in the answer."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80â€“90%,"[80, 90]",%,is_blank,is_blank,"""NVIDIA estimated that 80â€“90% of the ML workload is inference processing [Leo19].""",The quoted sentence from the Patterson 2021 document directly states that NVIDIA estimated the inference portion of the ML workload to be between 80% and 90%.
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emissions for training a Transformer with NAS (626,155 lbs) and mention that this is about five times the lifetime emissions of a car, but they do not give a specific emissionsâ€‘toâ€‘drivingâ€‘distance ratio or the number of miles a car travels in its lifetime. Without that ratio, the driving distance equivalent cannot be calculated from the provided information."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,is_blank,is_blank,"""The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.""","The cited passage from the 2025 paper states that after the 2022 peak the trend reversed and disclosures dropped, rather than continued to increase."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",1000Ã— larger,1000,is_blank,is_blank,is_blank,"For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000Ã— larger in size.","The quoted statement directly states that boosting the BLEU score from 5 to 40 for a GPTâ€‘3-based translation model necessitates a model that is 1,000 times larger than the baseline."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B ... using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.","The abstract of the JetMoE paper states that JetMoE-8B was trained using 30,000 H100 GPU hours, which directly answers the question about the GPU hours consumed during preâ€‘training."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,is_blank,is_blank,"""Estimations using TDP are nearly always an overestimation since it is rare for a GPU â€“ or any computing device â€“ to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worstâ€‘case overestimation of energy consumption by a factor of 4.1."" (Doc Chung2025#0024) 

""While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding realâ€‘time power consumption is only possible by using a tool like Code Carbon... Nonetheless, the TDPâ€‘based approach is often used in practice when estimating the carbon emissions of AI model training and it remains a fair approximation of the actual energy consumption of many hardware models."" (Doc Luccioni2023#0013)","The quoted passage from Chung2025 shows that TDPâ€‘based estimates are usually higher than actual usage, sometimes by a large factor, indicating unreliability. Luccioni2023 acknowledges that TDP gives only a fair approximation and not precise accuracy. Together these sources confirm that TDP is not a reliable and accurate method for estimating GPU energy consumption."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"""If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027""","The passage states that with current growth rates, the cost of the largest training runs will exceed one billion dollars by 2027, providing the required threshold."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for AlexNetâ€™s top-1 accuracy on ImageNet, so the answer cannot be determined with confidence from the documents."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,610 MWh",60610,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Inference energy (kWh) 1.0 Ã— 10âˆ’4"" from Table 5 in Doc [luccioni2024]","The inference energy per query for BLOOMzâ€‘7B is 1.0â€¯Ã—â€¯10â»â´â€¯kWh. One million inferences per download use 1.0â€¯Ã—â€¯10â»â´â€¯kWhâ€¯Ã—â€¯1,000,000â€¯=â€¯100â€¯kWh per download. With 606,096 downloads, total energy is 606,096â€¯Ã—â€¯100â€¯kWhâ€¯=â€¯60,609,600â€¯kWh, which equals 60,609.6â€¯MWh, rounded to 60,610â€¯MWh."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"FAIRâ€™s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context explicitly states that training FAIRâ€™s RoBERTa on 160â€¯GB of text required about 25,000 GPU hours, which directly answers the question."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 Ã—80G) servers.""","The text states the cluster has 24 servers, each containing 8 DGXâ€‘A800 GPUs; multiplying 24 by 8 yields 192 GPUs in total."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].,"The quoted sentence from the PattersonÂ 2021 document gives the exact number of floating point operations that OpenAI reported for GPTâ€‘3 training, which is 3.14Ã—10^23 FLOPs."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,is_blank,is_blank,"""Microsoft reporting a 34% increase in global water consumption between 2021 and 2022""","The context explicitly states that Microsoft reported a 34% increase in global water consumption during the specified period, which directly answers the question."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give the CO2 emissions from NAS (284 metric tons or 626,155 lbs) and compare it to five US cars, but they do not provide any information about equivalent emissions relative to an average American lifetime. Therefore the answer to the second part of the question cannot be derived from the given context."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,"The provided figures (e.g., Fig.Â 14 for Mixtral-CSâ€A100â€‘40GB dense) list throughput values at batch sizes 0,â€¯5,â€¯10,â€¯15,â€¯200 but do not include a data point for batch sizeâ€¯1. The text does not give a numeric ground truth throughput for batch sizeâ€¯1.","The context contains throughput data only for selected batch sizes (0,â€¯5,â€¯10,â€¯15,â€¯200) and no explicit value for batch sizeâ€¯1, so the answer cannot be inferred with confidence."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,MÃ©lange,MÃ©lange,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"""we introduce MÃ©lange, a GPU allocation framework that navigates these diverse LLM service characteristics and heterogeneous GPU option space to automatically and efficiently derive the minimal-cost GPU allocation for a given LLM service.""","The context states that MÃ©lange is a GPU allocation framework designed to deploy large language models by efficiently allocating GPUs across services, making it the framework referenced for multiâ€‘GPU deployment."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 NVIDIA V100 32GB GPUs,8,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,65B 8 64 4 128,"Table II lists the bare minimum hardware for LLaMAâ€‘65B on V100 32GB GPUs, showing a count of 8 GPUs is required."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a direct figure or sufficient data (such as total sessions and water usage) to calculate gallons of water consumed per ChatGPT user session in 2023.
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons of CO2e",47400,metric tons of CO2e,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.""","The document states that Amazonâ€™s onâ€‘site solar systems generate 123,000â€¯MWh annually and avoid roughly 47,400â€¯metric tons of COâ‚‚e compared to nonrenewable electricity sources, which directly answers the question."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided passages mention Yelp sentiment analysis benchmarks or compare traditional models to large language models in that context, so the answer cannot be determined from the given documents."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,homes,jegham2025,https://arxiv.org/pdf/2505.09598,"These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The document states that GPTâ€‘4oâ€™s projected annual energy use (391,509â€“463,269â€¯MWh) exceeds the annual consumption of 35,000 U.S. residential households, so the comparable number of homes is 35,000."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Table 3: OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0,"The table lists the OpenLLM Leaderboard average for each model; the JetMoE-8B column shows 53.0, which is its final average score."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied context passages contain information about the number of AI training runs conducted globally on renewable-only power in 2022, so a confident answer cannot be derived."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11.""","The cited passage explicitly states the PUE value for Google's Iowa datacenter during the Evolved Transformer run, confirming the answer of 1.11."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,TRUE,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Openâ€‘source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2)""","The passage explicitly states that openâ€‘source generalâ€‘purpose AI models are excluded from transparency requirements, including energyâ€‘consumption reporting, unless they pose a systemic risk, supporting the statement that they are fully exempt except in that case."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,3.28,3.28,is_blank,chung2025,https://arxiv.org/pdf/2505.06371,"Table 4: Energy per generation (Joules) and the percentage of decode energy consumption with PD disaggregation. Following recent trace analysis [79], we sampled input lengths from a Pareto distribution with alpha 2.5, and output lengths from an Exponential distribution, each with mean specified in the table. TP means tensor parallelism degree, and xPyD means it was deployed with x prefill instances and y decode instances, each wit

Llama 3.1 70B (TP=4, 1P1D) 276.93, 64.8% 907.60, 89.2% 1492.59, 50.0%","The table lists energy per generation for the Llama 3.1â€¯70B model under different node deployments. 276.93â€¯J corresponds to one node, and 907.60â€¯J corresponds to two nodes. Dividing 907.60 by 276.93 gives a factor of approximately 3.28, indicating that energy consumption increased by about 3.3Ã— when deploying on two nodes instead of one."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"In 2020, it was 1.59.","The Patterson2021 document explicitly states that the US national datacenter average PUE in 2020 was 1.59, providing the required numeric answer."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion,772000000000,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,yielding a total of approximately 772 billion GPT-4o queries in 2025,"The analysis explicitly states that the projected number of GPTâ€‘4o queries in 2025 is about 772â€¯billion, as shown in the quoted sentence."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,is_blank,is_blank,"""When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.""","The context explicitly states that for the OPT-2.7B model on a g4dn.xlarge instance, the KV Cache grows to 5.312GB when the batch size is 32."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""R&D staff costs including equity are between 29% and 49% of the total amortized cost.""","The cited passage explicitly states that for the four notable models, R&D staff costs (with equity) fall within a 29% to 49% range of the total amortized cost, which directly answers the question."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,is_blank,is_blank,"""In our inference experiments, we measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking""; ""used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference""","Both documents explicitly state that the CodeCarbon package was used to track and measure energy consumption during inference runs, making it the software package referenced for this purpose."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,TRUE,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""increasing the number of shards still tends to increase the energy costs of inference per response most overall""","The cited passage explicitly states that as the number of GPU shards increases, the energy cost per response for LLaMAâ€‘65B also increases, which directly supports the true statement."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""53% of articles cite the figure of 3â€¯Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search""","The analysis of 100 news articles reported that 53% of them quoted the contested estimate of 3â€¯Wh (or 10Ã— a Google search), so the required percentage is 53%."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Openâ€‘source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).""","The quoted passage states that openâ€‘source generalâ€‘purpose AI models are largely excluded from the current EU transparency requirements, so they are not required to report their energy consumption to authorities under the existing rules."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a numeric value for the total execution time of a sparse BlackMamba model fineâ€‘tuned on an NVIDIA A40â€‘48GB with a batch size of 84. The only occurrences of â€œSparse(bsz=84)â€ are in table headers without associated execution time numbers, so the answer cannot be determined with confidence."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]","The quoted sentence from Doc [luccioni2025a] specifies that these four companies accounted for roughly 30% of all corporate PPAs in 2020, providing the required percentage."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, MÃ©lange achieved cost reductions in what percentage range compared to single-GPU baselines?",15-77%,"[15, 77]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"Short-context Dataset (Arena). In Figs. 11a and 11d, MÃ©lange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The provided passage states that for the short-context Arena dataset with a 120â€¯ms SLO, MÃ©lange achieves a cost reduction ranging from 15â€¯% to 77â€¯% compared to singleâ€‘GPU baselines."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 metric tons CO2 equivalent,26,tCO2e,li2025a,https://arxiv.org/pdf/2309.03852,Table 3: net tCO2e 26,"Table 3 in the provided document lists the net carbon emissions for FLMâ€‘101B as 26 metric tons of COâ‚‚e, which directly answers the question."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,is_blank,is_blank,the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.,"The abstract of the paper explicitly names the function as the Compute Time Calibration Function (CTCF) and states that it improves instance selection accuracy by aligning theoretical GPU performance with actual performance, which is the requested function."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include a numeric energy consumption value for the o3 (GPT-4o) model on a long prompt, so the factor relative to GPT-4.1 nano cannot be determined from the documents."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.","The text explicitly states that the 2023 public health cost for U.S. data centers is about $6.7â€¯billion, which is derived using the average attribution method described earlier in the document."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain a clear numeric value for the total execution time of a sparse Mixtral model with batch size 1 on an NVIDIA A40-48GB GPU. The figure references and table snippets are incomplete and do not provide the required total time.
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The quoted passage explicitly states that Ollama was used to apply 4â€‘bit quantization and is described as an openâ€‘source platform that supports local (edge) deployment of large language models, which directly answers the question."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.0022 kL,0.0022,kL,morrison2025,https://arxiv.org/pdf/2503.05804,"""Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU."",","The document states that mining the rare earth content of an H100 GPU consumes 2.2 liters of water. Converting liters to kiloliters (1 kL = 1000 L) gives 2.2â€¯L Ã· 1000 = 0.0022â€¯kL, which is the estimated water consumption per GPU."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Senator Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""",The context explicitly states that Senator Edward J. Markey introduced the AI Environmental Impacts Act bill on 1 February 2024.
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,False,0,is_blank,is_blank,is_blank,"""Importantly, the health costs are unevenly distributed across counties and communities, particularly affecting lowâ€‘income counties that could experience approximately 200x perâ€‘household health costs than others.""","The quoted passage from Han et al. (2024) explicitly states that public health costs are unevenly distributed across counties and communities, contradicting the claim that they are evenly distributed."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement or data about the estimated average GPU lifetime before retirement in AI data centers in 2024, so the question cannot be answered with confidence from these documents."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Approximately 4.9 zettaFLOPs,4.9,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,"""The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec"" (throughput per GPU) 
""The FLM-101B model is trained on a cluster of 24 DGX-A800 GPU (8 Ã—80G) servers"" (192 GPUs total) 
""The 101B model with 26.54B tokens"" (tokens processed in final stage) 
""total time cost for training FLM-101B is 21.54 days"" (overall training time)","The throughput per GPU is 160â€¯TFLOPs/sec, and with 192 GPUs the cluster runs at about 30.7â€¯PFLOPs/sec.  The final 101B stage processes 26.54â€¯B tokens out of 311.55â€¯B tokens total, so it occupies roughly 8.5â€¯% of the 21.54â€‘day training period (~1.83â€¯days).  Multiplying throughput by duration yields â‰ˆ4.9â€¯zettaFLOPs of work for the final stage."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 GPUs,2,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,13B 2 64 1 64,"The table of bare minimum hardware lists the number of V100 32GB GPUs needed for each LLaMA variant. For the 13B model the entry shows â€˜2â€™, indicating two V100 GPUs are required."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Research shows that in North America, AWS can lower its customersâ€™ workload carbon footprints by up to 96% compared to onâ€‘premises computing workloads when the electricity AWS uses is matched with 100% renewable energy","The provided context states a 96% reduction in carbon footprints for North American workloads when moved to AWS, indicating the typical percent reduction customers can expect."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, which is over four times the ""five cars"" estimate.",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the â€œfive carsâ€ number.""","The context explicitly states the Gemma preâ€‘training emissions as 1247.61â€¯tCO2e and notes that this figure is more than four times the ""five cars"" baseline, directly supporting the answer."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,2 weeks (336 hours),336,hours,strubell2019,https://arxiv.org/pdf/1906.02243,"""Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).""","The provided context states that Peters et al. (2018) trained ELMo on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours. This matches the requested GPU configuration (3 NVIDIA GTX 1080 Ti GPUs) and gives the training duration."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,"""fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.""","The context explicitly states that for 2â€¯million queries on a sparse Mixtral model, the net cost on an NVIDIA H100 GPU is $3460, which directly answers the question."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided excerpts explicitly state that the relationship between runtime and energy consumption in the inference experiments with large language models is nearly linear. The documents discuss energy measurements, power capping, and the overall energy burden of inference but do not provide a statement or data indicating a nearly linear relationship between runtime and energy consumption. Therefore, the answer cannot be determined with confidence from the given material."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided contexts do not contain a specific numeric PUE value for Google's hyperscale data centers in 2021.
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a statement from Chen et al. (2025) specifying the price per hour for an NVIDIA H100.
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit statement indicating which storage service was used to shard and stream datasets for spot VMs that could terminate at any time. Therefore, I cannot confidently provide the requested information."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years"" (FigureÂ 2, DocÂ [wu2021b#0010] and DocÂ [wu2021b])","The quoted statement directly asserts that GPU theoretical performance per watt doubles approximately every 3â€“4 years, confirming the claim in the question."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents discuss CO2e estimates for AI workloads and specific computations, but contain no information about the average global CO2e footprint of an average human life in one year."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50â€“70%,"[50, 70]",%,is_blank,is_blank,"GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50â€“70% of the total provisioned power in the datacenter [52â€“54, 58].","The quoted sentence from Chung et al. explicitly states that GPUs consume 50â€“70% of the total provisioned power in a typical datacenter, providing the answer as a range."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,shen2024,https://arxiv.org/pdf/2404.07413,We conduct training on a cluster containing 12 nodes and 96 H100s.,"The document explicitly states that the training cluster comprised 12 nodes with a total of 96 H100 GPUs, giving the total count of GPUs used for training."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context explicitly states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh, making this the value to report."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"""which is about 280% more expensive than InferSaveâ€™s top choice.""","The context states that for a 400â€¯TPS SLO, Max-Performanceâ€™s g6e.xlarge costs $2.699 versus InferSaveâ€™s top choice at $0.71, which is described as about 280% more expensive."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""In addition, we propose reporting the financial cost or â€˜price tagâ€™ of developing, training, and running modelsâ€¦Reporting the computational price tag of finding, training, and running models is a key Green AI practice"".","The passage explicitly states that Green AI practice includes reporting the financial cost of finding, training, and running models, confirming the statement as true."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,A single short GPT-4o query consumes 0.42 Wh (Â±0.13 Wh).,"The cited sentence directly reports the energy consumption for a short GPT-4o query, providing the required value of 0.42 Wh."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,Approximately 1.25Ã— speedup,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The quoted passage states that for the 13B model, inference throughput on A100 GPUs is about 1.25 times that on V100 GPUs, indicating a 1.25Ã— speedup."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"As shown in Figure 8, OpenAIâ€™s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884)","The passage from Jeghamâ€¯2025 explicitly states that o3â€‘mini has the highest crossâ€‘efficiency score of 0.884, making it the top-ranked model in the ecoâ€‘efficiency DEA analysis."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,is_blank,is_blank,"Processor Average (Watts)  StDev  %  DNNs   used   to   calculate   average   power
TPU   v2   221   5%   Transformer ...
V100   GPU   325   2%   Transformer ...",The context lists the average power per processor: TPU v2 consumes 221â€¯W and V100 GPU consumes 325â€¯W.  The difference is 325â€¯Wâ€¯â€“â€¯221â€¯Wâ€¯=â€¯104â€¯W.
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""current averages of less than 3 years for cell phones [Cordella et al., 2020]""","The context from Wuâ€¯2021â€¯b explicitly states that the current average lifetime of cell phones is less than 3 years, supporting the claim that smartphones average lifetimes of under 3 years."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,is_blank,is_blank,"""training the GPT-3 language model in Microsoftâ€™s stateâ€‘ofâ€‘theâ€‘art U.S. data centers can directly evaporate 700,000 liters of clean freshwater""","The quoted sentence in document li2025b#0000 explicitly states that the GPTâ€‘3 training process evaporates 700,000 liters of clean freshwater, which directly answers the question."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain explicit information comparing GPU architecturesâ€™ energy efficiency specifically for models that generate only a single classification token. Therefore, a confident answer cannot be given."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"When actively training, the average GPU power is over 600W, over 85% of an H100â€™s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that during active training the average GPU power exceeds 600â€¯W. Therefore the answer is â€œover 600â€¯Wâ€, with 600 as the numeric lower bound and the unit W."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"AIâ€™s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The quoted sentence from the 2025 paper explicitly states that worldwide eâ€‘waste generation reached 62â€¯million tonnes in 2022, providing the required figure."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about the percentage of Amazonâ€™s People Managers globally who identified as women in 2023.
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20Ã—,20,times,wu2021a,https://arxiv.org/pdf/2111.00364,"""Facebookâ€™s recommendation and ranking model sizes have increased by 20 times during the same time period [11].""","The context explicitly states a 20â€‘fold increase in Facebookâ€™s recommendation and ranking model sizes between 2019 and 2021, which is reflected as a 20Ã— increase."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 â€“ 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,"""a recent study suggests that the global AI could consume 85 â€“ 134 TWh of electricity in 2027""","The quoted passage from the study in Doc [li2025b] directly states the projected electricity consumption range for global AI in 2027 as 85 to 134 TWh, which is the answer to the question."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,is_blank,is_blank,"""The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954 pt""","The quoted sentence from Strubell et al. (2019) gives the EPAâ€™s average CO2 emission factor as 0.954 pounds per kilowattâ€‘hour, which directly answers the question."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"a context window of 2,048 tokens","The FLM-101B Configurations section states that the modelâ€™s context window is 2,048 tokens, which directly answers the question."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61â€“76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""However, if we exclude equity the fraction for R&D staff drops to 19â€“33%, and the fractions of computing hardware costs and energy rise to 61â€“76% and 2â€“7% respectively.""","The quoted passage from Cottier et al. (2025) explicitly states that, when equity is excluded, computing hardware accounts for 61â€“76% of the total amortized cost for the four key models."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150,150,million packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We delivered 150 million packages via EVs.""","The Europe section of the 2023 sustainability report states Amazon delivered 150â€¯million packages via electric vehicles in 2023, which directly answers the question."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration","The quoted passage explicitly states that the AI Act does not address GHG emissions from AI applications, meaning it does not mandate disclosure of such emissions."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,is_blank,is_blank,"""metrics such as F1 score and overall accuracy may decline slightly post-optimization""","The supporting quote from the financial modeling discussion states that accuracy and F1 can decline after optimization, indicating they do not always improve."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,True,1,is_blank,is_blank,is_blank,"""LLM decoding is characterized by low compute-intensityâ€¦ GPUâ€™s computation units being underutilized, leading to VRAM bandwidth bottlenecking the GPUâ€™s throughput."" (Doc chung2025#0086) ""LLMâ€™s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion modelâ€™s power consumption is close to the maximum."" (Doc chung2025#0028)","The quoted passages state that LLM decoding has low compute-intensity and is limited by VRAM bandwidth, resulting in lower power draw compared to diffusion models, which consume power close to the GPUâ€™s maximum. This directly supports the claim that LLMs generally have lower power draw during inference."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""up to 80% in West US""","The figure caption states that for the short DenseNet 201 job, the Flexible Start optimization can yield up to an 80% reduction in CO2 emissions in the West US region, which is the maximum potential reduction mentioned."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied contexts mention Yelp sentiment analysis benchmarks or compare traditional models to large language models in that setting. Therefore the statement cannot be verified or refuted with the given documents.
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,GPU accounts for 74% of the total,"The context states that the GPU alone accounts for 74% of the electricity consumption during the BERT-base training experiment, directly answering the question."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244,244,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"United States 244 17,706",The table of Amazon Renewable Energy Projects shows the United States entry with 244 projects announced as of JanuaryÂ 2024.
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain a specific numeric value for the total number of tokens processed during the entire online inference workload evaluation, so a confident answer cannot be derived."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450,1450,is_blank,is_blank,is_blank,"Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. image generation 2.907 3.31
text classification 0.002 0.001
â€¦the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The table gives the mean energy for image generation (2.907â€¯kWh) and for text classification (0.002â€¯kWh). Dividing 2.907 by 0.002 yields approximately 1453, which the paper describes as â€œover 1450â€. Thus the most intensive task exceeds the least intensive by about 1450 times."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,GPUs,griggs2024,https://arxiv.org/pdf/2404.14527,"""Serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs""","The context explicitly states that serving Llama2â€‘70b at BF16 precision requires two NVIDIA A100â€‘80GB GPUs, supporting the numeric answer of 2."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the amount of fiber optic cable installed globally for AI workloads in 2023, so a confident answer cannot be derived."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.""",The authors explicitly state that reporting at the cumulative server level balances accuracy (captures total computationâ€‘related power) and feasibility (allows estimation when direct measurement is not possible).
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,TRUE,1,is_blank,is_blank,is_blank,"""CVâ€™s per-GPU speedup (speedup #GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)""","The cited perâ€‘GPU speedup values for CV on T4 GPUs remain essentially constant as the number of GPUs increases, indicating nearly linear intraâ€‘zone scaling for CV models."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5%,28.5,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""28.5% operational energy footprint reduction over the two-year time period"" (FigureÂ 8)","The cited figure and text state that iterative hardwareâ€‘software optimization led to a 28.5% reduction in operational energy footprint over the 2019â€‘2021 period, which directly answers the question."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain projections from Lawrence Berkeley National Laboratory and other sources, but none reference McKinsey projections for 2030 or give a percentage of U.S. national electricity consumption for data centers in that year. Therefore the answer cannot be determined from the available information."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,Amazonâ€™s AWS only covered fifty percent of its power usage with renewable energy.,"The quoted sentence from the 2019 context states that AWS covered 50â€¯% of its power usage with renewable energy, which corresponds to the 2018 figure requested."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google reports a 48% increase in GHG emissions since 2019""","The quoted sentence from the 2024 ESG report states a 48% increase in GHG emissions since 2019, directly providing the requested percentage."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800M,800,USD,is_blank,is_blank,"""we estimate that it cost $800M to acquire the hardware used to train GPT-4""",The quoted sentence from the cited paper directly states that the estimated upfront hardware acquisition cost for training GPTâ€‘4 is $800 million USD.
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping consumes less energy,Swapping,is_blank,is_blank,is_blank,"""when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.""","The cited figure description from Chung2025#0079 directly states that under overload, Swapping consistently consumes less energy than Recomputation, providing the basis for the answer."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,li2025b,https://arxiv.org/pdf/2304.03271,"GPT-3 was trained and deployed by OpenAI in Microsoftâ€™s data centers, with an estimated training energy of 1287 MWh [29].","The document li2025b explicitly states that the estimated training energy for the full GPT-3 model is 1287â€¯MWh, which directly answers the question."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,is_blank,is_blank,"""The umbrella term â€˜Sustainable AIâ€™ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.""","The quoted passage shows that Sustainable AI was defined to include both climate-positive applications and improving AIâ€™s environmental sustainability, so it is not limited to only climate-positive uses."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,about 70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,"""reducing inference computation by about 70% compared to Llama2-7B.""","The context states that JetMoE-8B reduces inference computation by about 70% relative to Llama2-7B, which directly answers the question."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",11.4%,11.4,%,dodge2022,https://arxiv.org/pdf/2206.05229,"Table 5: ""P&R 11.4%"" for the 6B Dense transformer model (pause and resume optimization).","The table shows that the Pause and Resume optimization achieves a 11.4% reduction in COâ‚‚ emissions for the 6B parameter transformer, which represents the maximum potential saving reported."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].""","The quoted sentence from the 2019 Strubell study directly provides the carbon footprint of training BERT as 626,155 pounds of CO2e."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a specific figure for the total hectares of land occupied by new AI data centers globally in 2022, so a confident answer cannot be derived from them."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",True,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fineâ€‘tuning."" (xia2024#0021)  ""Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fineâ€‘tuning."" (xia2024)","Both passages explicitly state that the MoE layer is a primary target for optimization to improve fineâ€‘tuning performance, confirming the statement is true."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 gCO2eq,0.32,gCO2eq,is_blank,is_blank,"""for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of ð¶ð‘‚2ð‘’ð‘žper 1,000 queries""","The specified passage states that the BERT-based model bertâ€‘baseâ€‘multilingualâ€‘uncasedâ€‘sentiment emits 0.32â€¯gCO2eq per 1,000 text classification queries, providing the required value."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MkWh,40,MkWh,li2025a,https://arxiv.org/pdf/2309.03852,Table 3: Energy (MkWh) 40,"The table lists the total energy consumption for FLMâ€‘101B as 40â€¯MkWh, which is the value requested."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.",The supporting passage explicitly states that 2022 was the year when direct environmental disclosures peaked.
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents discuss Googleâ€™s overall data center water usage but do not provide a specific figure for the freshwater consumption of DeepMind AlphaFold servers in 2023.
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",7k to 26k grams,"[7, 26]",k grams,dodge2022,https://arxiv.org/pdf/2206.05229,"( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The quoted sentence from Dodge et al. gives the emissions for the most efficient region (7kâ€¯grams) and the least efficient region (26kâ€¯grams), indicating a range of 7 to 26 thousand grams."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context contains projections for 2028 but does not mention any projected value for 2030, so the question cannot be answered with the given information."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,is_blank,is_blank,A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.,"The context explicitly states that adding compute resources to accelerate the MoE layers reduces the cost of fine-tuning, contradicting the claim that it can increase costs. Therefore the statement is false."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",TRUE,1,is_blank,is_blank,is_blank,"""only 7% slower than the fully local A-8 experiment"" and ""the throughput slowdown is almost identical (CV 7%, NLP 35%)""","Both documents state that intercontinental training for CV with high granularity slows performance by only 7% compared to local training, confirming the statement as true."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.""","The cited passage from ebert2024 explicitly states that GPU-level monitoring is discouraged for reporting overall AI energy use, contradicting the claim that it is recommended."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""finding that training accounted for only half of the modelâ€™s overall emissions [121]""","The supporting passage states that training accounted for only half of the modelâ€™s overall emissions, which corresponds to 50% of the total carbon footprint."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,$32.7,32.7,$,xia2024,https://arxiv.org/pdf/2408.04693,Table IV: ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE â€“ A40 48GB â€¦ Cost ($) 32.7,"The table lists the total cost for fineâ€‘tuning Mixtral on the GSM8K (GS) dataset with sparse MoE on an NVIDIA A40â€‘48GB GPU as $32.7, which is the value requested."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not give a specific water consumption value for an OLMo 60M model trained on 1.7 to 5.6 trillion tokens, nor does it convert any such value to days for a single U.S. person. Therefore the answer cannot be derived with confidence from the documents. "
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.30 metric tons,8.3,metric tons,is_blank,is_blank,"""one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)""",The provided context directly gives the estimate of 8.30 metric tons per year for average US home energy use.
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64,"The table of bare minimum hardware shows that for the LLaMAâ€‘7B model the required number of V100 32GB GPUs is 1, as indicated by the count of 1 in the V100 column."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts from Doc [shen2024] do not contain any mention of JetMoE-8Bâ€™s score on the GSM8k benchmark, so the answer cannot be determined from the given documents."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",Approximately 6.5 user requests,6.52,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,Table 1: Estimate of GPT-3â€™s operational water consumption footprint. ... Arizona 1.180 1.630 4.959 2.098 7.531 9.629 6.520 ...,"The table lists the column â€œ# of Requests for 500mlâ€ for Arizona as 6.520, indicating that roughly six and a half requests consume one 500â€¯ml bottle of water during GPTâ€‘3 training."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not include the maximum batch size for fineâ€‘tuning Mixtral in a dense configuration on the Hellaswag dataset using an NVIDIA A40 GPU with 48â€¯GB memory, so the answer cannot be determined with confidence."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",32 samples,32,samples,is_blank,is_blank,"Table III lists the maximum batch size for a sparse Mixtral model on the CS dataset with an NVIDIA A40-48â€¯GB GPU as ""Sparse(bsz=32)"". This is the batch size used for the longestâ€‘running MoE layer in the profiling study.","The table explicitly states that for a sparse Mixtral model fineâ€‘tuned on an A40â€‘48â€¯GB GPU, the maximum batch size supported (which corresponds to the longestâ€‘running MoE layer) is 32 samples."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",7.5 seconds,7.5,seconds,xia2024,https://arxiv.org/pdf/2408.04693,0.0 2.0 4.0 6.0 8.0 Forward Backward Optimizer Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84) 0.0 0.5 1.0 1.5 2.0,The table shows forward/backward execution times for each batch size; the 6.0â€‘second value corresponds to the sparse Mixtral model with batch size 10. The optimizer stage takes 1.5 seconds for the same configuration. Summing these gives a total execution time of 7.5 seconds.
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear, explicit numeric statement of the ground truth maximum batch size for fineâ€‘tuning Mixtral on an NVIDIA A100â€‘40GB GPU."
