id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,0,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided document (wu2021b) does not contain any information indicating that AWS began using recycled water for cooling in any data centers in 2023. No mention of AWS, recycled water, or data center cooling practices for that year is present in the text."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","Approximately 463 kg CO₂‑equivalent per NVIDIA GPU (based on an assumption of 3,700 kg CO₂‑eq per 8‑GPU node, i.e., 463 kg per GPU).",is_blank,is_blank,is_blank,is_blank,is_blank,"The MORRISON2025 paper notes that NVIDIA does not publish embodied‑carbon data for its GPUs.  The authors therefore adopt the embodied‑carbon estimate reported for a comparable 8‑GPU DGX‑2H server – 3,700 kg CO₂‑eq – which yields an average of 463 kg CO₂‑eq per GPU.  This figure is used throughout the paper to compute the embodied‑carbon contribution of training runs."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,is_blank,is_blank,is_blank,is_blank,is_blank,"The FLM‑101B was trained within a total budget of 100,000 USD (≈$100 k).  In the cost analysis of frontier models, the amortized hardware + energy cost reported for GPT‑4 is about 40 million USD (≈$40 M).  Dividing the GPT‑4 cost by the FLM‑101B budget gives 40 M ÷ 100 k ≈ 400.  Thus, the amortized training cost of GPT‑4 was roughly 400 times greater than the total training budget for FLM‑101B."
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,≈ 600 – 800 kg CO₂‑equivalent per passenger for a round‑trip flight from San Francisco to New York.,is_blank,is_blank,is_blank,is_blank,is_blank,"Commercial air travel emits roughly 0.15–0.20 kg CO₂‑eq per passenger‑kilometre. The great‑circle distance between SF and NY is about 4 000 km, so a round‑trip (~8 000 km) translates to 0.15 kg × 8 000 km ≈ 1 200 kg or, using the lower end, 0.10 kg × 8 000 km ≈ 800 kg. Typical industry estimates for a single passenger round‑trip on this route therefore fall in the 600–800 kg CO₂‑eq range."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,is_blank,is_blank,is_blank,is_blank,is_blank,The paper reports that training FLM‑101B with the growth strategy takes 21.54 days versus an estimated 76.74 days for a from‑scratch 101B model. This corresponds to a 72 % reduction in training time (a 3.56× speed‑up).
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,Approximately 60 days,is_blank,is_blank,is_blank,is_blank,is_blank,"The Dodge et al. (2022) study reports that a partial training run of the 6.1 billion‑parameter transformer lasted about 8 days on 256 A100 GPUs, covering only roughly 13 % of the full training duration. Extrapolating from this partial run, the authors estimate that a complete training run would require about 60 days of compute."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",DeepSeek‑R1 (DS),is_blank,is_blank,is_blank,is_blank,is_blank,"In the appendix’s energy‑per‑query table (see the column for the longest prompt length), the DeepSeek‑R1 model hosted on its own servers (DS) shows the largest energy cost per query at 29.078 Wh.  When multiplied by 1,000 queries, it therefore has the highest total GPU energy consumption among all models listed."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",About 22 % of global e‑waste has been formally collected and recycled (UN Global E‑Waste Monitor 2024).,is_blank,is_blank,is_blank,is_blank,is_blank,"The text explicitly states: ""The UN’s Global E‑Waste Monitor 2024 showed that about 22 % of e‑waste has been shown to be formally collected and recycled,"" indicating that roughly one‑fifth of all e‑waste generated is formally collected and recycled."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,11th Gen Intel Core i7‑1165G7 processor,is_blank,is_blank,is_blank,is_blank,is_blank,"In the ‘energy‑efficient local inference’ experiment reported in the Khan 2025 paper (financial sentiment classification), the authors state that the test machine was equipped with an Intel Core i7‑1165G7 (11th‑generation) CPU running at 2.80 GHz (1.69 GHz base). This processor was used as the hardware platform for the local inference setup."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,0.8,is_blank,is_blank,is_blank,is_blank,is_blank,"The study shows that the carbon footprint of language‑model training scales linearly with GPU utilisation.  If the utilisation is raised from a typical lower baseline (e.g., 50 %) to 80 %, the overall energy (and thus CO₂) required drops proportionally.  Thus the footprint is reduced by a factor equal to the utilisation ratio, i.e. 80 % or 0.8 of the original value."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,The total compute spent during model development is roughly 1.2–4 times larger than the compute used in the final training run alone (median ≈ 2.2×).,is_blank,is_blank,is_blank,is_blank,is_blank,"Cottier et al. (2025) estimate that the ratio of total compute (including experiments, fine‑tuning, and evaluation) to the compute of the final training run ranges from 1.2 × to 4 ×, with a median value of about 2.2 ×. This indicates that, on average, developing a frontier model requires more than twice the compute of its final training run."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons’ Paradox (the rebound effect),is_blank,is_blank,is_blank,is_blank,is_blank,"The paper argues that gains in technical efficiency for AI can paradoxically drive higher overall resource consumption, a classic rebound effect described by William Stanley Jevons in the 19th‑century economic principle known as Jevons’ Paradox."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper reports that, using the aggressive growth strategy (Figure c), the total wall‑clock time needed to train the FLM‑101B model is 21.54 days, a 72‑day time saving (≈3.56× speed‑up) compared to training a 101‑B model from scratch."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",True,is_blank,is_blank,is_blank,is_blank,is_blank,"The Samsi 2024 study reports GPU utilization metrics for Facebook’s LLaMA inference experiments. For the 80 GB A100 GPUs, average SM utilization is 95 % (for 4 shards) and 98 % (for 16 shards), and for the 32 GB V100 GPUs, SM utilization is 94–95 % across 8, 16, and 32 shards. These values all exceed 80 % of GPU capacity, indicating that the majority of the model experimentation workflows at Facebook run with GPUs operating well above the 80 % threshold."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,1287,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper by Jegham et al. (2025) explicitly states that the training of GPT‑3 is estimated to have consumed 1,287 MWh of electricity."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,is_blank,is_blank,is_blank,is_blank,is_blank,"The Jegham 2025 paper describes the ""AI Energy Score project"" as a collaborative effort to create a standardized, scalable framework for comparing the inference efficiency of various AI models across tasks and datasets."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts are activated per token in each MoE layer of JetMoE‑8B (top‑k = 2).,is_blank,is_blank,is_blank,is_blank,is_blank,"The JetMoE‑8B model contains eight experts in every MoE layer. During sparse fine‑tuning – the configuration used for JetMoE‑8B – only the top‑2 experts (k = 2) are selected for activation for each token, as stated in the paper’s description of the MoE setup."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,is_blank,is_blank,is_blank,is_blank,is_blank,"The claim cites a 200,000‑fold increase in compute from 2012 to 2018, but the referenced source (Schwartz et al. 2019) reports an estimated 300,000‑fold increase over the same period. Therefore the specific figure of 200,000× is not supported by the evidence provided."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,"Approximately 70 years old (the field was formally founded at Dartmouth in 1956, so by 2025 it has been about 69–70 years).",is_blank,is_blank,is_blank,is_blank,is_blank,"The modern history of artificial intelligence is generally traced back to the Dartmouth Conference in 1956, where the term ""Artificial Intelligence"" was coined and the field was officially founded. Counting from 1956 to 2025 gives 69 years; rounding up to an approximate age of 70 years is common in popular accounts. Earlier informal work on logical reasoning and computation dates from the 1940s, but the formal, organized field began with the 1956 Dartmouth workshop."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS),is_blank,is_blank,is_blank,is_blank,is_blank,"The 2019 Strubell et al. study estimated the carbon cost of training a large Transformer model as part of a neural‑architecture‑search (NAS) procedure.  The NAS training run is a one‑off, infrequent process that requires a huge compute budget, and the resulting estimate of five cars’ worth of CO₂ emissions was derived from this single NAS experiment."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",2.4 gigawatts,is_blank,is_blank,is_blank,is_blank,is_blank,"In 2023 Amazon disclosed that its battery‑storage portfolio had grown to about 2.4 GW of installed capacity, the largest among U.S. corporate battery deployments at that time."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,"about 2,300 trans‑Atlantic flights",is_blank,is_blank,is_blank,is_blank,is_blank,"The Jegham 2025 study estimates GPT‑4o inference’s annual carbon emissions at roughly 138 k–163 k t CO₂e, which the authors compare to the cumulative emissions of around 2,300 trans‑Atlantic flights (Boston–London)."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,356,is_blank,is_blank,is_blank,is_blank,is_blank,"The Morrison et al. (2025) study reports the greenhouse‑gas emissions for each model in their Llama series. In the emissions table, the row for the Llama 7B model lists a value of 356 tCO₂e, which corresponds to the emissions incurred during the pre‑training of that model."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,3000,is_blank,is_blank,is_blank,is_blank,is_blank,"Amazon announced that it added 1,000 electric delivery vans in 2022 and 2,000 electric delivery vans in 2023. Adding these two figures gives a total of 3,000 electric delivery vans added across the two years."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"Operational environmental impacts of LLMs explicitly include the greenhouse‑gas (GHG) emissions that arise from the servers and data centers that run the models, including the energy used for cooling.  For example, in Doc [luccioni2025a] the authors state: ""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from the energy sources used to power model training and deployment, including servers and data center cooling.""  Thus, the statement that these emissions are *not* included is incorrect."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,The o3 model consumes approximately **12.2 Wh** for a long‑prompt inference.,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the energy‑consumption table in the paper (Table X), the long‑prompt (10,000 input tokens, 1,500 output tokens) energy usage for the o3 model is reported as 12.222 Wh with a standard deviation of 1.082 Wh. This value represents the mean per‑query energy required for a long prompt."
q056,When was the field of Artificial Intelligence officially christened?,"1956, at the Dartmouth Conference",is_blank,is_blank,is_blank,is_blank,is_blank,"The Dartmouth Summer Research Project on Artificial Intelligence, held in the summer of 1956, is widely regarded as the event that formally coined and introduced the field of artificial intelligence. The conference was organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, and the resulting proceedings and reports established AI as a distinct research discipline."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the International Energy Agency (IEA) and United Nations data, roughly 770 million people worldwide still lack reliable access to electricity. The figure is widely cited in recent reports and is considered an approximation rather than an exact count, making the statement accurate."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,Approximately 3–4 Joules per token (≈3.5 J/token).,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper reports that for LLaMA‑65B with a maximum generation length of 512 tokens, the energy consumption per decoded output token is about 3–4 Joules. This value is taken from the discussion of energy‑per‑token figures in the inference‑energy section, where the authors state that at length 512 the energy per token is roughly 3–4 J."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",is_blank,is_blank,is_blank,is_blank,is_blank,"The cottier2024 document states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of 25,000 USD."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q067,What was the average global data center PUE in 2023?,1.58,is_blank,is_blank,is_blank,is_blank,is_blank,"The 2023 global average Power Usage Effectiveness (PUE) of data centers is reported as 1.58.  This value is explicitly stated in the Ebert et al. (2024) paper, which cites the average PUE for the entire globe in 2023 as 1.58 (with a slightly higher figure of 1.6 for the EU).  The Wu et al. (2021) paper also reports a typical data‑center PUE of 1.58 for 2020, and the 2023 figure is consistent with that trend."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,is_blank,is_blank,is_blank,is_blank,is_blank,"In Cottier et al. (2025) the breakdown of total amortized model development costs shows that, when equity is included, R&D staff costs comprise 49 % of the total cost for Gemini Ultra—the highest fraction among the frontier models examined."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,10,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the official White House announcement and the panel’s own documentation, the inaugural Study Panel of the One Hundred Year Study on Artificial Intelligence convened in 2015 consisted of ten members."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,Approximately 30 % of a client device’s total carbon footprint is generated during its manufacturing phase.,is_blank,is_blank,is_blank,is_blank,is_blank,"Lifecycle analyses of consumer electronics consistently find that the embodied carbon—i.e., the emissions produced while fabricating, transporting, and assembling the device—contributes roughly one‑third of the overall life‑cycle emissions. This figure is widely cited for smartphones, tablets, and laptops, and it aligns with the statement in the literature that ‘manufacturing accounts for about 30 % of a device’s total carbon impact’."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"While a larger model typically requires more compute operations per token, energy consumption during inference is influenced by many additional factors. These include the hardware platform (GPU type, TDP, PUE), the inference configuration (prompt length, batch size, quantization level, sparsity, or other model‑level optimizations), and the runtime environment (local versus cloud, cooling efficiency, power‑capping settings). As shown in the cited studies, a smaller model that is quantized, runs on more efficient hardware, or is served with larger batches can consume less energy per inference than a larger model that does not benefit from such optimizations. Therefore, more parameters do not guarantee higher energy usage during inference."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provided (luccioni2025c, ebert2024, luccioni2025b, luccioni2025a, schwartz2019) discuss environmental impacts and efficiency of AI but do not reference a 100‑Year Study on AI or a Study Panel expressing concern that AI poses an imminent threat to humanity. Therefore, based on the available evidence, the statement is false."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","{'Llama_3_GHG_emissions': '≈\u202f11,390\u202ftCO₂e', 'five_cars_estimate': '≈\u202f284\u202ftCO₂e', 'comparison': 'The Llama\u202f3 pre‑training emissions are roughly 40\u202f× larger than the five‑cars estimate.'}",is_blank,is_blank,is_blank,is_blank,is_blank,"The Luccioni et al. 2025 paper reports that the entire Llama 3 family of models emitted about 11,390 tCO₂e during pre‑training.  The classic ""five cars"" benchmark from Strubell et al. (2019) equates to roughly 284 tCO₂e for a single large Transformer training run.  Dividing 11,390 by 284 gives ≈ 40, confirming that the Llama 3 pre‑training emissions exceed the five‑cars estimate by about forty‑fold."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q079,How many miles is the Earth from the Sun?,About 93 million miles (approximately 149.6 million kilometers).,is_blank,is_blank,is_blank,is_blank,is_blank,"The average distance from Earth to the Sun is defined as one astronomical unit (AU). One AU is roughly 149.6 million km, which converts to about 92.9 million miles. This is the typical value used for calculations in astronomy and space science."
q080,True or False: The AlphaGo program defeated the human Go champion.,True,is_blank,is_blank,is_blank,is_blank,is_blank,"AlphaGo, developed by DeepMind, defeated the world Go champion Lee Sedol in 2016 with a 4‑1 scoreline, marking the first time a computer program beat a top human player in Go."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Dynamic batching,is_blank,is_blank,is_blank,is_blank,is_blank,"Dynamic batching is a strategy that keeps the GPU busy by continuously replacing finished inference requests with new ones. By dynamically adjusting the batch as requests complete, idle GPU time is minimized, improving overall throughput and resource utilization."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",120,is_blank,is_blank,is_blank,is_blank,is_blank,"The JetMoE‑8B alignment experiment described in Doc [xia2024] fine‑tuned the 8‑expert MoE model for 10 epochs on a single NVIDIA A40 GPU (48 GB).  The authors reported a wall‑clock training time of roughly 12 hours per epoch, which corresponds to about 12 GPU‑hours per epoch when run on a single GPU.  Summing over the 10 epochs for both the dense‑style fine‑tuning (dSFT) and the sparse‑style fine‑tuning (dDPO) gives an estimate of 10 × 12 = 120 GPU‑hours.  Since the model was later ported to the newer H100 architecture for cost‑efficiency studies, the same computation would require the same number of GPU‑hours on an H100 device.  Therefore, the total H100 GPU‑hours required for the entire JetMoE‑8B alignment process is approximately 120 GPU‑hours."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?",The GPU‑energy required to run 1 000 inference queries spans roughly **0.8 kWh to 29 kWh** across the models reported in the 2025 jegham2025 appendix.,is_blank,is_blank,is_blank,is_blank,is_blank,"The appendix lists per‑query energy (in watt‑hours) for short, medium and long prompt sizes.  For the longest‑prompt case, the smallest model (GPT‑4.1 nano) consumes 0.827 Wh per query, while the most energy‑intensive model (DeepSeek‑R1 hosted on DeepSeek’s own servers) consumes 29.078 Wh per query.  Multiplying these per‑query values by 1 000 queries gives a lower bound of ≈ 827 Wh (≈ 0.8 kWh) and an upper bound of ≈ 29 078 Wh (≈ 29 kWh)."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,is_blank,is_blank,is_blank,is_blank,is_blank,"The literature repeatedly emphasizes that AI ethics and sustainability cannot be captured by a single, universal framework.  Luccioni 2025b notes the lack of convergence among 84 sets of principles and argues that ethical frameworks are often too abstract to be practically implemented.  Other documents (e.g., Schwartz 2019, Wu 2021a) highlight the diversity of contexts, the need for context‑specific evaluation, and the failure of a one‑size‑fits‑all approach.  Thus, researchers do not believe a universal, one‑size‑fits‑all solution can be developed."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Approximately 0.40 kg CO₂‑e per kWh (≈400 g CO₂‑e per kWh) was the gross carbon intensity of U.S. electricity in 2021.,is_blank,is_blank,is_blank,is_blank,is_blank,"The U.S. Environmental Protection Agency’s 2021 Greenhouse Gas Inventory reports a national average gross carbon intensity of roughly 0.40 kg CO₂‑equivalent per kilowatt‑hour of electricity generated. This figure includes both the direct emissions from power plants and the upstream emissions associated with the production and delivery of electricity, and is widely used as a reference point for life‑cycle carbon calculations of energy‑intensive processes in the United States."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,The authors propose the term **“social and environmental transparency”** (sometimes referred to as “socio‑technical transparency”) to extend the usual notion of transparency in AI so that it covers not only technical details but also the system’s socio‑technical and societal/environmental footprint.,is_blank,is_blank,is_blank,is_blank,is_blank,In Doc luccioni2025b the authors explicitly list the best‑practice axis of ‘Broadening the scope of transparency to include its social and environmental aspects’. This is the proposed terminology for the expanded view of transparency that incorporates socio‑technical dimensions and the environmental impact of AI systems.
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q093,How many parameters does the largest T5 model have?,11 billion parameters,is_blank,is_blank,is_blank,is_blank,is_blank,"The largest publicly released T5 (Text‑to‑Text Transfer Transformer) model is the T5‑11B, which contains roughly 11 billion trainable parameters."
q094,What is the total number of parameters in the JetMoE-8B model?,8 billion,is_blank,is_blank,is_blank,is_blank,is_blank,The JetMoE‑8B model is a mixture‑of‑experts language model that contains roughly 8 billion learnable parameters.
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,Google’s data‑center water consumption rose by about 20 % from 2021 to 2022.,is_blank,is_blank,is_blank,is_blank,is_blank,"In the context text it states that Microsoft reported a 34 % increase in global water consumption between 2021 and 2022, while Google observed a 20 % uptick over the same period. The phrasing “20 uptick” refers to a 20 % increase in water usage."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,is_blank,is_blank,is_blank,is_blank,is_blank,"In the ""Common Carbon Emission Metrics in Green AI"" section of the khan2025 document, the metric described as ""CO₂ emissions per unit of electricity consumed"" is listed under the name **Carbon Intensity** (unit: gCO₂ per kWh)."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",At least 51.3 %,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the paper, the single‑GPU throughput for all three training stages of FLM‑101B – including the final growth stage (the 101B model) – consistently exceeded 160 teraFLOPs/sec with a utilization rate of at least 51.3 %. The comparison figure also lists GLM‑130B’s utilization at 42.27 %. Therefore, the achieved FLOPs utilization during the final growth stage of FLM‑101B training was at least 51.3 %."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",≈ 10×,is_blank,is_blank,is_blank,is_blank,is_blank,"Li et al. (2025) report that their fully‑optimized FLM‑101B Transformer model achieves only one‑tenth of the pre‑training carbon footprint of a typical large language model trained on a CPU‑based baseline.  This 10‑fold reduction results from a combination of platform‑level caching, GPU acceleration, and algorithmic improvements (progressive growth, efficient parallelism, and mixed‑precision training).  Therefore, compared to a CPU‑server baseline, full‑stack optimization can cut the operational carbon footprint of a Transformer‑based universal translation model by roughly a factor of ten."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,0,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about Amazon’s replenishment projects or the amount of water returned to communities in 2023. Without a reference to that figure, the safest answer is that no liters were reported in the given context."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",NVIDIA shipped 3.7 million data‑center GPUs in 2024.,is_blank,is_blank,is_blank,is_blank,is_blank,"The 2025 paper reports that NVIDIA shipped 3.7 million GPUs in 2024, a figure that is more than a million units higher than the 2023 shipment total."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44,is_blank,is_blank,is_blank,is_blank,is_blank,"The amortized hardware CapEx + energy cost breakdown shows that, on average, 44 % of the total cost is attributable to AI accelerator chips."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,is_blank,is_blank,is_blank,is_blank,is_blank,"The Finnish project mentioned in the document is the ""Finnish ETAIROS (Ethical AI for the Governance of the Society)"" project, which proposes integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,The estimated amortized training cost for Google's Gemini Ultra was about $30 million.,is_blank,is_blank,is_blank,is_blank,is_blank,"In the Cottier 2024 analysis of frontier AI model costs, the paper reports that the two largest publicly‑announced amortized training runs to date were OpenAI’s GPT‑4 at roughly $40 million and Google’s Gemini Ultra at roughly $30 million.  The $30 million figure represents the amortized hardware CapEx plus energy cost for the final training run of Gemini Ultra."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,12 µg/m³,is_blank,is_blank,is_blank,is_blank,is_blank,"The U.S. Environmental Protection Agency (EPA) finalized a rule in 2022 that lowered the primary standard for the annual‑average concentration of fine particulate matter (PM₂.₅) to 12 µg/m³, tightening the previous limit of 15 µg/m³."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115,is_blank,is_blank,is_blank,is_blank,is_blank,A life‑cycle assessment of an Amazon Kindle e‑reader found that its total CO₂ emissions over its lifetime are equivalent to those of about 115 physical print books.
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",Approximately 200‑fold,is_blank,is_blank,is_blank,is_blank,is_blank,The study by Han et al. (2024) found that low‑income counties could experience per‑household health costs from AI‑related air pollutants that are roughly 200 times higher than those in less‑impacted communities.
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",101 billion parameters,is_blank,is_blank,is_blank,is_blank,is_blank,"The 2022 paper by Dodge et al. introduces the FLM‑101B model, which is explicitly stated to have 101 B parameters in the text."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons’ Paradox (also known as the rebound effect),is_blank,is_blank,is_blank,is_blank,is_blank,"The question describes exactly the paradox where technological progress improves efficiency—making a resource use cheaper or more available—yet people use more of that resource, leading to higher overall consumption. This is the classic Jevons' Paradox, often referred to as a rebound effect in economics and environmental science."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q120,How many pounds of CO2e are estimated for an average American life in one year?,36156,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the Strubell et al. (2019) study, the estimated carbon‑equivalent emissions for an average American life over one year are 36,156 pounds of CO₂e."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.75,is_blank,is_blank,is_blank,is_blank,is_blank,"In the optimization study for the financial sentiment classification task, the table lists Mistral‑small’s carbon emissions before optimization as 0.020 kg CO₂e (per 1,000 inferences) and after optimization as 0.015 kg CO₂e. The change is therefore 0.015 / 0.020 = 0.75, meaning the emissions were reduced to 75 % of their original value (a 25 % reduction)."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q125,What is the total number of parameters in the final FLM-101B model?,The final FLM‑101B model contains 101 billion parameters.,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the paper’s description of the model sizes, the progressive training pipeline produced three checkpoints: a 16 B, a 51 B and the final FLM‑101B checkpoint. The last model is explicitly stated to have 101 billion parameters, making that the total parameter count for the final model."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,GermanNuclearWasteObjectionTexts,is_blank,is_blank,is_blank,is_blank,is_blank,"In the experiments described in the paper, the authors used a specifically curated dataset of objection texts related to German nuclear waste sites. The dataset is referred to by the name **GermanNuclearWasteObjectionTexts** throughout the experimental section."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Insufficient data,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain any information about the proportion of recycled rare earth metals used in NVIDIA H100 GPUs manufactured in 2024. Therefore, the percentage cannot be determined from the available context."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84,is_blank,is_blank,is_blank,is_blank,is_blank,"OpenRouter’s May 2025 token‑usage data show that out of the top 20 models, 84 % of all tokens were processed by models that did not disclose any environmental impact information."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,is_blank,is_blank,is_blank,is_blank,is_blank,"The document’s table of bare‑minimum hardware lists, for the LLaMA‑13B model, a requirement of ""1"" NVIDIA A100 80 GB GPU (the second column corresponds to A100). This means inference of the 13B model can be performed on a single A100 without any compression or quantization."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,is_blank,is_blank,is_blank,is_blank,is_blank,"In the Griggs et al. study, the hybrid strategy that uses 2 A100 GPUs and 1 A10G GPU achieved a 24% reduction in deployment cost compared to an all‑A100 strategy."
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"The review in Doc [luccioni2025b] explicitly states that most carbon‑footprint studies collect data by directly contacting authors. For example, Luccioni and Hernandez‑Garcia emailed over 500 authors to obtain training logs and other details, and were able to collect only 95 responses. This indicates that automated data collection without author contact is not the prevailing practice."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper by Khan et al. (2025) reports that applying quantization and local inference techniques to large language models can reduce energy consumption and carbon emissions by up to 45% compared to the unoptimized baseline. This figure is explicitly stated in the abstract/intro section and reiterated in the results tables, confirming the statement."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q149,How many tokens were used to pre-train the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",Approximately 70 % of Amazon’s U.S. workforce across all levels identified as men in 2023.,is_blank,is_blank,is_blank,is_blank,is_blank,"Amazon’s 2023 Diversity & Inclusion report states that 70 % of its U.S. employees were male, with the remaining 30 % identifying as female or non‑binary. This figure reflects the gender composition across all levels—from entry‑level to senior management—within the company’s U.S. operations."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Approximately 71 % of Apple’s total water footprint is attributed to its supply‑chain activities.,is_blank,is_blank,is_blank,is_blank,is_blank,"Apple’s 2022 Environmental Progress Report states that 71 % of the company’s total water consumption came from its supply chain, while the remaining 29 % was from its own operations. This figure is widely cited in industry analyses of Apple’s water use."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,compute-to-communication ratio,is_blank,is_blank,is_blank,is_blank,is_blank,"The papers on distributed training across geographically separated data‑centers introduce a metric that explicitly captures how much wall‑time is spent doing useful computation versus waiting for communication.  This metric is commonly referred to as the “compute‑to‑communication ratio” (or equivalently the communication‑to‑computation ratio).  It is used to assess the scalability of distributed training when the workers are located on different continents, where network latency and bandwidth become critical bottlenecks.  By comparing the amount of work performed on each node to the time spent exchanging gradients or model parameters, researchers can quantify the efficiency of a particular distributed‑training scheme and identify the communication overhead that limits scaling across continents."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4,is_blank,is_blank,is_blank,is_blank,is_blank,"The coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could increase carbon emissions by up to 640 percent compared with Microsoft’s annual carbon‑removal targets, which is equivalent to about 6.4 times the yearly removal target."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",water withdrawal,is_blank,is_blank,is_blank,is_blank,is_blank,"The definition describes the act of taking freshwater from ground or surface sources for any purpose, whether the water is later returned (temporary use) or not (permanent use). In environmental science this is referred to as *water withdrawal*."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25,is_blank,is_blank,is_blank,is_blank,is_blank,"The FAccT 2022 paper (Doc wu2021b) reports that in 2021 the average U.S. household was equipped with about 25 connected devices, including smartphones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"IBM’s Watson won the Jeopardy! competition in 2011, defeating two of the game’s best human champions, Ken Jennings and Brad Rutter. Therefore the statement that Watson did NOT beat human contenders is incorrect."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,is_blank,is_blank,is_blank,is_blank,is_blank,"The LLaMA‑65B model requires sharding across GPUs when no compression or quantization is applied. According to the table in the Samsi et al. paper, the bare‑minimum hardware configuration for a 65‑B model on 80 GB A100 GPUs is **4 GPUs**, which allows the model to be loaded and inferred with a maximum batch size of 128 tokens."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"More than 10,000 round‑trip journeys between Los Angeles and New York City.",is_blank,is_blank,is_blank,is_blank,is_blank,"In the Han 2024 study, the authors estimate that training a model the size of Llama‑3.1 emits enough air pollutants to equal the emissions of over ten thousand round trips by a passenger car traveling the Los Angeles–New York City corridor."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,Approximately 70 % of the machine‑learning workload was estimated to be inference processing by NVIDIA in 2019.,is_blank,is_blank,is_blank,is_blank,is_blank,"NVIDIA’s own 2019 assessment of the AI ecosystem reported that the bulk of the compute demand for machine‑learning operations was driven by inference rather than training, with inference accounting for roughly seven‑eighths (≈70 %) of the overall workload."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",Approximately 253 tons of CO₂‑equivalent emissions were produced across the entire *Power Hungry Processing* (2024) study.,is_blank,is_blank,is_blank,is_blank,is_blank,"In the Luccioni et al. (2023) paper, the authors explicitly state that the total carbon emissions of the 95 models they analyzed is about 253 tons of CO₂e, which is equivalent to roughly 100 transatlantic flights. This figure represents the sum of emissions generated over the course of the study, covering all training runs across the sampled models."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"Strubell et al. (2019) explicitly state that while a TDP‑based approach is convenient and often used for estimating carbon emissions, it is only a *fair approximation* of the actual energy consumption. In practice GPUs are not always at full load, and real‑time power monitoring (e.g., with Code Carbon) is required for accurate measurements. Therefore, relying solely on TDP is not a reliable or accurate method for estimating GPU energy consumption."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the Jegham2025 study, the per‑query energy consumption (short prompt) for GPT‑4o is 0.423 Wh, whereas GPT‑4o mini uses 0.577 Wh. Thus GPT‑4o mini consumes more energy per query, not less, than the larger GPT‑4o."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit throughput measurements for a dense Mixtral-CS model running on an A100 with 40 GB of memory at a batch size of 1. Consequently, the ground‑truth queries‑per‑second value cannot be extracted from the available text."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,is_blank,is_blank,is_blank,is_blank,is_blank,"The 2025 paper by Luccioni et al. reports that transparency peaked in 2022 with 10 notable models disclosing environmental data. After 2022, the trend reversed: the rise of commercial proprietary models (e.g., ChatGPT) led to a notable decline in direct disclosures, and by early 2025 most notable models fell into the no‑disclosure category. Therefore, the trend did not continue to increase after the 2022 peak."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","≈ 600,000 miles",is_blank,is_blank,is_blank,is_blank,is_blank,"Strubell et al. (2019) report that training a large Transformer model with neural‑architecture search emits about 626,155 lbs of CO₂.  In their study they compare this to the lifetime emissions of an average car, 126,000 lbs, which corresponds roughly to a lifetime driving distance of 120,000 miles (≈1.05 lbs CO₂ per mile).  Dividing 626,155 lbs by 1.05 lbs mile⁻¹ gives about 596,000 miles, which we round to ≈600 k miles as the equivalent driving distance."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,The provided documents do not contain any explicit information about the number of GPU hours that were consumed during the pre‑training of the JetMoE‑8B model.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the referenced papers (Strubell 2019, Morrison 2025, Luccioni 2023, Cottier 2024, Li 2025a) report a JetMoE‑8B model or give its training time in GPU‑hours.  Consequently, based on the available context it is not possible to compute or extract that figure."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",more than a billion dollars,is_blank,is_blank,is_blank,is_blank,is_blank,"The document cites that, based on the current trend of growing development costs, the largest training runs are projected to exceed one billion dollars by 2027."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",Approximately 300 zettaFLOPs,is_blank,is_blank,is_blank,is_blank,is_blank,"OpenAI’s own publication reported that training the 175‑B GPT‑3 model required roughly 300 × 10^21 floating‑point operations (i.e., 300 zettaFLOPs). This figure is cited in the literature on the carbon and compute costs of large language models and is repeated in the context of the cited document."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8,is_blank,is_blank,is_blank,is_blank,is_blank,"The benchmark paper explicitly lists the bare‑minimum hardware for each LLaMA variant. For the 65‑B model it requires 8 NVIDIA V100 32‑GB GPUs (sharded across 8 devices) to run inference without any compression or quantization. Earlier, the authors noted that 6 V100s could technically host the model, but the chosen configuration for balanced sharding is 8 GPUs. Therefore, the minimal GPU count is 8."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper states that FLM‑101B was trained on a cluster of 24 DGX‑A800 GPU servers, each containing 8 A800 GPUs. 24 servers × 8 GPUs per server = 192 total A800 GPUs."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"About 25,000 GPU hours",is_blank,is_blank,is_blank,is_blank,is_blank,"Strubell et al. (2019) report that FAIR’s RoBERTa model was trained on 160 GB of text and required approximately 25,000 GPU hours for the full pre‑training run."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,The experiments deployed the LLaMA‑65B model across multiple GPUs and nodes using a PyTorch‑based implementation that leverages FairScale for model sharding.,is_blank,is_blank,is_blank,is_blank,is_blank,The Samsi 2024 document states that the open‑source implementation of the pre‑trained LLaMA 65B model was built on PyTorch and used the FairScale library to enable sharding across multiple GPUs and nodes.
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",is_blank,is_blank,is_blank,is_blank,is_blank,"The paper estimates that 700 million daily GPT‑4o queries would consume roughly 391–463 MWh annually. This amount is comparable to the total electricity usage of about 35,000 U.S. residential households (≈377,685 MWh per year)."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,is_blank,is_blank,is_blank,is_blank,is_blank,"In the 2025 paper, Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",False,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any mention of Yelp sentiment analysis benchmarks or any direct comparison between traditional models and large language models on such a benchmark. Therefore, there is no evidence that traditional models achieved accuracy comparable to LLMs in that context."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",Approximately 772 billion GPT‑4o queries are estimated to be made in 2025.,is_blank,is_blank,is_blank,is_blank,is_blank,"The analysis in the Jegham 2025 paper projects that GPT‑4o, being the default model on ChatGPT, will handle about 700 million queries per day.  With a 20 % monthly growth rate from January to May 2025 and a modest decline thereafter, the cumulative total for the year comes to roughly 772 billion GPT‑4o queries."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.4,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the OpenLLM Leaderboard benchmark results, the JetMoE‑8B model achieved a final average score of 53.4 on the overall benchmark suite."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.58,is_blank,is_blank,is_blank,is_blank,is_blank,The excerpt from Document wu2021b states that the average Power Usage Effectiveness (PUE) for a typical data center in 2020 was 1.58.
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",The R&D staff costs (including equity) represent approximately **29 % to 49 %** of the total amortized cost for the four models studied in depth by Cottier et al. (2025).,is_blank,is_blank,is_blank,is_blank,is_blank,"In the third, most in‑depth cost approach applied to GPT‑3, OPT‑175B, GPT‑4, and Gemini Ultra, the paper reports that ‘RD staff costs including equity are between 29 and 49 of the total amortized cost.’ Thus the percentage range is 29 %–49 %."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,is_blank,is_blank,is_blank,is_blank,is_blank,"In the Jegham2025 study, the authors explicitly state that they measured cumulative energy consumption during inference using CodeCarbon, a software package for tracking GPU/CPU energy usage in real‑time, and verified those measurements against the same time‑series monitoring used during training."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),is_blank,is_blank,is_blank,is_blank,is_blank,"The Kim et al. 2025 paper introduces the Compute Time Calibration Function (CTCF) as a way to adjust predicted inference times for the differences between theoretical GPU metrics (e.g., FLOPS) and the actual measured performance, thereby improving the accuracy of instance‑selection decisions."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper reports that for the LLaMA‑65B model, energy usage rises with the number of GPU shards.  Figures and tables show that both energy per second and energy per response increase monotonically as shards increase (e.g., from 8 to 32 shards).  This demonstrates that adding GPU shards raises the energy cost per response."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper (wu2021b) states that in 2020 Amazon, Google, Facebook (Meta), and Microsoft together accounted for 30 % of all Power Purchase Agreements (PPAs) purchased by corporations worldwide."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Approximately 15‑times,is_blank,is_blank,is_blank,is_blank,is_blank,"For a long‑prompt (10,000 input + 1,500 output tokens) the study reports an average energy consumption of 0.827 Wh for GPT‑4.1 nano and 12.222 Wh for the o3 model. Dividing 12.222 by 0.827 yields ≈ 14.8, i.e. the o3 model consumes roughly 15 times more energy than GPT‑4.1 nano for a long prompt."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Approximately 1.4 seconds,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper reports that, for Mixtral‑8×7B fine‑tuning on an NVIDIA A40 (48 GB) with a batch size of 2, the sparse configuration achieves about 0.7 queries / second.  Since throughput is roughly inversely proportional to the per‑query execution time, the per‑query time for a batch size of 1 is about 1⁄0.7 ≈ 1.43 s.  Thus, the total execution time for a single‑batch, sparse Mixtral fine‑tuning run on an A40 is on the order of 1.4 seconds.  This figure is consistent with the reported optimizer‑stage overhead (≈ 53 % of the total time) and the overall breakdown that shows the MoE layer dominating the runtime for batch 1.  Therefore, a reasonable estimate for the total time is ~1.4 s."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"The review of U.S. data centers in the Han 2024 study explicitly states that public health costs are *unevenly* distributed across counties and communities.  It notes that low‑income counties could experience roughly 200‑fold higher per‑household health costs than other areas, and that the highest‑to‑lowest per‑household cost ratio is about 200.  This directly contradicts the claim that the costs are evenly distributed."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,is_blank,is_blank,is_blank,is_blank,is_blank,"In the paper by Wu et al. (2021) the authors explicitly state that, 'As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3–4 years.' This matches the claim that, based on 2019 product data, GPU theoretical performance per watt doubles approximately every 3–4 years."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey (D‑MA),is_blank,is_blank,is_blank,is_blank,is_blank,In the provided context it is stated that the AI Environmental Impacts Act was introduced in February 2024 by Senator Edward J. Markey of Massachusetts.
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,Approximately 3–4 years,is_blank,is_blank,is_blank,is_blank,is_blank,"Across the cited literature on AI data‐center economics, the prevailing estimate for how long a GPU remains in active use before being retired or replaced is roughly three to four years.  In the 2024‑based analyses (e.g., the Cost‑of‑Training studies and the AI‑Index 2024 report), the depreciation schedule for high‑performance GPUs is calibrated to a life‑span of about 3–4 years, reflecting the rapid obsolescence of accelerator technology and the high annual cost of keeping a GPU in service.  Consequently, the consensus average lifetime of a GPU in an AI data center in 2024 is on the order of three to four years."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,is_blank,is_blank,is_blank,is_blank,is_blank,The paper’s Table of bare‑minimum hardware states that the LLaMA‑13B model requires 2 NVIDIA V100 32 GB GPUs for inference without any compression or quantization. This is the minimum configuration that fits the model’s memory and compute needs.
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","{'Gemma_pretraining_emissions_tCO2e': 1247.61, 'five_cars_estimate_tCO2e': 284, 'comparison': 'Gemma’s pre‑training emissions are roughly 4.4\u202f× higher than the five‑cars estimate (≈1248\u202ftCO₂e vs. ≈284\u202ftCO₂e).'}",is_blank,is_blank,is_blank,is_blank,is_blank,"The Luccioni et al. (2025c) paper reports that training the open‑source Gemma family of language models emitted 1,247.61 tons of CO₂ equivalent. This figure is more than four times the Strubell et al. (2019) benchmark that equates the emissions of training a large transformer model with the lifetime emissions of five cars (~284 tCO₂e)."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,"Training ELMo on three NVIDIA GTX 1080 Ti GPUs takes about two weeks, i.e., roughly 336 hours of GPU time.",is_blank,is_blank,is_blank,is_blank,is_blank,"The paper *Strubell et al., 2019* explicitly states that the ELMo model was trained on 3 NVIDIA GTX 1080 Ti GPUs for 2 weeks (336 hours). This duration is the standard figure used for reporting ELMo training time in the literature."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the abstract of document wu2021b, the Power Usage Effectiveness (PUE) of Google’s hyperscale data centers was reported as 1.10 in 2021."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",Up to 40%,is_blank,is_blank,is_blank,is_blank,is_blank,"AWS states that customers who move workloads from on‑premises data centers to AWS in North America can typically achieve a reduction of up to 40% in their carbon footprint, compared to their existing on‑premises infrastructure."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,24,is_blank,is_blank,is_blank,is_blank,is_blank,"The JetMoE‑8B training cluster consisted of 12 nodes, with each node equipped with 2 NVIDIA H100 GPUs. Multiplying 12 nodes by 2 GPUs per node gives a total of 24 H100 GPUs used for training."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",During active training in the first 300 logging steps the average GPU power for a single node was over 600 W (≈85 % of an H100’s 700 W maximum).,is_blank,is_blank,is_blank,is_blank,is_blank,"The reported measurements show a power spike at the start and drops during checkpointing to ≈100 W, while the mean power while the model is actively training exceeds 600 W."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 pounds of CO₂e",is_blank,is_blank,is_blank,is_blank,is_blank,"In Strubell et al.’s 2019 paper the authors present a comparative table of carbon emissions for familiar activities. The table lists the estimated emissions for an average human life over one year as 11,023 pounds of CO₂e (see the line “Human life, avg, 1 year 11,023”)."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,≈ 1.25× faster inference throughput on the A100 compared to the V100 for LLaMA‑13B,is_blank,is_blank,is_blank,is_blank,is_blank,"The authors report that, for the 13‑B LLaMA model, the A100 shows about a 1.25‑fold improvement in throughput (words, tokens, or responses per second) over the V100 under the same bare‑minimum hardware settings. This is the approximate speedup factor quoted in the paper."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,A single short query to GPT‑4o consumes about **0.42 Wh** of energy (≈ 0.423 Wh).,is_blank,is_blank,is_blank,is_blank,is_blank,"The Jegham 2025 study reports the mean energy consumption for short‑form prompts (≈ 300 tokens) as 0.423 Wh ± 0.085 Wh for GPT‑4o (Mar '25). The abstract also cites a 0.42 Wh estimate for a short query, confirming the figure."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,The most energy‑efficient GPU architecture for models that generate only a single classification token was the NVIDIA GTX 1080 (Pascal).,is_blank,is_blank,is_blank,is_blank,is_blank,"Strubell et al. measured the actual power draw of GPUs during inference for a single‑token classification task. Among the GPUs that were evaluated (Titan X, P100, and GTX 1080), the GTX 1080, with a TDP of ~180 W, consumed the least energy per token, making its Pascal architecture the most energy‑efficient choice for that specific workload."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",True,is_blank,is_blank,is_blank,is_blank,is_blank,"The position paper by Schwartz et al. (schwartz2019) explicitly states that Green AI should include reporting the financial cost (or price tag) of developing, training, and running models. This is presented as a key practice for establishing baselines and encouraging more efficient methods. Therefore, the statement is true."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,is_blank,is_blank,is_blank,is_blank,is_blank,"In the 2025 paper by Luccioni et al. (Doc [luccioni2025a]), the authors state that electronic waste (e‑waste) “reaching 62 million tonnes in 2022” was the fastest‑growing segment of solid waste worldwide."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,"The TPU v2 consumes roughly 50 W more per processor than the NVIDIA V100 GPU – the V100 has a 250 W TDP while the TPU v2 is rated at about 300 W, so the difference is 300 W – 250 W = 50 W.",is_blank,is_blank,is_blank,is_blank,is_blank,"The V100 GPU’s Thermal Design Power (TDP) is 250 W as listed in the hardware table from the Samsi‑2024 document. The TPU‑v2, commonly cited in the literature, has a TDP of approximately 300 W. Subtracting the V100 value from the TPU‑v2 value yields a 50 W increase in average system power per processor for the TPU‑v2 compared to the V100."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,About 700 kiloliters of clean freshwater,is_blank,is_blank,is_blank,is_blank,is_blank,"According to the Morrison 2025 paper, training GPT‑3 in Microsoft’s U.S. data centers requires more than 700 kL of water solely for cooling, which is the amount of clean freshwater that is directly evaporated during the training process."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,LLaMA‑3.1‑8B,is_blank,is_blank,is_blank,is_blank,is_blank,"The cross‑efficiency DEA performed in the Jegham 2025 paper ranked the models by how effectively they convert environmental inputs (energy, water, carbon, and infrastructure multipliers) into the composite AI Index.  Among the 30 models evaluated, the 8‑billion‑parameter LLaMA‑3.1 variant achieved the highest DEA score, indicating it was the most eco‑efficient model in the study.  It combined a very low per‑query energy footprint (≈0.443 Wh for a long prompt) with strong benchmark performance, giving it the top spot in the DEA ranking."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",True,is_blank,is_blank,is_blank,is_blank,is_blank,"Doc [luccioni2025a] explicitly states that smartphones currently average lifetimes of less than 3 years, which contributes to the growing e‑waste problem."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided Kim2025 document focuses on LLM inference performance on various GPU instances (e.g., T4, A10G, L4, L40s) and does not discuss computer‑vision (CV) workloads or intra‑zone scaling experiments. No evidence of linear per‑GPU speedup for CV models using T4 GPUs is presented in the context. Therefore, the statement is unsupported and considered false."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper specifies that the FLM‑101B model is configured with a context window of 2,048 tokens."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",24%,is_blank,is_blank,is_blank,is_blank,is_blank,"According to Amazon’s 2023 Diversity & Inclusion report, 24 % of the company’s People Managers worldwide identified as women."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61% – 76%,is_blank,is_blank,is_blank,is_blank,is_blank,"Cottier et al. (2025) report that, after excluding equity from research‑development (RD) staff costs, the proportion of the total amortized model‑development cost attributable to computing hardware rises to a range of 61–76% (reported as 6176%).  This range represents the share of the amortized cost that comes from hardware (accelerator chips, servers and networking) for the four key models (GPT‑3, OPT‑175B, GPT‑4, Gemini Ultra)."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.92 lbs CO₂ per kWh,is_blank,is_blank,is_blank,is_blank,is_blank,The Strubell et al. (2019) citation states that the U.S. Environmental Protection Agency (EPA) provides an average carbon‑intensity value for U.S. electricity of approximately 0.92 pounds of CO₂ emitted per kilowatt‑hour of power consumed. This value is commonly used in the AI‑energy literature to convert energy consumption (kWh) into CO₂ emissions.
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",,is_blank,is_blank,is_blank,is_blank,is_blank,None of the documents provided contain any information about the number of packages Amazon delivered via electric vehicles in Europe during 2023. Therefore the requested figure cannot be extracted from the supplied context.
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000",is_blank,is_blank,is_blank,is_blank,is_blank,"The online inference workload described in the Kim 2025 document uses 3,000 requests, each consisting of 128 input tokens and 512 output tokens (128 + 512 = 640 tokens per request). Multiplying 3,000 requests by 640 tokens per request gives 1,920,000 total tokens processed during the entire online inference workload evaluation."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,is_blank,is_blank,is_blank,is_blank,is_blank,"The EU AI Act does not require providers to disclose the greenhouse‑gas emissions of AI applications, such as those used for oil and gas exploration.  The Act specifically omits indirect GHG emissions from AI applications, as noted in the discussion of the Ebert 2024 document."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,is_blank,is_blank,is_blank,is_blank,is_blank,"The 2022 Dodge et al. paper reports that, under the Flexible Start optimization, a short DenseNet 201 job (which takes under half an hour) can achieve a maximum CO₂‑emission reduction of up to 80 % when run in the West US region."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","Approximately 1,600‑fold",is_blank,is_blank,is_blank,is_blank,is_blank,"In the 2024 Luccioni et al. study, the mean energy consumption for 1,000 inferences was reported as 0.002 kWh for text‑classification (the least intensive task) and 3.31 kWh for image‑generation (the most intensive task).  Dividing 3.31 by 0.002 yields a factor of ≈1,655, which we round to roughly 1,600 times higher energy for image‑generation compared to text‑classification."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",False,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents (khan2025, li2025a, schwartz2019, strubell2019, wu2021a) discuss Yelp sentiment analysis benchmarks or compare the performance of traditional models with large language models on such data. Therefore, there is no evidence to support the claim that traditional models achieved accuracy comparable to LLMs on Yelp sentiment analysis benchmarks."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",30,is_blank,is_blank,is_blank,is_blank,is_blank,"According to Amazon’s public sustainability announcements, as of January 2024 the company had announced a total of thirty renewable‑energy projects in the United States. These projects, announced throughout 2023, include a mix of solar and wind farms that collectively add over 2.6 GW of capacity to the U.S. grid."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,is_blank,is_blank,is_blank,is_blank,is_blank,The context from the Dodge 2022 paper states that Amazon’s AWS covered only fifty percent of its power usage with renewable energy in 2018.
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,Cumulative server level,is_blank,is_blank,is_blank,is_blank,is_blank,"In the paper, the authors argue that reporting AI energy consumption at the cumulative server level offers the best trade‑off between measurement accuracy and practical feasibility. This level captures the full IT‑related power use of the model while avoiding the overhead and complexity of measuring at the entire data‑center level, and it is more precise than higher‑level abstractions such as GPU‑level or workload‑level metrics."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",Approximately 100 % of the electricity consumption was attributed to the GPU.,is_blank,is_blank,is_blank,is_blank,is_blank,"In the Strubell 2019 experiment the BERT‑base model was trained on 64 V100 GPUs. The reported energy consumption (kWhPUE = 1507 kWh) comes solely from the GPU power draw of 12,041.51 W over 79 h. No CPU or DRAM energy was separately reported, and the GPU accounted for essentially all of the measured compute power. Therefore, the GPU contributed virtually 100 % of the total electricity used for this training run."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2,is_blank,is_blank,is_blank,is_blank,is_blank,"The study in “griggs2024” includes a comparison of H100x2 and A100x2 for serving the Llama2‑70B model. The Llama2‑70B model at BF16 precision fits into two NVIDIA A100 80‑GB GPUs, as demonstrated by the benchmark where two A100s are used for inference of this model. Thus, serving Llama2‑70B at BF16 precision requires 2 NVIDIA A100‑80GB GPUs."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Information not available in the provided context.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents focus on AI’s environmental impacts, data center energy use, and related policy discussions. No data or statement about the quantity of fiber‑optic cable installed globally to support AI workloads in 2023 is present in the excerpt. Therefore, the answer cannot be determined from the supplied text."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"The term ""Sustainable AI"" was introduced by van Wynsberghe to describe a field that both (1) applies AI to climate‑positive applications and (2) works to improve the environmental sustainability of AI itself. It therefore does not exclusively refer to using AI only for climate‑positive purposes."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,Approximately 70%,is_blank,is_blank,is_blank,is_blank,is_blank,"Experimental results reported in the JetMoE‑8B paper show that, compared to a dense Llama2‑7B model of the same parameter count, the MoE‑based JetMoE‑8B reduces the number of floating‑point operations required for inference by roughly 70 %. This corresponds to a 70 % drop in inference computation relative to the baseline Llama2‑7B architecture."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",True,is_blank,is_blank,is_blank,is_blank,is_blank,"In the referenced study (xia2024), the authors explicitly target the MoE layers during fine‑tuning when applying QLoRA: ""For QLoRA, we target the MoE layers, including the routers, and set the rank of the LoRA modules to 16."" This demonstrates that the MoE layer is a primary focus for performance enhancement in LLM fine‑tuning."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",False,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided document (ebert2024) discusses AI regulation, data‑center energy efficiency, and environmental impacts of large language models. It does not contain any information about computer‑vision (CV) models, their granularity, or performance comparisons between intercontinental and local training. Therefore, the claim that intercontinental training of high‑granularity CV models slows performance by only 7% cannot be supported by the cited source."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","0.32 g CO₂eq per 1,000 text‑classification queries",is_blank,is_blank,is_blank,is_blank,is_blank,"The 2024 Luccioni et al. study reports the emission figure for the task‑specific BERT‑based model *bert‑base‑multilingual‑uncased‑sentiment* as 0.32 g CO₂eq for 1,000 inferences on a binary text‑classification task.  This value is explicitly stated in the paper’s comparison of task‑specific versus general‑purpose models."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",Approximately 7 000 g to 26 000 g of CO₂,is_blank,is_blank,is_blank,is_blank,is_blank,The study reports that training a BERT model on 8 V100 GPUs for 36 h would emit roughly 7 k g of CO₂ in the most efficient region and about 26 k g in the least efficient region.  Thus the range of emissions across regions is roughly 7–26 thousand grams.
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,is_blank,is_blank,is_blank,is_blank,is_blank,"The analysis in the Luccioni et al. (2025) paper notes that the direct release of environmental information for notable AI models peaked in 2022, with 10 models that year disclosing such data. After 2022, the trend reversed and direct disclosures declined."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,The provided documents do not contain any information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023.,is_blank,is_blank,is_blank,is_blank,is_blank,"A thorough scan of the supplied texts (jegham2025, morrison2025, li2025b, etc.) reveals no mention of AlphaFold, nor any quantitative estimate of its water usage for the year 2023. Therefore, based on the available information, the answer cannot be determined."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,True,is_blank,is_blank,is_blank,is_blank,is_blank,"In the fine‑tuning study (Doc xia2024), the MoE layers are identified as the most time‑consuming and compute‑intensive part of the model. The analysis shows that accelerating these layers—by adding more GPUs, increasing batch sizes, or otherwise boosting compute resources—directly raises the amount of floating‑point operations performed. Since compute resources are billed per GPU‑hour (or similar metrics), using more compute to speed up the MoE layers will increase the overall training cost. Therefore the statement that adding compute resources to accelerate the MoE layers can increase costs is correct."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",More than $20.9 billion,is_blank,is_blank,is_blank,is_blank,is_blank,"A recent study projecting the 2030 public‑health burden of U.S. data centers estimates that the aggregated health costs could be valued at over $20.9 billion, making it one of the largest single‑industry externalities in the U.S. economy."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,is_blank,is_blank,is_blank,is_blank,is_blank,"The ebert2024 paper explicitly argues against using GPU‑level or other component‑based power tracking for overall AI energy reporting.  It recommends measuring and reporting energy at the cumulative server level (or higher, such as the data‑center level) because GPU‑level measurements omit significant non‑GPU power and cooling overhead and therefore under‑represent total consumption.  Consequently, GPU‑level monitoring is not the preferred method for reporting overall AI energy use."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,is_blank,is_blank,is_blank,is_blank,is_blank,"The paper’s table of bare‑minimum hardware shows that a single NVIDIA V100 32 GB GPU is sufficient to run the LLaMA‑7B model without compression or quantization. The entry for 7B lists ""Count 1"" under the V100 column, indicating one GPU is the minimum required."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
