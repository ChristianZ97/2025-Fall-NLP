id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied passages mention AWS using recycled water for cooling or the number of data centers involved, so the answer cannot be determined from the provided documents."
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any electricity consumption figures for U.S. data centers for the years 2010–2014, so the requested average increase cannot be determined with confidence."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""first commercially‑produced microprocessor Intel 4004 was manufactured in 10,000 nm process technology in 1971, and ran at 740 kHz ... Fifty years later, the typical microprocessor is manufactured in a 5+ nm process technology and is capable of running at 5,000,000 kHz ... This is a more than 6,750 fold improvement in processor clock speed""","The passage explicitly states that the clock speed increased from 740 kHz to 5,000,000 kHz, which is a 6,750‑fold increase, providing the required answer."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents contain information about permitted annual emission limits for nitrogen oxides from data center backup generators in northern Virginia for the specified period.
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of the final average performance score of FLM-101B on the Open LLM Leaderboard, so the answer cannot be determined from the given documents."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific numeric value for the estimated GPU power usage in kWh for the SGLang benchmarking scenario described in the question.
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,≈4000 lbs,4000,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"""Air travel, 1 passenger, NYSF 1984""",The Strubell 2019 table lists 1984 lbs of CO₂‑equivalent for a one‑way flight from NY to SF (or SF to NY). Multiplying by two for a round trip gives roughly 4000 lbs of CO₂e.
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about the Amazon Solar Farm Maryland-CPV Backbone or its CO2e avoidance equivalent to cars taken off the road.
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""","The growth strategy reduced training time from 76.74 days to 21.54 days, a difference of 55.20 days. 55.20 days / 76.74 days ≈ 0.72, i.e., 72% of the original training time was saved."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the launch year of the One Hundred Year Study on Artificial Intelligence.
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a clear numeric value for the amortized training cost of GPT‑4, nor a precise numeric total training budget for FLM‑101B that can be used to compute a factor. Therefore, the answer cannot be determined with confidence from the provided context."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a numeric estimate of embodied carbon emissions per GPU. No specific value in kg per GPU is found in the excerpts, so a confident answer cannot be provided."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,Both models incorporate eight experts in their MoE layers.,"The context from doc xia2024 explicitly states that each MoE layer contains eight experts, providing the required number."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the total execution time of a dense BlackMamba model with a batch size of 30 on an NVIDIA A40-48 GB GPU. Hence, I cannot answer the question with confidence."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific figure for the energy consumption (in MWh) of pre-training the BLOOM model, so a confident answer cannot be given."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"The UN’s Global E-Waste Monitor 2024 showed that about 22 of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling.","The quoted sentence from the document states that roughly 22% of e‑waste is formally collected and recycled, directly answering the question."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement about the number of days it took to train GPT‑3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec, so the answer cannot be determined from the given context."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The context documents provide estimates for premature deaths in 2028 but do not include explicit figures for 2030 or a breakdown of scope‑2 pollutants alone. Therefore there is insufficient information to answer the question with confidence.
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided document does not contain a clear statement of the duration of the full training run for the 6.1 billion‑parameter model, so the answer cannot be derived with confidence."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",GPT-4.1,GPT-4.1,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Table of energy per query: 10.20, 1.12, 0.30, 4.35, 0.35 (preceding the line ""GPT-4.1 Apr, 2025"").","The appendix lists per‑query GPU energy consumptions for several models. The largest value shown is 10.20 Wh for GPT‑4.1, which is higher than all other listed models. For 1,000 queries, this yields the highest overall GPU energy consumption among the models in the appendix."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain information about the Switch Transformer, its 1500 billion‑parameter size, or the percentage of parameters activated per token."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents mention a hardware processor used for an experimental setup of energy‑efficient local inference in financial sentiment classification, so the answer cannot be determined from the given information."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents provide an estimate of the total energy consumption for a 6.1 billion‑parameter transformer model, so the answer cannot be determined from the available information."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The supplied documents do not contain any reference to a study titled ""Power Hungry Processing"" (2024) or information about the number of models sampled and analyzed for such a study."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72 time‑saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).""",The provided passage explicitly states that the growth strategy led to a total wall‑clock training time of 21.54 days for FLM‑101B.
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate"" and ""Economists refer to such transformations as Jevons' Paradox, which was proposed in the 19th century by economist William Stanley Jevons…""","The paper explicitly names Jevons' Paradox as the economic principle explaining why efficiency gains can lead to rebound effects, undermining net environmental benefits."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific figure for the number of zettaFLOPs required to train the English portion of the FLM-101B model.
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about GPU utilization percentages in Facebook model experimentation workflows, so the statement cannot be evaluated with confidence."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,1.2x to 4x,"[1.2, 4]",is_blank,cottier2024,https://arxiv.org/pdf/2405.21015,"""Based on this, we sampled the factor from a log-normal distribution with a 90 CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.""","The cited passage from Cottier et al. (2025) states that the total compute expended during model development exceeds that of the final training run by a factor ranging from 1.2 to 4 times, providing the answer to the question."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity","The Jegham et al. 2025 document explicitly states that GPT‑3’s training consumed 1,287 MWh of electricity, providing the requested value."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""These methodologies were then adapted into the AI Energy Score , a project aiming to establish a unified approach for comparing the inference efficiency of AI models."", cited in Doc [luccioni2025c]","The excerpt from Luccioni et al. explicitly names the collaborative project as ""AI Energy Score,"" which is described as aiming to create a standardized method for comparing inference efficiency across AI models."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a numeric value for the execution time of the longest kernel of the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48 GB GPU. Therefore the answer cannot be determined with confidence.
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.  The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The provided context states a 300,000× increase in compute from 2012 to 2018, not a 200,000× increase, so the statement is false."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a statement about the drop in global carbon emissions in 2020 during the COVID-19 pandemic.
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a statement or numeric estimate of the age of the artificial intelligence field as of 2025, so the answer cannot be determined with confidence from the given context."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contains any information about the energy use decrease percentage for the Llama 3.1 8B model when targeting a TPOT of 100 ms versus minimizing latency.
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about Amazon’s energy storage capacity as of 2023.
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""For dense fine-tuning, all experts are activated, whereas for sparse fine-tuning, only the top two experts are selected for each token.""","The context states that during sparse fine‑tuning only the top two experts are activated for each token, which applies to the MoE layers of the JetMoE‑8B model, indicating a top‑k of 2."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement or numeric value describing how the overall carbon footprint decreases when GPU utilization is increased to 80% for Language Model training. No relevant factor is specified, so an accurate answer cannot be derived from the given context."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific figure for global AI demand water withdrawal by 2027, so a confident answer cannot be derived."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain explicit information about the number of AWS data center regions that had 100% renewable electricity consumption in 2023, so a confident answer cannot be derived from the given context."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the percentage of AI inference workloads in Asia that were powered by coal in 2023, so a confident answer cannot be derived."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about Amazon electric delivery vans added in 2022 or 2023, so the answer cannot be determined from the given context."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",neural architecture search (NAS),neural architecture search (NAS),is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""the five cars estimate was based on a neural architecture search (NAS) approach.""  and  ""the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average‑efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO-equivalent GHG emissions.""","The 2019 study by Strubell et al. calculated the five‑cars estimate using a neural architecture search (NAS) methodology, which is a specialized, rarely performed AI process. The quoted text from the document confirms that NAS is the basis for the estimate."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents contain a numerical estimate of the GHG emissions (in tCO2e) for pre‑training the Llama 7B model, so the answer cannot be determined from the available information."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific value for the average WUE of Google’s AI‑dedicated data centers in 2024. No statement or table identifies Google’s data center WUE, so a confident answer cannot be derived from the supplied context."
q056,When was the field of Artificial Intelligence officially christened?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the year the field of Artificial Intelligence was officially christened.
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the number of people lacking stable electricity supply.
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,The average data center PUE in 2023 was 1.58 globally,"The provided context states that the global average PUE for data centers in 2023 was 1.58, which directly answers the question about AI‑dedicated data centers’ PUE."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3–4 Joules,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"""with length 512, we see that it takes about 3-4 Joules for a output token""","The Samsi et al. study reports that for LLaMA‑65B at a maximum generation length of 512 tokens, the energy consumed per decoded output token is approximately 3–4 Joule. This value comes directly from the quoted sentence in the paper."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any information about JetMoE-8B or the number of its parameters activated during inference.
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit mention of the number of transatlantic flights that would match the projected annual carbon emissions from GPT‑4o inference. Without such a figure in the documents, a confident answer cannot be given."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the energy consumption of the o3 model for a long prompt. The only energy figures given are a general range for the most energy-intensive and most efficient models, but no model-specific number is provided."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Our empirical analysis in  suggests that the AI research community has paid relatively little attention to computational efficiency. In fact, as Figure illustrates, the computational cost of research is increasing exponentially, at a pace that far exceeds Moore's Law . is on the rise despite the well-known diminishing returns of increased cost (, Figure ).""","The provided document discusses the rising cost of AI research and notes that it is increasing despite diminishing returns, but it does not mention a concept called ""Red AI"" nor state that it is declining. Therefore the claim is not supported by the documents and is false."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit information about Facebook's RM2 model size reduction from 32-bit to 16-bit quantization, so a confident answer cannot be derived."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.""","The quoted passage from the document explicitly states that operational environmental impacts of LLMs include the greenhouse‑gas emissions that come from servers and data‑center cooling, contradicting the claim that they do not include these emissions. Hence the statement is false."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,8,8,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"-D  -S  -D  -S  
     2  8  6  20 
          1  3  2  8","The table in the paper lists the maximum batch sizes for dense (D) and sparse (S) fine‑tuning for Mixtral and BlackMamba. The second row of the table (1 2 3 8) corresponds to the GSM8K dataset, showing that the sparse BlackMamba model can use a batch size of 8 samples on an NVIDIA A40 GPU with 48 GB memory."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention sparsely activated DNNs or provide evidence that they consume less than 1/10th the energy of large dense DNNs while maintaining accuracy.
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,"""The optimizer stage in fine‑tuning takes a considerable portion of the running time (up to 53 when conducting sparse fine‑tuning with batch size = 1)""","The context specifies that during BlackMamba sparse fine‑tuning on an NVIDIA A40‑48GB GPU with batch size = 1, the optimizer stage accounts for up to 53 % of the total running time."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context documents do not contain any information about Microsoft contracting wind turbines to power Azure AI clusters in 2023, so a confident answer cannot be derived from the given sources."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally and 1.6 in the EU.""","The quoted sentence from the ebert2024 document states the global average PUE for 2023 as 1.58, which directly answers the question."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""misinformation evolving from inaccurate or de-contextualized best-effort estimates of greenhouse gas emissions.""","The document discusses that claims about AI’s impact on global GHG emissions are often based on inaccurate or de-contextualized estimates, indicating that the widely cited 5‑10% reduction claim lacks clear, publicly available calculations and solid scientific grounding."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the number of members of the inaugural 2015 Study Panel of the One Hundred Year Study on AI.
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain any information about the cost of training AI2's Grover on 256 TPU chips for two weeks, so a confident answer cannot be derived."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the 100 Year Study on AI or its Study Panel, so the claim cannot be verified or refuted with the given information."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",29–49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""For these models, we find that RD staff costs including equity are between 29 and 49 of the total amortized cost.""",The cited passage from Cottier et al. (2025) explicitly states that for the four models examined—including Gemini Ultra—the proportion of total amortized development cost attributable to R&D staff (including equity compensation) ranges from 29% to 49%.
q080,True or False: The AlphaGo program defeated the human Go champion.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents contain no information about AlphaGo or its match against a human Go champion. Therefore the answer cannot be determined from the given context.
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the Earth‑Sun distance, so the answer cannot be derived from the given context."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain the energy consumption figure (0.083 kWh per 1,000 queries) for Flan‑T5‑xxl or any related data needed to compute a daily consumption estimate for 1 billion queries. Without this key information, a reliable answer cannot be derived from the supplied context."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the JetMoE-8B alignment process or the number of H100 GPU hours used.
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric percentage describing how much of a client device’s total carbon footprint is due to manufacturing. Therefore, confidence in an answer cannot be established from the given context."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents contain a numeric factor describing Facebook’s AI training infrastructure capacity growth between Yr1‑Q1 and Yr2‑Q2 (2019‑2021).
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific figure for the amount of CO2 emitted by OpenAI’s API requests in January 2024, so the answer cannot be determined from the documents given."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any mention of a 2025 Chen et al. paper or the name of an LLM inference system that uses model‑attention disaggregation. Therefore, a confident answer cannot be derived from the given context."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention classification experiments on German public administration texts or report accuracies for models using sentence embeddings on that domain, so the answer cannot be determined from the supplied context."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit mention of the stable-diffusion-xl-base-1.0 model or its CO2eq emissions per 1,000 inferences, so the answer cannot be determined from the available information."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Results show the most energy‑intensive models exceed 29 Wh per long prompt, over 65 the most efficient systems.","The provided documents indicate that energy use during inference varies by model and deployment setup rather than being strictly proportional to parameter count. The Jegham2025 study shows a range of energy consumption across models of different sizes, implying that a larger number of parameters does not guarantee higher energy usage in all cases."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the gross carbon intensity of energy according to the U.S. average mix in 2021, so the answer cannot be determined from the supplied context."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"""an increase of about 26.7.""","The Max‑Performance policy chose g6e.xlarge costing 2.699, while InferSave chose g4dn.xlarge costing 2.13. The relative cost increase is (2.699–2.13)/2.13 ≈ 0.267, or 26.7 %."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit mention of a batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, so the answer cannot be determined with confidence from the given context."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","420 Wh to 29,000 Wh","[420, 29000]",Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""Results show the most energy‑intensive models exceed 29Wh per long prompt, over 65 the most efficient systems. Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes…""","The 2025 study reports per‑prompt GPU energy consumption ranging from 0.42 Wh for a short query to 29 Wh for a long prompt. Scaling these values to 1,000 queries gives a total energy range of 0.42 × 1000 = 420 Wh to 29 × 1000 = 29,000 Wh. This range is directly supported by the abstract of the study."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the efforts aiming to study AI's ethical ramifications should be made in tandem with those evaluating its impacts on the environment""","The passage indicates that researchers advocate for integrated but not a universal, one-size-fits-all approach to AI ethics and sustainability, implying they do not believe such a universal approach can be developed."
q094,What is the total number of parameters in the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about the JetMoE-8B model's parameter count.
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided contexts do not contain information about a decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents.
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,morrison2025,https://arxiv.org/pdf/2503.05804,"Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34 increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20 uptick in the same period.","The context notes that Google experienced a 20% increase in data center water consumption from 2021 to 2022, so the percentage increase is 20%."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40 million,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""most expensive publicly‑announced training runs to date are OpenAI's GPT‑4 at 40M and Google's Gemini Ultra at 30M.""","The context states that OpenAI’s GPT‑4 had an estimated amortized training cost of 40 million dollars, which is the value requested."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,is_blank,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Carbon Intensity  gCO/ kWh  CO emissions per unit of electricity consumed,"The framework lists a metric called ""Carbon Intensity"" and defines it as ""CO emissions per unit of electricity consumed"" (gCO/kWh)."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents mention Amazon's replenishment projects or provide data on liters of water returned to communities in 2023, so the answer cannot be determined from the given context."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a statement about the number of data center GPUs NVIDIA shipped in 2024. Therefore, the answer cannot be determined from the supplied context."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",51.3%,51.3,%,li2025a,https://arxiv.org/pdf/2309.03852,"""The single‑GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3.""","The document reports that during every stage of FLM‑101B training, including the final growth stage, the measured FLOPs utilization was at least 51.3 %. This directly answers the question for the final stage."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit statement or numeric value indicating the fraction of local throughput achieved for NLP when training was distributed across four continents versus remaining local. Therefore, I cannot answer the question with confidence."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",TRUE,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,"""Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts.""","The rubei2025 abstract states that using specific tags (i.e., custom tags in prompt engineering) reduces energy consumption for code generation tasks, which aligns with the claim about one‑shot, zero‑shot, and few‑shot techniques in source code completion. Thus the statement is supported and considered true."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any mention of a Finnish project or its acronym that integrates ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided contexts do not contain any explicit statement about the factor by which full‑stack optimization reduces the operational carbon footprint of a Transformer‑based universal translation model compared to a CPU server baseline.
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30 million,30000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""We find that the most expensive publicly‑announced training runs to date are OpenAI’s GPT‑4 at 40M and Google’s Gemini Ultra at 30M.""",The cited sentence from the Cottier 2024 paper explicitly states that the estimated amortized training cost for Google’s Gemini Ultra is 30 million dollars.
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the EPA’s primary standard for PM2.5, so the answer cannot be determined from them."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any statement linking the CO2 emissions of an Amazon Kindle e-reader to a specific number of physical print books, so the question cannot be answered with confidence."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",approximately 200×,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""low‑income counties that could experience approximately 200x per‑household health costs than others.""","The study reports that the per‑household health burden in the most affected, economically‑disadvantaged communities can be about 200 times higher than in less impacted communities."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information on the energy consumption of the DS Llama 70B model for inference on the FKTG dataset.
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the energy consumption of a Meena training run or a comparison between Meena and GPT-3 energy usage, so the question cannot be answered confidently."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We also argue for an interpretation of the AI Act that includes environmental concerns in the mandatory risk assessments (sustainability risk assessment, SIA)""","The AI Act requires risk assessments for high‑risk AI systems, but it does not explicitly mandate that these assessments include environmental risks. The quoted passage indicates that the authors are proposing an interpretation to add environmental concerns, not that the Act itself requires it."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",47-64%,"[47, 64]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""Computing hardware makes up 4764, while energy comprises only 26.""","The passage states that computing hardware—primarily AI accelerator chips—constitutes 47–64 % of the total amortized hardware and energy cost, indicating that on average roughly half of the cost is due to accelerator chips."
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a Table 2 or any mention of average energy consumption for 1,000 image‑generation inferences, so the answer cannot be determined from the given context."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 lbs",36156,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Human life, avg, 1 year  11,023
American life, avg, 1 year  36,156","The Strubell et al. (2019) table lists the estimated CO₂e emissions for an average American life over one year as 36,156 pounds. This directly answers the question."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""economists refer to such transformations as , which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries.""","The passage identifies Jevons’ Paradox as the phenomenon where efficiency gains lead to increased overall resource consumption, also referred to as rebound effects. The answer directly matches this description."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the supplied documents provide a numeric Power Usage Effectiveness (PUE) value specifically for Facebook’s data centers, so a confident answer cannot be derived from the context."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about Mistral-small’s emissions before or after optimization for the financial sentiment classification task, so the multiplier cannot be determined from the available context."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,"""Benefiting from our , the we produce three models with 16B, 51B, and 101B (FLM-101B) parameters in a single training.""","The context explicitly states that the FLM-101B model has 101 billion parameters, which is reflected in the answer and the supporting quote."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",6.1 billion parameters,6100000000,parameters,dodge2022,https://arxiv.org/pdf/2206.05229,pretraining of a 6.1 billion parameter language model,The Dodge et al. 2022 paper explicitly states that the large language model they analyzed had 6.1 billion parameters.
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about a dataset name for German nuclear waste site objection texts classified in experiments.
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Marion County,Marion County,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""WV Marion (978.0, 1458.5) 0.80""… WV Mason (897.6, 1380.4) 0.71… WV Marshall (831.2, 1336.5) 0.77… (table showing Marion has the highest per‑household cost range).",The table in the paper lists West Virginia counties with their per‑household health cost ranges; Marion County has the highest upper bound (1458.5) and therefore is projected to have the highest per‑household cost in 2030.
q093,How many parameters does the largest T5 model have?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents contain a statement or data about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit numeric value for the combined training and fine-tuning energy cost (in kWh) of the BLOOMz-7B model from the 'Power Hungry Processing' study, so the answer cannot be determined with confidence."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""84 of LLM usage is through models with no disclosure, 14 for indirectly disclosed models, and only 2 for models with direct disclosure.""","The OpenRouter May 2025 data shows that 84 % of token usage came from models that did not disclose any environmental impact information, which directly answers the question."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give the energy consumption for training (51,686 kWh) and fine‑tuning (7,571 kWh) of BLOOMz‑7B, totaling 59,257 kWh, but they do not provide an inference energy cost per inference for BLOOMz‑7B. Without that key figure, we cannot calculate how many inferences would equal the initial training and fine‑tuning energy cost."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a concrete figure for the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. The only mention is a percentage reduction up to 45% in one study, but no absolute tCO2e value is supplied."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Using 2 A100s and 1 A10G results in a 24 cost saving over A100-only and 31 over A10G-only.""","The passage from griggs2024 reports that a mixed strategy of 2 A100 GPUs and 1 A10G GPU yields a 24% cost saving compared to an A100‑only deployment, directly answering the question."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a stated energy consumption (in kWh) for a full training run of a 6.1B parameter model, so the question cannot be answered with confidence based on the available information."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provided do not contain a clear numeric value for the freshwater consumption (in liters) of Meta’s Llama 3 inference serving clusters in 2024. No excerpt or table in the context specifies this figure, so it cannot be determined with confidence from the given information."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,"""This is equivalent to approximately 44 of the data centers' total electricity cost.""","The Han 2024 document states that in 2023 the public health cost of U.S. data centers was about 6.7 billion dollars, which is equivalent to approximately 44% of the data centers' total electricity cost, using the average attribution method."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit estimate or range of CO2 emissions for a complete training run of a 6.1 billion‑parameter transformer model, nor do they provide sufficient data to derive such a range with confidence."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,TRUE,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45 post quantization""",The khan2025 document explicitly states that quantization and related deployment techniques can reduce carbon emissions by up to 45%.
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a reference to Chen et al. (2025) nor any price information for an NVIDIA H20 GPU. Therefore, the answer cannot be derived from the given documents."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95,95,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"""We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4.""","The passage states that after contacting over 500 authors, the researchers collected data for 95 models (from 77 papers). This provides the answer to the question."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 NVIDIA A100 80GB GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,"""13B  2  64  1  64"" – the second count in the table corresponds to the bare‑minimum number of A100 GPUs required for LLaMA‑13B.","The table in the Samsi 2024 paper lists bare‑minimum GPU counts for each model on V100 and A100. For LLaMA‑13B the A100 column shows a count of 1, indicating that a single NVIDIA A100‑80GB GPU suffices for inference without compression or quantization."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents provided do not contain any information about Amazon Renewable Energy Projects announced in the United Kingdom.
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about the percentage of Amazon's workforce in the United States that identified as men in 2023.
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Model Size 7B   1  64  1  64"" from the bare‑minimum hardware table indicates that a single NVIDIA A100 80 GB GPU is sufficient for LLaMA‑7B inference without compression or quantization.","The table lists the required GPU count for each model. For the 7B model the count under the A100 column is 1, meaning one A100 80 GB GPU is the bare minimum needed. This is consistent with the text noting that the 7B model was run on a single GPU. Thus the minimum number of A100 GPUs required is one."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about Apple’s total water footprint or the portion attributable to its supply chain, so the question cannot be answered with confidence from the given context."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the total execution time (in seconds) of a sparse BlackMamba model fine‑tuned on an NVIDIA A40‑48GB GPU with a batch size of 84, so an answer cannot be given with confidence."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","78,468 kWh",78468,kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"w/ tuning experimentation 78,468","The study’s energy‑usage table lists the total energy consumed for all model experimentation and evaluation as 78,468 kWh."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""In our email to authors, we asked them to provide the details we needed to carry out this calculation, i.e the location of the computer or server where their model was trained (either cloud or local), the hardware used, and the total model training time.""","The quoted sentence shows that this study explicitly contacted authors to obtain training details, indicating that not all carbon footprint analyses gather information automatically. Therefore, the claim that most analyses do so is not supported by the provided documents."
q149,How many tokens were used to pre-train the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any mention of the number of tokens used to pre-train the JetMoE-8B model, so the answer cannot be determined with confidence."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 connected devices,25,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.""","The cited passage from Doc wu2021b explicitly states that the average U.S. household has 25 connected devices, which directly answers the question."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about how often the Standing Committee of the One Hundred Year Study forms a Study Panel.
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",640%,640,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year.""","The quoted passage from the document states the deal could add up to 640 percent more carbon emissions than Microsoft’s yearly carbon removal targets, providing the exact figure requested."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about IBM Watson’s performance in the Jeopardy challenge, so the answer cannot be determined from the supplied context."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a direct statement linking the 3.2 tCO₂e figure for the Evolved Transformer NAS to an equivalent number of passengers on a round‑trip flight between San Francisco and New York, so the answer cannot be derived with confidence from the supplied material."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention JetMoE-8B-Chat or its MT-Bench score, so the answer cannot be determined from the supplied context."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Approximately $1.00 per H100 GPU‑hour,1,USD per GPU-hour,li2025a,https://arxiv.org/pdf/2309.03852,"""We train three models with 16B, 51B, 101B, the total time cost for training FLM‑101B is 21.54 days"" and ""FLM‑101B is trained on a cluster of 24 DGX‑A800 GPU (8×80G) servers"" and ""budget of 100K""","The training used 24 servers × 8 GPUs = 192 GPUs for 21.54 days. GPU‑hours = 21.54 days × 24 h/day × 192 GPUs ≈ 99,264 GPU‑hours. With a reported budget of $100,000, the cost per GPU‑hour is $100,000 ÷ 99,264 ≈ $1.00, giving an approximate cost of $1.00 per H100 GPU‑hour."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water.""","The quoted sentence from Doc luccioni2025a states that between 10 and 50 GPT‑3 queries use roughly half a liter of water, so the study estimates that 10–50 queries consume about 0.5 L."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"WUE can be computed based on either water withdrawal (the total volume drawn from natural or municipal sources) or water consumption (the portion of withdrawn water permanently lost, primarily through evaporation).","The quoted sentence defines ‘water withdrawal’ as the total volume of freshwater taken from ground or surface sources, whether temporarily or permanently, for various uses, matching the question’s description."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",14.76%,14.76,%,han2024,https://arxiv.org/pdf/2412.06288,"Altoona, IA 6.912.1  (1.84, 3.17)1221.52 (34000)11.78 (10600)14.76","The table for training Llama‑3.1 scale models lists, for Altoona, IA, the health cost as a percentage of the electricity cost, shown as 14.76%. This directly answers the question."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents contain a statement about the percentage of ML workload that is inference processing by NVIDIA in 2019.
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear numeric range of energy consumption in MWh for pre-training large language models, so a confident answer cannot be derived from them."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Table in samsi2024 shows for LLaMA‑65B the bare‑minimum hardware: a count of 4 A100 80 GB GPUs (the 65B row lists ""Count 4"" for A100).","The table explicitly lists the minimum number of A100 GPUs needed for LLaMA‑65B inference as 4, with no compression or quantization applied."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000",10000,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips""","The Han et al. (2024) document explicitly states that training a Llama‑3.1 scale model generates air pollutants equivalent to over 10,000 round‑trip car journeys between Los Angeles and New York City."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided Griggs et al. (2024) context does not contain any explicit numeric value for the normalized on‑demand hourly price of an H100 GPU. Without this information, a confident answer cannot be given."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain explicit information comparing the energy per query of GPT-4o mini to that of GPT-4o, so the statement cannot be verified."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit numeric throughput measurement for a dense Mixtral-CS model running on an A100-40GB GPU at batch size 1, so the answer cannot be determined with confidence."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the relationship between GPT‑3 model size and BLEU score improvements from 5 to 40, so the answer cannot be determined from the given context."
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any mention of a metric for assessing the ratio of computation to communication time when scaling distributed training across continents, so the answer cannot be determined with confidence from the supplied information."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The direct release of environmental information peaked in 2022, with 10 of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.""","The cited passage from the 2025 paper indicates that after the 2022 peak, direct disclosure actually decreased rather than continued to increase, so the statement is false."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain no explicit numeric value for the amount of water used for cooling during OpenAI's GPT-4 training run. Therefore, a confident answer cannot be derived from the supplied context."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear statement of the total CO2 equivalent emissions for the entire 'Power Hungry Processing' (2024) study, so the answer cannot be determined with confidence."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10–50 medium‑length GPT‑3 completions,"[10, 50]",is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""10–50 queries on GPT‑3 consumes around half a liter of water.""","The context states that 10–50 GPT‑3 queries use about 0.5 L of water. A 500 mL bottle contains 0.5 L, so dividing this volume by the per‑query water use (≈0.01–0.05 L) yields a range of roughly 10 to 50 medium‑length completions that could be produced with that amount of water."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the inference energy consumption of the BLOOMz-7B model, so it is impossible to compute the total energy required for the specified number of inferences."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",More than a billion dollars,1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"""If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well‑funded organizations will be able to finance frontier AI models.""","The cited passage directly states that, assuming the current cost growth trend persists, the cost of the largest training runs will exceed one billion dollars by 2027."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE-8B model or specify the number of H100 GPU hours used during its pre-training, so the answer cannot be determined from the given context."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific monthly on-demand rental cost for serving Llama-2-70B using 2 NVIDIA A100 GPUs, so the hourly cost cannot be derived from the given information."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit mention of the top‑1 ImageNet accuracy achieved by AlexNet in 2012, so the answer cannot be determined from the supplied material."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 A800 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX‑A800 GPU (8×80G) servers.""","The passage states that FLM‑101B training used 24 servers, each equipped with 8 A800 GPUs, totaling 24 × 8 = 192 GPUs."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 NVIDIA V100 32GB GPUs,8,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs."" and the table lists 65B 8 64 for V100.","The document states that although 6 V100 GPUs could technically shard the model, the bare‑minimum hardware configuration used for inference of LLaMA‑65B is 8 V100 GPUs, as shown in the table of bare‑minimum requirements."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,luccioni2023,https://arxiv.org/pdf/2302.08476,"""In this study, we use a TDP-based approach is often used in practice when estimating the carbon emissions of AI model training and it remains a fair approximation of the actual energy consumption of many hardware models.""","The cited passage from Luccioni 2023 states that a TDP‑based approach is a ""fair approximation"" of actual GPU energy use, implying it is not fully reliable or accurate. Therefore the claim that it is a reliable and accurate method is false."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emission amount for a Transformer model with neural architecture search (626,155 lbs) but do not contain an emissions‑to‑driving‑distance ratio needed to compute the equivalent driving distance. Therefore, the answer cannot be derived from the given information."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Experiments were performed using open‑source implementation of the pre‑trained LLaMA 65B model available via request from Meta and evaluation scripts available via GitHub. This implementation of the model uses Pytorch and the FairScale library to enable model sharding across multiple GPUs and nodes.""",The passage explicitly states that the LLaMA 65B deployment across multiple GPUs and nodes was enabled by the FairScale library.
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the supplied documents contain information about Amazon's on-site solar energy systems or the amount of CO2e they avoid compared to nonrenewable electricity sources.
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a specific figure for the total floating-point operations used to train GPT‑3 as published by OpenAI.
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain any information about the Llama 3.1 70B model or its energy consumption when deployed on one versus two nodes, so the question cannot be answered with confidence."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention Yelp sentiment analysis benchmarks or provide any accuracy comparison between traditional models and large language models. Therefore the answer cannot be determined from the given context.
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.""","The 2025 paper notes a 34% rise in Microsoft’s global water consumption from 2021 to 2022, which directly answers the question."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain a clear statement of the GPU hours required to train FAIR's RoBERTa on 160 GB of text, so I cannot confidently answer the question."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes","The Jegham2025 paper explicitly states that 700 million daily GPT‑4o queries, each consuming 0.42 Wh, would use as much electricity annually as 35,000 U.S. homes, providing the required comparison."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided contexts do not contain a specific numeric estimate of water consumption per ChatGPT user session in 2023, so a confident answer cannot be given."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","626,155 lbs CO₂e, about 5 average American lifetimes",626155,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"""NAS  P100x8  1515.43  274,120  656,347  626,155"" (Strubell 2019) and ""Car, avg incl. fuel, 1 lifetime 126,000"" (Strubell 2019), showing the NAS CO₂e of 626,155 lbs and a car lifetime emission of 126,000 lbs, yielding roughly five car lifetimes.  Additionally, the abstract of Luccioni 2025c states: ""NAS approach ... could yield 626,155 pounds (284 metric tons) CO-equivalent GHG emissions (COe), or about five times the emissions of a car during its lifetime.""","The NAS CO₂e estimate from Strubell 2019 is 626,155 lbs. Dividing this by the car lifetime CO₂e of 126,000 lbs gives ≈4.97, i.e., about five average American lifetimes. The supporting quotes directly provide the values used for the calculation."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the JetMoE-8B model or its performance on the OpenLLM Leaderboard, so the answer cannot be derived from the given context."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a statement about the number of AI training runs globally that used renewable-only power in 2022. Therefore, the answer cannot be determined from the supplied context."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""In our inference experiments, we measure cumulative energy consumption using CodeCarbon tracking, which was verified against the same time series monitoring used throughout training.""","The document states that the inference runs’ energy consumption was measured with CodeCarbon tracking, indicating that CodeCarbon is the software package used for measurement."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about an analysis of 100 news articles on ChatGPT's energy use or any percentage citing the 10‑times‑Google‑search/3 Wh estimate.
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.58,1.58,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""average PUE for a typical data center in 2020 is 1.58.""","The Wu2021b document states that the average Power Usage Effectiveness for a typical data center in 2020 was 1.58, which is interpreted as the US national average PUE for that year."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,3.104,3.104,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,Iowa 1.160 0.140 3.104 0.180 4.634 0.560 14.403,"The table in li2025b lists PUE values for each U.S. data‑center location; the third number in each row is the PUE. For Iowa the third value is 3.104, which is the PUE when the Evolved Transformer was run."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",Approximately 255.5 billion GPT‑4o queries,255500000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"""Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes""",The paper estimates 700 million GPT‑4o queries per day. Multiplying by 365 days gives about 255.5 billion queries in 2025.
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Overall, we see an average increase in energy per second with the number of shards. ... we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure ) to 1024 (Figure ) does not induce a clear or significant effect in inference energy costs.","The context from Samsi 2024 explicitly states that as the number of GPU shards increases, the energy cost per response for the LLaMA‑65B model also increases. Therefore, the statement is true."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"For these models, we find that RD staff costs including equity are between 29 and 49 of the total amortized cost.",The cited sentence from Cottier et al. (2025) explicitly states that the R&D staff costs (including equity) account for a range of 29% to 49% of the total amortized cost for the four models studied.
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),CTCF,is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"""proposes the Compute Time Calibration Function (CTCF) to accurately determine the optimal instance based on user requirements.""","The document specifies that the Compute Time Calibration Function (CTCF) adjusts predicted GPU times to match actual performance, thereby improving the accuracy of instance selection. The quoted sentence directly names the function and its purpose."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a specific estimate of water consumption for mining rare earth materials required to manufacture a single H100 GPU, nor does any document give a value for 0.1% rare earth metal by mass. Therefore the answer cannot be determined from the given information."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the KV Cache size of the OPT-2.7B model at a batch size of 32, so a confident answer cannot be given."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any statement confirming or denying that open‑source general‑purpose AI models are fully exempt from reporting energy consumption under the AI Act unless they pose systemic risk. Therefore, a confident answer cannot be given."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the Arena dataset, short-context workloads, a 120 ms SLO, or the percentage range of cost reductions achieved by Mélange compared to single‑GPU baselines."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a statement about the percentage of PPAs held by Amazon, Microsoft, Meta, and Google in 2020."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context provides a general throughput figure of 160 teraFLOPs/sec for all training stages but does not give the specific duration or a per-stage throughput table for the final 101B stage, so the total computational work cannot be calculated with the available information."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Importantly, the health costs are unevenly distributed across counties and communities, particularly affecting low-income counties that could experience approximately 200x per-household health costs than others.","The Han 2024 study explicitly states that public health costs of AI are unevenly distributed across U.S. counties, contradicting the claim that they are evenly distributed."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear numeric value for the total execution time in seconds for a sparse Mixtral model with batch size 1 fine‑tuned on an NVIDIA A40-48GB GPU. The available information references percentages of execution time and general breakdowns, but no absolute time measurement is given."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The AI Act applies further down in the value chain, targeting entities that develop and deploy AI systems.""","The provided context states that the AI Act imposes reporting obligations on entities that develop and deploy AI systems, but it does not specify that open‑source general‑purpose AI models are required to report their energy consumption to authorities. Therefore, the claim is not supported by the documents and is considered false."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""",The excerpt from Doc ebert2024 explicitly states that Senator Edward J. Markey introduced the AI Environmental Impacts Act bill on 1 February 2024.
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years.""","The quoted sentence from wu2021b explicitly states that GPU theoretical performance per watt doubles every 3–4 years, which directly supports the claim in the question."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the average GPU lifetime before retirement in AI data centers in 2024.
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not include any explicit statement or data indicating that the relationship between runtime and energy consumption in inference experiments was found to be nearly linear, so a confident answer cannot be derived."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about a storage service used to shard and stream datasets for spot VMs that could terminate at any time.
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,2,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Table showing ""13B 2 64"" under the V100 32GB column, and text stating ""The 7B model was run on a single GPU and 13B on two GPUs""",The table in the document lists the bare minimum GPU count for each LLaMA variant; for LLaMA‑13B it specifies 2 V100 32GB GPUs. The accompanying text confirms that the 13B model was run on two GPUs. Thus the bare minimum number required is two.
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain explicit numeric values for the energy consumption of the o3 model and GPT‑4.1 nano per long prompt, so a factor cannot be determined with certainty."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific price per hour for an NVIDIA H100, so the answer cannot be confidently determined from the available information."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021)""",The quoted statement in the document directly states the PUE for Google's hyperscale data centers in 2021 as 1.10.
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",6.7 billion USD,6.7,USD,han2024,https://arxiv.org/pdf/2412.06288,"""Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about 6.7 billion, or 47.5 per household, in 2023.""","The paragraph in the Han 2024 document explicitly states that the total public health cost in 2023 is approximately 6.7 billion USD, which answers the question based on the average attribution method."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the percentage reduction in carbon footprint that customers can expect when moving workloads from on-premises data centers to AWS in North America. Without such data, a confident answer cannot be given."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the net cost of fine‑tuning a sparse Mixtral model with 2 million queries on an NVIDIA H100 GPU, so a confident answer cannot be derived."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a numeric value for the total estimated net carbon emissions of FLM‑101B pre‑training.
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,336 hours,336,hours,strubell2019,https://arxiv.org/pdf/1906.02243,"""ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).""","The Strubell 2019 document explicitly reports that ELMo training on three GTX 1080 Ti GPUs took 2 weeks, which equals 336 hours."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any information about JetMoE-8B or the number of H100 GPUs used in its training infrastructure.
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 lbs",11023,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Human life, avg, 1 year  11,023","The Strubell 2019 paper lists a table of COe (pounds) for various consumption categories, where the entry for an average human life over one year is 11,023 lbs. This directly answers the question for a global average."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"Results show the most energy-intensive models exceed 29Wh per long prompt, over 65 the most efficient systems. Even a 0.42Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35000 U.S. homes...","The Jegham2025 document directly reports that a single short query to GPT‑4o consumes 0.42 Wh, which is the value requested."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the average GPU power during the first 300 logging steps of OLMo 2 7B training, so the answer cannot be determined with confidence."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,Approximately 1.25× faster on A100 compared to V100,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The document states that for LLaMA‑13B the throughput on A100 GPUs is about 1.25 times higher than on V100 GPUs, indicating a 1.25× speedup."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain power and TDP information only for NVIDIA V100 GPUs. No information about TPU v2 or its average system power per processor is present, so the difference cannot be determined from the given sources."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""AI's expanding operational footprint also contributes to electronic waste (e‑waste), which is now the fastest‑growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.""","The 2025 paper explicitly states that electronic waste worldwide reached 62 million tonnes in 2022, providing the numeric figure and year requested."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"""but at a cost of 2.699, which is about 280 more expensive than 's top choice.""","The paragraph states that the Max‑Performance g6e.xlarge costs 2.699 while InferSave’s top choice g4dn.xlarge costs 0.71. The difference is 1.989, which is about 280% of 0.71, matching the stated “about 280 more expensive.”"
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Reporting the computational price tag of finding, training, and running models is a key practice (see ).","The passage from Schwartz et al. explicitly states that Green AI involves reporting the financial cost (price tag) of finding, training, and running models, directly supporting the claim that the statement is true."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear, explicit mention of the open-source tool used for 4‑bit quantization and local deployment in the financial sentiment case study. The relevant sentence contains a placeholder where the tool name should appear, so the information is insufficient to answer confidently."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.14 L/kWh,3.14,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"""the EWIF for the U.S. (3.14 L/kWh)"" in the text and the U.S. Average row of the table shows 3.142 as the off‑site water consumption value.","The document states that the U.S. average water intensity factor (EWIF) for electricity generation is 3.14 L/kWh, which directly answers the question about national average water consumption for electricity generation."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,NVIDIA A10G,A10G,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"""For smaller request sizes, A10G exhibits up to greater than A100."" and ""smaller sizes are best served on A10G.""","The excerpts indicate that for very small request sizes—such as generating a single classification token—A10G delivers higher tokens‑per‑dollar (i.e., greater energy efficiency) than higher‑end GPUs like the A100."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear statement indicating which specific model achieved the highest rank in the eco-efficiency analysis using DEA. No explicit ranking or model name is mentioned in the excerpts. Therefore, the answer cannot be determined with confidence from the given documents."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about intra‑zone scaling with T4 GPUs or the performance of CV models, so the statement cannot be verified."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the average lifespan of smartphones, so the statement cannot be verified or refuted from the given context."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"""The FLM-101B model is structured with a hidden state dimension of , a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of .""","The provided passage from doc li2025a explicitly states that the FLM‑101B model has a context window of 2,048 tokens, which directly answers the question."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided contexts contain information about Amazon's People Managers or their gender distribution, so the question cannot be answered confidently from the given documents."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain any comparison of power draw between LLM inference and diffusion model inference, so the truth of the statement cannot be determined from the given sources."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about a financial sentiment case study or its accuracy/F1 outcomes, so the question cannot be answered with confidence."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain specific numeric information about Facebook's recommendation and ranking model sizes in 2019 and 2021, so we cannot determine the increase."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about Amazon’s EV package deliveries in Europe for 2023, so the answer cannot be determined from the given context."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents mention that the EPA supplies an average CO2 emission factor for U.S. power, but the actual numeric value (in lbs/kWh) is not included in the excerpts. Therefore the answer cannot be determined from the given information."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear numeric value for the amount of clean freshwater evaporated during GPT-3 training in Microsoft’s U.S. data centers.
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a specific numeric value for the GHG emissions (tCO2e) of Google's Gemma pre‑training process, nor do they provide a direct comparison to the five‑cars estimate. Therefore, a confident answer cannot be derived from the available information."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,kim2025,https://arxiv.org/pdf/2504.11816,"""To model a real‑time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens … The workload evaluates a total of 3000 requests.""","Each online inference request processes 128 input + 512 output = 640 tokens. Over 3000 requests the total tokens processed is 640 × 3000 = 1,920,000 tokens."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information regarding the Flexible Start optimization analysis, DenseNet 201, or the maximum potential percentage reduction in CO2 emissions for a short job in the West US region."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention Yelp sentiment analysis benchmarks, traditional models, or large language models. Therefore, there is insufficient information to determine the truth value of the statement."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the number of Amazon Renewable Energy Projects announced in the United States as of January 2024, so the answer cannot be determined from the given context."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit numerical information comparing the energy required for image generation versus text classification from the 2024 study, so I cannot provide a confident answer."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61-76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,the fractions of computing hardware costs and energy rise to 6176 and 27 respectively.,"The cited passage states that when equity is excluded, the share of total amortized cost attributed to computing hardware increases to a range of 61–76 % for the four key models examined."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level.""",The passage from ebert2024 explicitly states that the authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility.
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We also find that the AI Act fails to address indirect greenhouse gas emissions from AI applications.""","The ebert2024 passage explicitly states that the AI Act does not address indirect greenhouse gas emissions from AI applications, indicating it does not mandate providers to disclose such emissions, including for domains like oil and gas exploration."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,17%,17,%,strubell2019,https://arxiv.org/pdf/1906.02243,"""Consumer Renew. Gas Coal Nuc. China 22 3 65 4 Germany 40 7 38 13 United States 17 35 27 19 Amazon‑AWS 17 24 30 26 Google 56 14 15 10 Microsoft 32 23 31 10""","The table in the Strubell 2019 paper lists the percentage of energy sourced from renewables for Amazon‑AWS as 17 %, indicating that 17 % of its power usage in 2018 was covered by renewable energy."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain any information about the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023, so the answer cannot be determined with confidence."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,US$40 million,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""OpenAI's GPT‑4 at 40M and Google’s Gemini Ultra at 30M.""","The cottier2024 document reports that the most expensive publicly‑announced training run is OpenAI’s GPT‑4 at 40 million dollars, which is taken as the estimated upfront hardware acquisition cost for GPT‑4."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain any mention of a projected range of electricity consumption by global AI in 2027.
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear numeric statement of the total operational energy footprint reduction achieved at Facebook from 2019 to 2021 due to iterative hardware-software optimization.
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about preemption mechanisms such as recomputation or swapping, nor do they discuss their relative energy consumption. Therefore, a confident answer cannot be derived from the given context."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"The umbrella term `Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The quoted passage indicates that ‘Sustainable AI’ was intended to cover both the use of AI for climate-positive work and the improvement of AI’s own environmental sustainability, so the statement that it was proposed to only encompass climate-positive applications is incorrect."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context documents do not contain information about Google’s 2024 environmental report or the percentage increase in GHG emissions since 2019 reported therein.
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain information about the JetMoE-8B architecture or its inference computation relative to Llama2-7B.
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any mention of McKinsey projections regarding U.S. data center electricity consumption for 2030, so the answer cannot be determined from the available information."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the maximum batch size for fine‑tuning a Mixtral model on an NVIDIA A100‑40GB GPU, so the answer cannot be determined with confidence."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any explicit mention of the 'Pause and Resume' optimization or the maximum potential emissions saving for a 6B parameter transformer. Therefore, I cannot confidently provide an answer."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear statement or calculation indicating the percentage of total electricity consumption attributed to the GPU in a BERT-base training experiment.
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2,2,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"""comparison between H100x2 and A100x2 serving Llama2-70b.""","The document lists a subsection comparing H100x2 and A100x2 for serving the Llama2-70b model, indicating that two A100‑80GB GPUs are used for that configuration."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contains information about CV models with high granularity, intercontinental training, or a 7% performance slowdown compared to local training."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so a confident answer cannot be derived from the supplied context."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a clear numeric estimate of the training energy consumption for the full GPT‑3 model in MWh, so the answer cannot be determined with confidence."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"For QLoRA, we target the MoE layers, including the routers, and set the rank of the LoRA modules to 16.","The context states that during fine‑tuning, the MoE layers (including routers) are explicitly targeted for parameter‑efficient fine‑tuning, indicating that the MoE layer is often a focus when improving LLM fine‑tuning performance."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit mention of the land area, in hectares, occupied by new AI data centers globally in 2022. Therefore, a confident answer cannot be derived from the given context."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the CO₂ emissions of the bert-base-multilingual-uncased-sentiment model per 1,000 queries, so the answer cannot be determined from the documents."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""The direct release of environmental information peaked in 2022, with 10 of notable models that year releasing some degree of information.""","The passage in luccioni2025c states that the peak of direct environmental disclosures occurred in 2022, after which the trend declined. Hence the year is 2022."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?",1438 lbs CO2e,1438,lbs CO2e,is_blank,is_blank,"""BERT V100x64 12,041.51 79 1507 1438 375112,571""","The Strubell et al. (2019) table reports the carbon emissions for training BERT on 64 V100 GPUs as 1438 lbs CO2e, which is the value requested."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided excerpts from the documents do not contain a specific numeric value for the execution time of a sparse Mixtral model fine-tuned on an NVIDIA A40-48GB with a batch size of 10. Therefore, the answer cannot be determined with confidence from the given information."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""7B   1  64"" – indicates that the bare minimum hardware for LLaMA‑7B requires one GPU.",The table in the Samsi et al. 2024 context shows that LLaMA‑7B needs a count of 1 NVIDIA V100 32 GB GPU for inference when no compression or quantization is applied.
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","Approximately 2,738 days",2738,days,morrison2025,https://arxiv.org/pdf/2503.05804,"""the amount of water consumed by one average person in the United States in about 7.5 years.""","The paper states that the water consumption of training even the smallest OLMo models (up to 13B parameters) equals about 7.5 years of water use for an average US person. Converting 7.5 years to days (7.5 × 365 ≈ 2,738 days) gives the equivalent days of water usage for the 60 M‑parameter model trained on 1.7–5.6 trillion tokens."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit estimate of the total monetary or GPU-hour cost for fine‑tuning a Mixtral model on GSM8K with sparse MoE on an NVIDIA A40-48GB GPU, so the answer cannot be derived with confidence."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We also compare levels of measurement within data centers and recommend measurement at the cumulative server level.""  ""The energy consumption must be measured at the uninterruptible power system (UPS) or, if not existent, at the power distribution unit (PDU) or at another point specified by the data center.""","The ebert2024 document states that, for reporting overall AI energy use, measurement should be performed at the cumulative server level (or UPS/PDU), not at the GPU level. Therefore, GPU‑level power consumption monitoring is not the recommended preferred method."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",20.9 billion,20.9,billion dollars,han2024,https://arxiv.org/pdf/2412.06288,"""the total public health impact of U.S. data centers is estimated to reach 11.7 billion and 20.9 billion in 2028""","The document states that, under high‑growth scenarios, the public health burden of U.S. data centers could reach 20.9 billion dollars. This figure is the highest projected value provided and is listed as the upper bound in the study’s 2028 projection, which is the only available numeric estimate of the burden in the supplied documents."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",20 samples,20,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse.

-D  -S  -D  -S  
     2  8  6  20 
          1  3  2  8","The table in the document lists the maximum batch sizes for dense (D) and sparse (S) fine‑tuning on the Mixtral model. For the sparse configuration, the largest batch size listed is 20 samples, which corresponds to the longest‑running MoE layer under the NVIDIA A40‑48 GB GPU setup."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2 samples,2,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Maximum batch size table shows Mixtral dense = 2: ""-D  -S  -D  -S  2  8  6  20""","The table of maximum batch sizes for different model‑dataset combinations lists Mixtral‑8x7B dense batch size as 2 samples for the Hellaswag dataset on an NVIDIA A40 GPU with 48 GB memory. The first number in the table row corresponds to this configuration, yielding a maximum batch size of 2 samples."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided document excerpts do not contain a specific numeric value for the total energy consumption of training the FLM‑101B model, so the answer cannot be determined with confidence from the given information."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric percentage indicating what portion of BLOOM’s overall emissions was due to training, so the answer cannot be determined with confidence from the available information."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any mention of the JetMoE-8B model or its GSM8k benchmark score, so the question cannot be answered with confidence."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain explicit numeric values for the CO2 emission range across geographic regions for the BERT training experiment, so the answer cannot be determined with confidence."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""The execution time of LLM fine‑tuning is dominated by the MoE layer."" 
""Matrix multiplication operations in the MoE layer contribute significantly to the end‑to‑end execution time, making the MoE layer the costliest component in LLM fine‑tuning.""","The MoE layer is identified as the most time‑consuming and costliest part of fine‑tuning. Adding compute resources to accelerate this layer therefore increases the amount of compute (and thus monetary) cost, making the statement true."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",Approximately 25 user requests would consume a 500 ml bottle of water.,25,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""We assume a per-request server energy consumption of 0.004 kWh for our conversation task."" – li2025b

Arizona WUE (total water usage effectiveness) = 4.959 L per kWh – li2025b table.",The per-request energy use is 0.004 kWh. Multiplying by Arizona’s total WUE of 4.959 L/kWh gives ~0.0198 L (≈19.8 ml) of water per request. Dividing 0.5 L (500 ml) by 0.0198 L yields about 25 requests.
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents contain a numeric estimate of CO2 emissions in metric tons for one year of average U.S. home energy use. The only metric‑ton figures refer to AI model training, not household energy consumption."
