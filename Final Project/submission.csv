id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2 tCO2e,1.2,tCO2e,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: ... a single passenger round trip SF-NY is ~1.2t CO 2 e (Table 2).,"The question asks for the CO2e of a single passenger round trip from Sanâ€¯Francisco to Newâ€¯York. Doc [patterson2021] explicitly states that such a trip emits approximately 1.2â€¯t CO2e, which is the value used in the answer."
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,Doc [wu2021b]: 'The total energy consumption of the US data centers increased by about 4% from 2010-2014',"The question asks for the average increase in U.S. data center electricity consumption between 2010 and 2014. Doc [wu2021b] explicitly states that the total energy consumption of U.S. data centers increased by about 4% over that period, which is the answer."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72% time-saving,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: are listed in Table 1. Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The question asks for the percentage of time saved by using the growth strategy for the 101B model. Doc [li2025a] explicitly states a 72% timeâ€‘saving, so the answer is 72%."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg,463,kg,is_blank,is_blank,"Doc [morrison2025#0049]: 'or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.'","The question asks for estimated embodied carbon per GPU. Doc [morrison2025#0049] explicitly states that 3700â€¯kg CO2eq per eightâ€‘GPU server node equals 463â€¯kg per GPU, providing the required figure."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.'","The passage explicitly states that the processor clock speed improved by more than 6,750 times from 1971 to 2021, which is the fold increase requested."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021;luccioni2023,https://arxiv.org/pdf/2104.10350;https://arxiv.org/pdf/2302.08476,"Doc [patterson2021]: 'It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.'  Doc [luccioni2023]: '... training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs) [38].'","The question asks for the training duration for GPTâ€‘3 on 10,000 V100 GPUs operating at 24.6 TeraFLOPS/sec. Both Doc [patterson2021] and Doc [luccioni2023] explicitly state the training time as approximately 14.8 days, so this value is reported as the answer in days."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [cottier2024#0043]: 'GPT-4 has the largest amortized hardware and energy cost, at $40M.'
Doc [li2025a]: 'trained with a budget of $100K.'","The amortized training cost of GPT-4 is $40M while the total training budget for FLM-101B is $100K. Dividing 40,000,000 by 100,000 yields a factor of 400, so GPT-4â€™s cost is 400Ã— greater than FLM-101Bâ€™s budget."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,Doc [morrison2025#0138]: ... Llama 3.2 1B 8 0.036 ...,"The table in Doc [morrison2025#0138] reports the GPU power usage for Llamaâ€¯3.2â€¯1B at a request rate of 8â€¯req/s as 0.036â€¯kWh, which directly answers the question."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give estimates for 2028 but do not mention the number of premature deaths for 2030, so the answer cannot be determined from the available context."
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: The project is expected to employ more than 200 skilled workers during peak construction activities and provide millions of dollars in local tax revenue. Once completed, it is expected to be the largest solar farm in Maryland. Featuring more than 326,000 solar panels, Amazon Solar Farm Marylandâ€“CPV Backbone will avoid more than 64,000 metric tons of CO2e each yearâ€”the equivalent of taking more than 13,900 cars off the road.","The document states that the avoided CO2e from the solar farm is equivalent to taking more than 13,900 cars off the road, which directly answers the question."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons",13000,tons,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 'The total permitted site-level annual emission limits are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.' 
Doc [han2024]: 'The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.'","The question asks for the total permitted annual emission limits for NOx from data center backup generators in northern Virginia between Janâ€¯1â€¯2023 and Decâ€¯1â€¯2024. Both excerpts from Doc [han2024] state that the limits are approximately 13,000â€¯tons of NOx, which directly answers the query."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a]: The UNâ€™s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled,"The question asks for the percentage of e-waste formally collected and recycled according to the UNâ€™s Global E-Waste Monitor 2024. Doc [luccioni2025a] explicitly states this value as about 22%, so the answer is 22%."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"Doc [dodge2022#0056]: 'This model was not trained to completion, but only until 13%; a full training run would take 60 days.'","The question asks for the estimated duration of a full training run. Doc [dodge2022#0056] explicitly states that a full training run would take 60 days, which is taken as the answer."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,4 data centers,4,data centers,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'In 2023, AWS increased the number of data centers using recycled water for cooling fromÂ 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.'","The passage lists four specific data centers that began using recycled water for cooling in 2023 â€“ two in Virginia, one in California, and one in Singapore â€“ indicating that 4 data centers started this practice that year."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society . It considers the science, engineering, and deployment of AI-enabled computing systems.","The question asks for the launch year of the One Hundred Year Study on Artificial Intelligence. Doc [stone2022] states that it was launched in the fall of 2014, so the answer is 2014."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention a hardware processor used for energyâ€‘efficient local inference in financial sentiment classification, so the answer cannot be determined from the context."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","Command-R Plus (3,426 Wh for 1,000 queries)",3426,Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: h GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The appendix of the 2025 study (Doc [luccioni2025c]) lists GPU energy consumption per 1,000 queries for various models, and the highest value reported is 3,426 Wh for Commandâ€‘R Plus, indicating it has the greatest inference energy usage among the listed models."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3Ã—,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3Ã—.'","The question asks by what factor the carbon footprint decreases when GPU utilization rises to 80% for LM training. Doc [wu2021a] explicitly states the decrease is 3Ã—, so the answer is 3Ã—."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The question asks how many experts are in each MoE layer. Doc [shen2024] explicitly states that each layer has 8 experts, so the answer is 8 experts."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).',"The question asks for the cost in zettaFLOPs of training the English portion of FLM-101B. Doc [li2025a] explicitly states that the English portion costs 28.22 zettaFLOPs, so that is the value to report."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models,88,models,luccioni2024,https://arxiv.org/pdf/2311.16863,Doc [luccioni2024]: 'we ran each of the 88 models' ; Doc [luccioni2024#0029]: 'we sampled 88 models',"The study explicitly states that it sampled 88 machine learning models, and that each of the 88 models was run, confirming the total number of models examined."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,Doc [luccioni2025a#0002]: 'the problem of Jevonsâ€™ Paradox applies to AI' ; Doc [luccioni2025a#0009]: 'Economists refer to such transformations as Jevonsâ€™ Paradox',"The question asks which economic principle explains why technical efficiency gains may not reduce environmental impact. Both passages from the 2025 paper explicitly name Jevonsâ€™ Paradox as the principle behind rebound effects, so the answer is Jevonsâ€™ Paradox."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.25%,0.25,%,is_blank,is_blank,Doc [patterson2021#0065]: 'Percent of model activated on every token 0.25%',"The table in Doc [patterson2021#0065] lists the Switch Transformer (1500â€¯B parameters) as having 0.25% of its parameters activated per token, which answers the question directly."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","The question asks for the wallâ€‘clock time using the growth strategy. The passage from Doc [li2025a] explicitly states that the total time cost for training FLM-101B under the growth schedule is 21.54 days, which directly provides the required answer."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 â€“ 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: ""global AI demand is projected to account for 4.2 â€“ 6.6 billion cubic meters of water withdrawal in 2027""","The question asks for the projected water withdrawal in 2027. Doc [li2025b] states that the global AI demand is projected to account for 4.2 â€“ 6.6 billion cubic meters of water withdrawal in 2027, so the answer is that range."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'RedÂ AI is on the rise despite the well-known diminishing returns of increased cost (e.g., FigureÂ 3).'","The question asserts that Red AI is on the decline, but the cited passage from Doc [schwartz2019] explicitly states that Red AI is on the rise, directly contradicting the statement. Therefore the correct answer is False."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the execution time of the longest kernel in the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU, so I cannot answer this question based on the available context."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.6 MWh,103.6,MWh,is_blank,is_blank,"Doc [dodge2022#0056]: 'The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/...' Doc [dodge2022#0054]: 'We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.'","The question asks for the estimated total energy consumption in MWh for a full training run of a 6.1â€¯billionâ€‘parameter transformer. Doc [dodge2022#0056] gives the partial run energy (13.8â€¯MWh for 13% of the run) and indicates the full run estimate, while Doc [dodge2022#0054] directly states a full run would consume about 103,593â€¯kWh (â‰ˆ103.6â€¯MWh). These documents provide the numeric estimate, so the answer is 103.6â€¯MWh."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Fig. 10. A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.'","The cited passage from Doc [wu2021a] explicitly states that the majority of experimentation workflows use GPUs at only 30-50% capacity, which is far below the claimed 80% threshold, so the statement is false."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not clearly identify which numerical value corresponds to FLM-101Bâ€™s average performance on the Open LLM Leaderboard, so the answer cannot be determined with confidence."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].'","The question asks for the reported percentage drop in global carbon emissions in 2020, and Doc [wu2021b] explicitly states the emissions fell by 6.4%, so that is the answer."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.'","The question asks for the name of the collaborative project that aims to create a standardized method for comparing inference efficiency. The passage from Doc [luccioni2025c] explicitly names that project as the AI Energy Score, which matches the description in the question."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,1.2x to 4x,"[1.2, 4]",x,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024#0030]: 'a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.'","The question asks for the factor by which total compute for model development exceeds that of the final training run. Doc [cottier2024#0030] (and corroborated by Doc [cottier2024] and Doc [cottier2024#0029]) states the ratio ranges from 1.2x to 4x, which directly answers the question."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity...'","All three documents explicitly state that the training of GPT-3 consumed 1,287â€¯MWh, so the answer is 1,287â€¯MWh."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",False,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.'","The statement claims a 200,000x increase, but Doc [schwartz2019] reports a 300,000x increase over the same sixâ€‘year period, so the claim is not supported and is false."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts per token per layer,2,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'we set the same number of experts to 8 and topâ€‘k to 2 for every layer.',"The question asks how many experts are activated per token in each layer of JetMoEâ€‘8B. Doc [shen2024] explicitly states that the model uses topâ€‘k = 2, meaning two experts are selected for each token in every layer. Thus the answer is two experts per token per layer."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the total execution time in seconds of a dense BlackMamba model with batch size 30 fineâ€‘tuned on an NVIDIA A40-48 GB GPU, so I cannot answer this question with confidence."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not give enough information to determine the percentage of AI inference workloads in Asia powered by coal in 2023.
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,272 transatlantic flights,272,flights,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: the cumulative emissions from approximately 272 transatlantic flights between Boston and London.,"The question asks how many transatlantic flights the projected annual emissions of GPTâ€‘4o inference are comparable to. Doc [jegham2025] explicitly states that the emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights, so 272 is the answer."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,is_blank,is_blank,Doc [luccioni2025c#0096]: ... Llama 7B 63 Meta 356 14 ...,"The table in Doc [luccioni2025c#0096] lists the GHG emissions for preâ€‘training the Llama 7B model as 14 tCO2e, which directly answers the question."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,60 years,60,years,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022#0021]: 'there have been significant advances since the fieldâ€™s inception sixty years ago.'
Doc [stone2022]: 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.'","The documents state that the fieldâ€™s inception was sixty years ago and that it began in 1956, which places the fieldâ€™s age at roughly 60 years in 2025. Therefore, the approximate age is 60 years."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS),Neural Architecture Search,is_blank,luccioni2025c;luccioni2023,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2302.08476,"Doc [luccioni2025c#0037]: 'NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently.'; Doc [luccioni2023]: 'training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.'","The question asks for the specific AI process underlying the fiveâ€‘cars estimate. The cited passages show that the 2019 estimate was derived from the Neural Architecture Search (NAS) training of a large Transformer model, a process that is infrequently performed, thereby explaining the origin of the fiveâ€‘cars figure."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents explicitly state the energy consumption in MWh for pre-training the BLOOM model, so I cannot answer confidently."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22 AWS data center regions,22,regions,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023#0277]: 'Amazonâ€™s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed byÂ 22Â AWS data center regions is matched with renewable energy so'","The question asks for the number of AWS data center regions that achieved 100% renewable electricity consumption in 2023. The cited passage states that 22 regions met this target, so the answer is 22 regions."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The question asks for Amazonâ€™s energy storage capacity in 2023. Doc [amazon2023] states that Amazon now holds 1.3 GW of storage capacity, so the answer is 1.3 GW."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a specific energy consumption value for the o3 model when handling a long prompt, so I cannot answer the question with confidence."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,PUE,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'The average data center PUE in 2023 was 1.58 globally',"The question asks for the global average PUE of AIâ€‘dedicated data centers in 2023. The only global PUE figure given in the context is 1.58 for 2023, which is the best available estimate for AIâ€‘dedicated centers, so the answer is 1.58 PUE."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,year,stone2022,https://arxiv.org/pdf/2211.06318,Doc [stone2022]: 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.',"The question asks for the year the field was officially christened. Doc [stone2022] explicitly states that this occurred in 1956, so the answer is 1956 (year)."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency]'.","The question asks whether the statement about 770â€¯million people lacking stable electricity is true. Doc [wu2021b] explicitly states that approximately 770 million people lack such access, so the correct answer is True."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a specific percentage reduction for energy use when targeting a TPOT of 100â€¯ms for Llamaâ€¯3.1â€¯8B. The only explicit percentage given is a 44â€¯% reduction at 77â€¯ms, which does not directly answer the question for 100â€¯ms."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B parameters,2,B,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.'","The question asks how many parameters are activated during inference. The quoted passage from Doc [shen2024] explicitly states that JetMoE-8B activates 2B parameters per input token, so the answer is 2B."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,8 samples,8,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 1) Maximum Batch Size Support: The maximum batch size in fineâ€‘tuning is determined by GPU memory size, model size, sequence length, and MoE sparsity. The LLM TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8 Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)","The table in Doc [xia2024] lists the maximum batch sizes for BlackMambaâ€‘S (sparse) on the MATH dataset, which corresponds to the GSM8K dataset. The value shown is 8, indicating that the maximum batch size supported for fineâ€‘tuning BlackMamba with a sparse setup on the GSM8K dataset using an NVIDIA A40 GPU is 8 samples."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 J per token,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The question asks for the energy consumption per token at a maximum generation length of 512 tokens. Doc [samsi2024] explicitly states that at length 512 it takes about 3-4 Joules per output token, providing a clear numeric range for the answer."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'sions arising from energy sources used to power model training and deployment, including servers and data center cooling.'; Doc [morrison2025]: 'Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training a'","The excerpts from Doc [morrison2025] explicitly state that operational environmental impacts encompass GHG emissions from energy sources used to power model training, which includes servers and data center cooling. Therefore, the claim that such emissions are not included is false."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","25,000 USD",25000,USD,is_blank,is_blank,"Doc [schwartz2019#0023]: 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.'","The context explicitly states that training Grover on 256 TPU chips for two weeks cost an estimated $25,000, so the answer is 25,000 USD."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1).,"The question asks for the percentage of running time the optimizer stage occupies in BlackMamba sparse fineâ€‘tuning on an NVIDIA A40â€‘48GB GPU with batch size 1. The passage from Doc [xia2024] explicitly states that this share is up to 53%, providing the required value."
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a specific average WUE value for Google's AIâ€‘dedicated data centers in 2024, so a confident answer cannot be derived from the available context."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,members,stone2022,https://arxiv.org/pdf/2211.06318,Doc [stone2022]: 'The seventeen-member Study Panel',"The question asks for the number of members in the inaugural 2015 Study Panel. Doc [stone2022] explicitly states that the panel was a seventeenâ€‘member Study Panel, so the answer is 17 members."
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'By converting 32-bit ï¬‚oating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.'","The question asks for the percentage reduction in RM2 model size after quantization. Doc [wu2021a] explicitly states a 15% reduction, directly answering the question."
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: 'The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCGâ€™s experience...',"The question asks whether the claim is supported by clear, publicly available calculations and sound scientific grounding. Doc [luccioni2025c] states that the 5â€‘10% estimate is based on BCG experience, that calculations are not detailed, and that applying project observations to global emissions lacks scientific grounding. Therefore the claim is not supported, so the answer is False."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].',"The question asks for the average global data center PUE in 2023. Doc [ebert2024] explicitly states that the average PUE was 1.58, so that is the answer."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",TRUE,1,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy.'  Doc [patterson2021#0111]: 'Sparsely activated mixture-of-expert-style models can provide more than 10X reductions in computation requirements and energy costs for both training and inference while providing significantly higher accuracy than dense.'","The documents state that sparsely activated DNNs consume less than oneâ€‘tenth the energy of dense DNNs while maintaining accuracy, confirming the claim as true."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.'","The question asks for the percentage of Gemini Ultraâ€™s development cost attributed to R&D staff (including equity). The cited passage in Doc [cottier2024] states that Gemini Ultraâ€™s R&D staff cost fraction is 49%, providing the required value."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.'","The passage from the Study Panel states that it found no cause for concern that AI is an imminent threat to humankind, so the statement that it is concerned is false."
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,Doc [luccioni2024]: 'Flan-T5-xxl 11B 11.48 0.083'; Doc [luccioni2024#0016]: '1 billion queries to the model per day.',"Doc [luccioni2024] gives that Flanâ€‘T5â€‘xxl consumes 0.083â€¯kWh per 1,000 queries. 1â€¯billion queries equal 1,000,000â€¯kâ€‘queries, so energy =0.083â€¯kWhâ€¯Ã—â€¯1,000,000â€¯=â€¯83,000â€¯kWh, which is 83â€¯MWh. Doc [luccioni2024#0016] confirms the 1â€¯billionâ€‘query daily volume."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,"14,400 electric delivery vans",14400,electric delivery vans,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: United StatesÂ â€¢ Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context gives 2,600 electric delivery vans in 2022 and 11,800 in 2023 for Amazonâ€™s U.S. fleet. Adding these two numbers yields a total of 14,400 vans added across the two years."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: ... manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.,"The question asks for the percentage of a client deviceâ€™s total carbon footprint attributable to manufacturing. The passage from Doc [wu2021a] states that manufacturing carbon cost accounts for 74% of the total footprint of client devices, so 74% is the answer."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a specific figure for the metric tons of CO2 emitted by OpenAI's API requests in January 2024.
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e, over 40Ã— the five cars estimate",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","The context explicitly states that Metaâ€™s Llama 3 family emitted 11,390â€¯tCO2e during preâ€‘training, which is cited as more than 40 times the carbon footprint represented by the â€˜five carsâ€™ estimate."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a value for the Earth-Sun distance in miles.
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9Ã—,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9Ã— increase in AI training infrastructure capacity over the 1.5 years.',"The question asks for the factor of increase in AI training infrastructure capacity over the 1.5â€‘year period. Doc [wu2021a] explicitly states a 2.9Ã— increase, so the answer is 2.9Ã— with a dimensionless unit."
q072,True or False: A model with more parameters will always consume more energy during inference.,False,0,is_blank,is_blank,is_blank,"Doc [chung2025#0062]: '-world LLM-based applications, we expect this trend to continue. Memory consumption of operations and energy amortization. Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.'","The passage from Doc [chung2025#0062] explicitly states that while larger models tend to consume more energy, this is not always trueâ€”larger Phiâ€‘3 Small consumes less energy than the smaller Mini under certain batch sizes. Therefore, the claim that a model with more parameters will always consume more energy during inference is false."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous Batching,Continuous Batching,is_blank,is_blank,is_blank,"Doc [fernandez2025#0045]: 'Continuous Batching Reduces Energy Use. ... continuously replacing completed requests with new ones, ...'","The question asks for the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones. Doc [fernandez2025#0045] and Doc [fernandez2025#0046] describe this as Continuous Batching, so that is the answer."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents do not provide a clear number of wind turbines directly contracted by Microsoft for Azure AI clusters in 2023, so I cannot answer the question with confidence."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: The entire alignment process takes 60 H100 GPU hours.,"The document states that the whole alignment process, which includes both dSFT and dDPO fineâ€‘tuning, requires 60 H100 GPU hours, providing the numerical answer directly."
q080,True or False: The AlphaGo program defeated the human Go champion.,True,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: the recent success of  AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to","The passage from Doc [stone2022] explicitly states that AlphaGo beat the human Go champion, so the statement is true."
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).'","The question asks for the range of GPU energy usage for 1,000 inference queries in the 2025 studyâ€™s appendix. Doc [luccioni2025c] explicitly states that the energy ranges from 0.06â€¯Wh to over 3,426â€¯Wh, so that is the required range."
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams of CO2eq",1594,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of ð¶ðŽ2ð‘’ð‘ž for 1,000 inferences,'","The question asks for the emissions per 1,000 inferences of the stableâ€‘diffusionâ€‘xlâ€‘baseâ€‘1.0 model. Doc [luccioni2024] explicitly states that this model emits 1,594 grams of CO2eq for 1,000 inferences, so that is the required value."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e/KWh,0.429,kg CO2e/KWh,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].,"The question asks for the gross carbon intensity of energy in 2021 for the U.S. average mix.  Doc [patterson2021] explicitly states that the value is 0.429 kg CO2e per kWh, which directly answers the query."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems [ 56].","The question asks for the term proposed to broaden AI transparency to include socio-technical aspects and environmental footprint. Doc [luccioni2025b] explicitly names this term as ""social transparency,"" providing the direct answer."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,is_blank,is_blank,"Doc [erben2023#0021]: 'Hivemind [39] is a PyTorch-based [32] framework ...'
Doc [erben2023#0165]: 'We used the Hivemind framework for all of our experiments, as it provided the base for training on spot instances in high latency, low bandwidth networks.'","The question asks for the decentralized PyTorch-based framework that enables distributed spot instance training across clouds and continents. Both documents explicitly identify Hivemind as that framework, confirming it is the correct answer."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",26.7%,26.7,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'SLO Evaluated PoliciesSelected InstancesCoff(%) TPS(avg.)Total Price($) 100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13 Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699'","The Maxâ€‘Performance policy selected g6e.xlarge costing $2.699, while InferSave selected g4dn.xlarge costing $2.13. The difference is $0.569, which is (0.569/2.13)*100 â‰ˆ 26.7% more expensive."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b#0134]: 'We do not pretend to have developed a universal approach for either of these issues (a nd do not believe that one can exist) â€“ ...' ; Doc [luccioni2025b]: 'Given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines.'","The documents state that researchers do not believe a universal, oneâ€‘sizeâ€‘fitsâ€‘all approach can be developed, so the statement is False."
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention classification experiments on German public administration texts or specify a sentenceâ€‘embedding modelâ€™s accuracy.
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,is_blank,is_blank,"Doc [chen2024#0003]: 'we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.'","The question asks for the name of the LLM inference system developed in the Chen et al. paper that uses model-attention disaggregation. The passage from Doc [chen2024#0003] explicitly names the system as 'Lamina', confirming the answer."
q096,What is the name of the emissions metric defined as 'COâ‚‚ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,Doc [khan2025]: 'Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency',"The question asks for the metric name defined as COâ‚‚ emissions per unit of electricity consumed. Doc [khan2025] explicitly defines the metric â€œCarbon Intensityâ€ with the unit gCO2/kWh and the description â€˜CO2 emissions per unit of electricity consumedâ€™, making it the correct answer."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,Doc [li2025b]: 'the companyâ€™s data center water consumption increased by âˆ¼20% from 2021 to 2022 and by âˆ¼17% from 2022 to 2023 [4]'. Doc [luccioni2025a]: 'Google observed a 20% uptick in the same period'.,"The question asks for the percentage increase in Googleâ€™s data center water consumption from 2021 to 2022. Both Doc [li2025b] and Doc [luccioni2025a] explicitly state a 20% increase for that period, so the answer is 20%."
q093,How many parameters does the largest T5 model have?,11B,11,billion,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO 2 e.","The question asks for the number of parameters of the largest T5 model. Doc [patterson2021] explicitly states that the largest T5 size has 11B parameters, so the answer is 11B, which equals 11 billion parameters."
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810Ã—,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Doc [wu2021a]: 'Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810Ã—.'","The question asks the reduction factor relative to a CPU baseline. The cited passage explicitly states a 810Ã— reduction in operational carbon footprint when applying full-stack optimization, providing the factor directly."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.64 of local throughput,0.64,is_blank,is_blank,is_blank,Doc [erben2023#0088]: '... 36% slower for NLP compared to the A-4 runs',"The documents state that the Câ€‘4 (fourâ€‘continent) experiment for NLP is 36% slower than the local Aâ€‘4 run, meaning it achieves 64% (0.64) of the local throughput."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3.7,million GPUs,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a#0081]: 'NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].'","The question asks for the number of data center GPUs NVIDIA shipped in 2024. Doc [luccioni2025a#0081] explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, which directly answers the query."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5B liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: '3.5B Liters of water returned to communities from replenishment projects in 2023, with additional volume contracted and r'","The Amazon 2023 sustainability report reports that 3.5 billion liters of water were returned to communities from replenishment projects in 2023, which directly answers the question."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: '101 4 4 12 192 2160 165 52.88%',"The table for the 101B stage of FLM-101B training lists the utilization as 52.88%, which is the achieved FLOPs utilization percentage in the final growth stage."
q094,What is the total number of parameters in the JetMoE-8B model?,8B parameters,8,B,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'JetMoE-8B has 8B parameters while only activating 2B for each input token.'; Doc [shen2024]: '# Total Params 7B 16B 2B 8B',"The context explicitly states that JetMoE-8B has 8 billion parameters, as shown in the table of total parameters and in the sentence describing its parameter count. Therefore the total number of parameters is 8B."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems","The question asks for the acronym of the Finnish project. The passage from Doc [luccioni2025b] explicitly names the project as ""ETAIROS"" and describes its focus, providing the required answer."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",True,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Doc [rubei2025]: 'Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.',"The document states that custom tags reduce energy consumption for zeroâ€‘shot, oneâ€‘shot, and fewâ€‘shots in source code completion, confirming the statement as true."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: 'the most expensive publicly-announced training runs to date are OpenAIâ€™s GPT-4 at $40M and Googleâ€™s Gemini Ultra at $30M.',"The question asks for the estimated amortized training costs of GPTâ€‘4. Doc [cottier2024] explicitly states that GPTâ€‘4â€™s amortized training cost is $40â€¯million, providing the numeric value and unit needed for the answer."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 Âµg/mÂ³,9,Âµg/mÂ³,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024]: 'the EPAâ€™s recently tightened standard for PM2.5 sets an annual average limit of 9Âµg/m 3',"The question asks for the EPAâ€™s tightened primary standard for the annual average limit of PM2.5. Doc [han2024] explicitly states that the standard is 9â€¯Âµg/mÂ³, so that is the answer."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,%,is_blank,is_blank,"Doc [cottier2024#0049]: '... on average, 44% goes toward AI accelerator chips.'","The question asks for the average percentage of amortized hardware and energy cost attributed to AI accelerator chips. Doc [cottier2024#0049] (and repeated in Doc [cottier2024#0050]) explicitly states that on average 44% of the cost goes toward AI accelerator chips, providing the required answer."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the energy consumption of the DS Llama 70B model on the FKTG dataset, so the answer cannot be determined from the available context."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: ""Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI providers, these provisions lack sufficient emphasis on environmental factors.""","The AI Act mandates risk assessment for GPAI models with systemic risk, but the documents state that the obligation does not include environmental risks, indicating that the Act does not require environmental risk assessment."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'a life cycle assessment (LCA) ... has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].'","The question asks for the number of physical print books equivalent to one Kindleâ€™s CO2 emissions. Doc [luccioni2025a] explicitly states that 115 books equal one Kindle, so the answer is 115 books."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,"Doc [wu2021a]: 'Achieving a Power Usage Effectiveness (PUE) of about 1.10,'","The question asks for Facebookâ€™s data center PUE. Doc [wu2021a] explicitly states the PUE is about 1.10, and Doc [wu2021b] confirms the value as 1.10 for 2020. Thus the answer is 1.10."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200x,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024#0003]: 'per-household health burden could be 200x more than that in less-impacted communities.',"The question asks for the factor by which the per-household health burden in disadvantaged communities exceeds that in less-impacted communities. Doc [han2024#0003] explicitly states a 200â€‘fold increase, so 200 is the answer."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",over 6.1 billion parameters,6.1,billion parameters,is_blank,is_blank,Doc [dodge2022#0056]: 'We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s.',"The 2022 Dodge et al. paper reports that the large language model they analyzed had over 6.1â€¯billion parameters, as stated in Doc [dodge2022#0056]."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,$30M,30,M USD,cottier2024,https://arxiv.org/pdf/2405.21015,Doc [cottier2024]: training costs. We find that the most expensive publicly-announced training runs to date are OpenAIâ€™s GPT-4 at $40M and Googleâ€™s Gemini Ultra at $30M.,"The passage from Doc [cottier2024] explicitly states that Gemini Ultraâ€™s amortized training cost is $30M, so the answer is $30M with a unit of million USD."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 pounds of CO2e",36156,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156'","The question asks for the estimated CO2e in pounds for an average American life in one year. In Doc [strubell2019] a table lists ""American life, avg, 1 year 36,156"" under the heading â€˜Consumption CO 2e (lbs)â€™, indicating 36,156 pounds of CO2e. Hence the answer is 36,156 lbs."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox,Jevons Paradox,is_blank,luccioni2025b;luccioni2025a,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025b]: 'when technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use.'; Doc [luccioni2025a]: 'Jevonsâ€™ Paradox anticipates increases in resource consumption following efficiency improvements.'","The question describes a phenomenon where efficiency gains lead to higher usage and overall resource consumption. Both Doc [luccioni2025b] and Doc [luccioni2025a] identify this as Jevons Paradox, confirming the answer."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not contain a clear statement of the multiplier by which Mistralâ€‘smallâ€™s emissions changed after optimization in the financial sentiment classification task.
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,Doc [luccioni2024]: 'image generation 2.907 3.31',"Table 2 of the 2024 study lists the mean energy consumption for 1,000 image generation inferences as 2.907 kWh, which is the value requested."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a percentage of recycled rare earth metals used in NVIDIA H100 GPUs manufactured in 2024.
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,is_blank,is_blank,"Doc [luccioni2024#0109]: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.'","The question asks for the total energy consumed in the 2024 study 'Power Hungry Processing'. Doc [luccioni2024#0109] explicitly states that 754.66 kWh of energy were used for all model experimentation and evaluation, providing the required figure."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed","The question asks for the percentage of token usage that occurred through models with no environmental impact disclosure. Doc [luccioni2025c] explicitly states that 84% of LLM usage is through such models, so 84% is the answer."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 'the energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process'","The Power Hungry Processing study gives the training energy for BLOOMz-7B as 51,686â€¯kWh and the fineâ€‘tuning energy as 7,571â€¯kWh (Doc [ebert2024]). Adding these values yields 59,257â€¯kWh, which is the combined energy cost reported for training and fineâ€‘tuning the BLOOMzâ€‘7B model."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The table in Doc [luccioni2024] lists the number of inferences needed for each BLOOMz model for deployment energy to match training and fineâ€‘tuning energy. For BLOOMzâ€‘7B the value is 592,570,000, which is the answer to the question."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents mention a dataset name for German nuclear waste site objection texts used in experiments.
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons of CO2,"[21, 78]",tCO2,is_blank,is_blank,"Doc [dodge2022#0092]: 'If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it w' and Doc [dodge2022#0069]: 'If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it w'","The question asks for the estimated CO2 range for a full training run of a 6.1â€¯billionâ€‘parameter transformer. Both Doc [dodge2022#0092] and Doc [dodge2022#0069] state that a complete run would emit 21â€“78 metric tons of CO2, so this range is taken as the answer."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: 'Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.',"The question asks for the cost savings percentage when mixing A100 and A10G GPUs versus an A100-only strategy. The cited passage from Doc [griggs2024] explicitly states a 24% savings, providing the required value and unit."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The documents do not provide a specific numeric value for carbon emissions avoided by pruning and quantizing large language models in 2023.
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",$4.63/hr,4.63,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,Doc [chen2024]: 'Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr',"The table in Doc [chen2024] lists the price per chip for the NVIDIA H20 as $4.63 per hour, which directly answers the question about the hourly price for that GPU."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","Marshall County, West Virginia",Marshall County,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: 1405.5) 0.62
OH Gallia 1107.6(828.1, 1387.2) 0.74
WV Marshall 1083.8(831.2, 1336.5) 0.77
WV Taylor 1052.5(853.1, 1252.0) 0.70
PA Fayette 992.6(782.0, 1203.3) 0.74
PA Greene 944.9(770.3, 1119.5) 0.88
WV Brooke 918.8(693.2, 1144.4) 0.69
WV Jackson 871.9(703.8, 1039.9) 0.73","The table of county-level per-household health costs lists West Virginia counties and shows WV Marshall with a cost of 1083.8, which is higher than the other WV counties listed. Therefore, Marshall County is projected to have the highest per-household health cost."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,3 passengers,3,passengers,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: e   of   84.5%   seat   occupancy,   yielding   1.2t   of   CO 2 e   per   passenger   round   trip.   Thus,   the   CO 2 e   equivalent   of   NAS   is   ~3   passengers   taking   a   round   trip   between   San   Francisco   and   New   York.   22","The passage from Doc [patterson2021] states that the 3.2â€¯tCO2e emissions of the Evolved Transformer NAS are equivalent to about 3 passengers on a roundâ€‘trip SFâ€“NY flight, so the answer is 3 passengers."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64,"The table in Doc [samsi2024] lists the bare minimum hardware for each LLaMA variant; for 13B it shows a count of 1 A100 80GB GPU, indicating that one such GPU is sufficient for inference without compression or quantization."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'FLM-101B Params 175B 280B 540B 130B 70B 101B',"The question asks for the total number of parameters in the final FLMâ€‘101B model. In Doc [li2025a] the table lists the parameter counts for each model and shows FLMâ€‘101B with 101B parameters, so the answer is 101B."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44 %,44,%,han2024,https://arxiv.org/pdf/2412.06288,Doc [han2024]: 'This is equivalent to approximately 44% of the data centersâ€™ total electricity c',"The context states that in 2023 the public health cost of U.S. data centers was about 44% of their total electricity cost, a figure derived using the average attribution method."
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,False,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: 'most carbon foot print analyses gather the information manually by writing to authors.',"The question asserts that most carbonâ€‘footprint analyses are automated, but Doc [luccioni2025b] explicitly states that most analyses gather the information manually by writing to authors, contradicting the claim."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The context does not provide clear, unambiguous numeric values for the total energy consumed by a full GPT-3 training run or by a single Meena training run, so I cannot compute the ratio with confidence."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,answers,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: '... and were only able to collect 95 answers, with many authors refusing to provide the relevant information...'","The question asks how many answers were obtained after contacting over 500 authors. Doc [luccioni2025b] explicitly states that 95 answers were collected, providing the precise number needed to answer the question."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,True,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"Doc [khan2025]: 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.'","The question asks whether sustainable deployment techniques for LLMs can achieve up to a 45% reduction in carbon emissions after quantization. Doc [khan2025] explicitly states that these methods reduce emissions by up to 45% post quantization, confirming the statement as true."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",Approximately 1.9â€¯billion inferences,1918500000,inferences,is_blank,is_blank,"Doc [dodge2022#0056]: 'The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days.'
Doc [luccioni2024#0085]: 'Inference energy (kWh) 1.0 Ã— 10âˆ’4 7.3 Ã— 10âˆ’5 6.2 Ã— 10âˆ’5 5.4 Ã— 10âˆ’5'","The full training run for the 6.1B model is estimated at 103,593â€¯kWh (Doc [dodge2022#0056]). The BLOOMzâ€‘7B inference energy per inference is 5.4â€¯Ã—â€¯10â»âµâ€¯kWh (Doc [luccioni2024#0085]). Dividing 103,593 by 5.4â€¯Ã—â€¯10â»âµ gives â‰ˆ1.9â€¯Ã—â€¯10â¹ inferences, matching the training energy cost."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64',"The table in Doc [samsi2024] lists the bare minimum hardware for the LLaMAâ€‘7B model, showing that a single A100 80GB GPU suffices for inference without compression or quantization."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,is_blank,is_blank,Doc [li2025b#0034]: 'Apple reports that its supply chain accounts for 99% of its total water footprint [23].',"The passage directly states that 99% of Appleâ€™s total water footprint comes from its supply chain, which answers the question about the percentage of the total water footprint attributable to the supply chain."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36 projects,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Doc [amazon2023]: Projects announced as of January 2024. Project Location Number of Projects Total MW Capacityâ€  United Kingdom 36 901,"The table of Amazon Renewable Energy Projects announced as of January 2024 lists 36 projects in the United Kingdom, which is the number requested."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",$3.33 per H100 GPU-hour (approx.),3.33,$/h,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'less than$0.1 million, using 30,000 H100 GPU hours.'","The JetMoE project budget is less than $0.1 million and it used 30,000 H100 GPU hours. Dividing the budget by the hours gives an approximate cost of $3.33 per H100 GPU-hour."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation).'","The question asks for the term describing freshwater extracted from ground or surface sources, either temporarily or permanently, for various uses. Doc [li2025b] explicitly defines this as ""Water withdrawal,"" which matches the description."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,"55,475,000,000 liters",55475000000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'Metaâ€™s global electricity water consumption intensity factor of 3.70 L/kWh in 2024 (i.e., 55,475 megaliters divided by 14,975,435 MWh)'","The document gives Metaâ€™s 2024 water consumption as 55,475 megaliters, which converts to 55,475,000,000 liters. This is the total freshwater consumed by the companyâ€™s data center fleet, including the LlamaÂ 3 inference serving clusters."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",exceeds 120%,120,%,is_blank,is_blank,Doc [han2024#0121]: 'The total health cost can even exceed 120% of the electricity cost and vary widely depending on ...',"The question asks for the health cost as a percentage of the electricity cost for training a Llamaâ€‘3.1 model in Altoona, Iowa. Doc [han2024#0121] explicitly states that the health cost can even exceed 120% of the electricity cost, which directly answers the query."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times more,6.4,times,luccioni2025a;luccioni2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2504.00797,Doc [luccioni2025a]: 'could add up to 640 percent more carbon emissions compared to the companyâ€™s carbon removal targets for the year',"The documents state the deal could add up to 640â€¯percent more emissions, which equals 6.4 times the yearly removal target, so the answer is 6.4 times more."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25T tokens,1250000000000.0,tokens,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: 'JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.'","The question asks for the number of tokens used to preâ€‘train JetMoEâ€‘8B. Doc [shen2024] explicitly states the model was trained on 1.25 trillion tokens, so the answer is 1.25T tokens."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a numeric execution time for sparse BlackMamba fineâ€‘tuning on an NVIDIA A40 with batch size 84.
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,is_blank,is_blank,"Doc [erben2023#0015]: ... we introduce thegranularity metric, the ratio of calculation to communicati ...","The question asks for the metric used to evaluate the ratio of computation to communication time in intercontinental distributed training. Doc [erben2023#0015] explicitly states that the authors ""introduce thegranularity metric, the ratio of calculation to communication time,"" confirming that the granularity metric is the answer."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,Every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: led computing systems. As its core activity , the Standing Committee that oversees the One Hundred Y ear Study forms a Study Panel every five years to assess the current state of AI.",The passage from Doc [stone2022] explicitly states that the Standing Committee forms a Study Panel every five years.
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021]'.","The question asks for the average number of connected devices per U.S. household in 2021. Doc [wu2021b] explicitly states that the average household has 25 connected devices, providing the required numeric answer and unit."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8â€“3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: els. In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The question asks for the range of preâ€‘training energy consumption in MWh. Doc [luccioni2025c] explicitly states that publicly available data show a span from 0.8â€¯MWh to 3,500â€¯MWh, which matches the requested range."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"Doc [stone2022]: 'IBMâ€™s Watson program, which beat human contenders to win the Jeopardy challenge in 2011'","The question asks whether IBMâ€™s Watson did NOT beat human contenders. The cited passage from Doc [stone2022] states that Watson did beat human contenders, so the correct answer is FALSE."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10 â€“ 50 medium-length completions,"[10, 50]",completions,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'GPT-3 needs to â€œdrinkâ€ (i.e., consume) a 500ml bottle of water for roughly 10 â€“ 50 medium-length responses, depending on when and where it is deployed.'","The context states that one 500â€¯mL bottle of water allows GPTâ€‘3 to produce between 10 and 50 mediumâ€‘length completions, so the answer is a range of 10â€“50 completions."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80â€“90%,"[80, 90]",%,is_blank,is_blank,"Doc [patterson2021#0020]: 'Most companies spend more energy on serving a DNN model (performing inference) than on training it. For example, NVIDIA estimated that 80â€“90% of the ML workload is inference processing [Leo19].'","The question asks for the percentage of ML workload that NVIDIA estimated to be inference processing in 2019. Doc [patterson2021#0020] explicitly states that NVIDIA estimated 80â€“90% of the ML workload is inference, so the answer is that range."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10â€“50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,"Doc [luccioni2025a]: 'with one paper suggesting that 10â€“50 queries on GPT-3 consumes around half a liter of water [68]'. Doc [li2025b]: 'GPT-3 needs to â€œdrinkâ€ (i.e., consume) a 500ml bottle of water for roughly 10 â€“ 50 medium-length responses, depending on when and where it is deployed.'","The question asks for the number of GPTâ€‘3 queries that consume about half a liter of water. Both Doc [luccioni2025a] and Doc [li2025b] state that 10â€“50 queries use roughly 0.5â€¯L, so the answer is a range of 10 to 50 queries."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips",10000,round trips,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [han2024#0002]: 'Training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.'","The question asks how many car round trips between LA and NYC are equivalent to the air pollutants emitted by training a Llamaâ€‘3.1â€‘scale model. Doc [han2024#0002] explicitly states the figure is more than 10,000, so the answer is 10,000+ round trips."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4 A100 GPUs,4,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.'","The question asks for the bare minimum number of A100 80GB GPUs needed for LLaMAâ€‘65B inference. Both Doc [samsi2024] and Doc [rubei2025#0055] state that 4 A100 GPUs are required, so 4 is the correct answer."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681 MT-Bench score,6.681,MT-Bench score,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: JetMoE-8B-chat 6.681 Llama-2-13b-chat 6.650,"The table in Doc [shen2024] lists the MT-Bench scores for JetMoE-8B-chat and Llama-2-13b-chat. JetMoE-8B-chat scores 6.681, which is higher than Llama-2-13b-chatâ€™s 6.650, indicating that after alignment JetMoE-8B-Chat achieved 6.681."
q168,The 2024 Griggs et al. paper reports that MÃ©lange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: 'Compared to using only a single GPU type, MÃ©lange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.'","The question asks for the maximum deployment cost reduction reported for conversational chat settings. Doc [griggs2024] states that the reduction is up to 77% in conversational settings, which directly answers the question."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",754.6 tCO2e,754.6,tCO2e,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: In total, for all of model experimentation and evaluation, we used a total of 754.6","The passage from Doc [luccioni2024] states that the entire Power Hungry Processing 2024 study produced a total of 754.6 CO2 equivalent emissions, which is the quantity requested."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.5164,7.5164,$\/h,griggs2024,https://arxiv.org/pdf/2404.14527,Doc [griggs2024]: On-demand Price ($/h) 0.7 1.01 3.67 7.5164,"The table in Doc [griggs2024] lists the normalized onâ€‘demand hourly price for the H100 GPU as 7.5164 $/h, which is the value requested."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","1,000Ã— larger",1000,is_blank,is_blank,is_blank,"Doc [wu2021a#0013]: ... to increase the model quality BLEU score from 5 to 40 requires a model 1, 000Ã— larger in size.","The document states that boosting the BLEU score from 5 to 40 for a GPTâ€‘3 based translation model requires the model to be 1,000 times larger, so the answer is a 1000Ã— increase."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,False,0,is_blank,is_blank,is_blank,Doc [chung2025#0055]: 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU â€“ or any computing device â€“ to draw its maximum power at every moment in time.',The passages from Doc [chung2025#0055] and Doc [chung2025#0056] state that estimating GPU energy consumption solely from TDP leads to overestimation and can be up to a factor of 4.1 higher than actual usage. This indicates that TDP-based estimation is not a reliable and accurate method.
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"Doc [shen2024]: '30,000 H100 GPU hours.'","The question asks for the number of H100 GPU hours used to preâ€‘train JetMoEâ€‘8B. Doc [shen2024] explicitly states that JetMoEâ€‘8B was trained with 30,000 H100 GPU hours, so that is the required answer."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.' Doc [jegham2025#0071]: 'GPT-4o consumes around 2.875 Wh while GPT-4o miniâ€™s consumption is slightly higher at 3.098 Wh.'","Both cited passages state that GPTâ€‘4o mini consumes more energy per query than GPTâ€‘4o, so the claim that it consumes less energy is false."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$7.22 per hour,7.22,$/h,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024]: 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.'","The context states the monthly onâ€‘demand rental cost for 2 A100 GPUs is over $5,200. Dividing that by 720 hours (30 days Ã— 24 h) yields approximately $7.22 per hour, which is reflected in the answer."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","1,000,000,000 dollars",1000000000,dollars,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.'; Doc [cottier2024#0065]: 'the amortized cost of frontier training runs will exceed one billion dollars by 2027.'","Both documents state that, under the current growth trend, the cost of the largest training runs will surpass one billion dollars by 2027, so the answer is one billion dollars."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about the cooling water usage for GPTâ€‘4 training.
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,is_blank,is_blank,Doc [luccioni2024#0083]: 'Inference energy (kWh) 1.0 Ã— 10âˆ’4 ...',"The inference energy per request for BLOOMzâ€‘7B is 1.0â€¯Ã—â€¯10â»â´â€¯kWh. One million inferences use 1.0â€¯Ã—â€¯10â»â´â€¯kWhâ€¯Ã—â€¯1,000,000â€¯=â€¯100â€¯kWh per download. With 606,096 downloads, total energy = 100â€¯kWhâ€¯Ã—â€¯606,096â€¯=â€¯60,609,600â€¯kWh, which is 60,609.6â€¯MWh."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give the CO2 emissions for training with NAS (626,155 lbs) but do not provide an emissions-to-driving-distance ratio or any mileage equivalent needed to compute the driving distance, so the question cannot be answered with confidence based on the available context."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 FLOPs,3.14e+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: 'OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].',"The question asks for the total floating point operations reported by OpenAI for GPTâ€‘3 training. Doc [patterson2021] explicitly states the value as 3.14E+23 FLOPs, which is used as the answer."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents contain a numeric value for the top-1 accuracy of AlexNet on ImageNet.
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 GPUs,8,GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [rubei2025#0055]: '8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required...' 
Doc [samsi2024]: 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required...'",Both documents state that the bare minimum configuration for running LLaMAâ€‘65B inference without compression or quantization requires 8 NVIDIA V100 32GB GPUs. Therefore the minimum number is 8 GPUs.
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 A800 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 Ã—80G) servers.',The document states that each of the 24 servers contains 8 A800 GPUs. Multiplying 24 servers by 8 GPUs per server gives 192 GPUs in total.
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'FAIRâ€™s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.'","The question asks for the GPU hours needed to train FAIRâ€™s RoBERTa on 160â€¯GB of text. Doc [schwartz2019] explicitly states that this training required about 25,000 GPU hours, so that is the answer."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'This implementation of the model uses Pytorch and the FairScale [20] library to enable model sharding across multiple GPUs and nodes.',"The question asks for the framework used to deploy large language models across multiple GPUs and nodes. Doc [samsi2024] explicitly states that the FairScale library is used for model sharding across multiple GPUs and nodes, so the answer is FairScale."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a direct or calculable comparison between the NAS emissions and the average American lifetime emissions, so the answer cannot be determined with confidence."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",36.8%,36.8,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,Amazon Representation by the Numbers 36.8%29.8% 8.8% 25.6%,"The Amazon Sustainability Report (2023) provides a representation table where the first percentage listed, 36.8%, corresponds to men in the United States workforce. This matches the questionâ€™s request for the U.S. workforce men percentage."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain information about Yelp sentiment analysis benchmarks or a comparison between traditional models and large language models for that specific benchmark.
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",False,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: ... direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.
Doc [luccioni2025c]: ... the broader AI industry has paradoxically been trending in the opposite direction, disclosing less information over time.","The 2025 paper reports that after 2022 the trend reversed, with disclosures decreasing rather than increasing. Therefore the statement that the trend continued to increase is false."
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents give a clear, quantified amount of water consumed per ChatGPT user session in 2023, so I cannot answer this question based on the context."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the energy consumption factor for deploying the Llama 3.1 70B model on two nodes versus one node, so the answer cannot be determined from the given context."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any information about the number of AI training runs conducted globally on renewable-only power in 2022, so I cannot answer this question with confidence."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].'","The question asks for Microsoftâ€™s reported percentage increase in global water consumption between 2021 and 2022. Doc [luccioni2025a] explicitly states a 34% increase, which directly answers the question."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025]: 'n from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73]'","The question asks for the total estimated GPTâ€‘4o queries in 2025. Doc [jegham2025] explicitly states that the total is approximately 772â€¯billion queries, so that is the answer."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: datacenter   PUE   online   every   quarter .   The   PUE   for   the   Iowa   datacenter   where   we   ran   Evolved   Transformer   is   1.11,   a   factor   of   1.4X   better.","The question asks for the PUE value of the Iowa datacenter when the Evolved Transformer was run. The passage from Doc [patterson2021] explicitly states that the PUE for that datacenter is 1.11, so that is the answer."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,U.S. homes,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025#0002]: 'Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes...'","The passage states that scaling 0.42â€¯Wh queries to 700â€¯million per day yields annual electricity use equivalent to 35,000 U.S. homes, directly answering the question."
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons of CO2e",47400,tCO2e,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'These on-site solar energy systems are estimated to generate 123,000 MWh annuallyâ€”enough energy to power over 33,600 European homesâ€”and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.'","The question asks for the avoided CO2e from Amazonâ€™s onâ€‘site solar systems. The passage in Doc [amazon2023] states that these systems avoid roughly 47,400 metric tons of CO2e annually, providing the required figure."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",16.09 zettaFLOPs,16.09,zettaFLOPs,li2025a;cottier2024,https://arxiv.org/pdf/2309.03852;https://arxiv.org/pdf/2405.21015,"Doc [li2025a#0026]: '...the 101B model with 26.54B tokens.'
Doc [li2025a]: 'Params 175B 280B 540B 130B 70B 101B'
Doc [cottier2024]: 'For example, smaller versions of GPT-3 used 4.5e22 FLOP (based on compute = 6Ã— parameters Ã— tokens)'","The final 101B stage uses 26.54â€¯B tokens and a 101â€¯Bâ€‘parameter model. Applying the formula computeâ€¯=â€¯6â€¯Ã—â€¯parametersâ€¯Ã—â€¯tokens from Doc [cottier2024] gives 6â€¯Ã—â€¯101â€¯Ã—â€¯26.54â€¯Ã—â€¯10^18â€¯=â€¯1.61â€¯Ã—â€¯10^22 FLOPs, i.e., ~16.09â€¯zettaFLOPs."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,is_blank,is_blank,"Doc [kim2025#0019]: 'as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB'","The question asks for the KV Cache size at batch size 32 for the OPTâ€‘2.7B model. Doc [kim2025#0019] explicitly states that the cache expands to 5.312GB at batch size 32, providing the precise value needed."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29% to 49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: e model (i.e. both experiments and training). We select four especially notable models for this approachâ€”GPT-3, OPT-175B, GPT-4, and Gemini Ultra. For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The question asks for the percentage range of R&D staff costs (including equity) for the four models studied by Cottier et al. The quoted passage from Doc [cottier2024] explicitly states that this range is 29% to 49% of the total amortized cost, providing the required answer."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.'","The question asks for the name of the function that adjusts for discrepancies between theoretical and actual GPU performance. Doc [kim2025] explicitly names this function as the Compute Time Calibration Function (CTCF), so that is the answer."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,True,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The passage states that openâ€‘source generalâ€‘purpose AI models are largely excluded from transparency requirements unless they pose systemic risk, which implies they are exempt from reporting energy consumption unless that risk exists. Therefore the statement is true."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59 PUE,1.59,PUE,patterson2021,https://arxiv.org/pdf/2104.10350,"Doc [patterson2021]: 'The US national datacenter average in 2018 was 1.58, which is the value [Str19] used ; In 2020, it was 1.59.'","The passage from Doc [patterson2021] explicitly states that the US national datacenter average PUE in 2020 was 1.59, so that is the answer."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",14.9 queries/sec,14.9,queries/sec,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 14.9 Dense(bsz=1) Dense(bsz=6) Sparse(bsz=1) Sparse(bsz=6) Sparse(bsz=20) Blackmamba-MATH0 5 10 15 20 2.2 5.3 2.2 6.5 11.6 Dense(bsz=1) Dense(bsz=2) Sparse(bsz=1) Sparse(bsz=2) Sparse(bsz=8) Throughput (quries/second) Fig. 8. Query throughput of Mixtral and BlackMamba.,"The figure shows throughput values for Mixtral-CS; the value 14.9 corresponds to Dense(bsz=1), which is the groundâ€‘truth throughput for a dense Mixtral-CS-A100-40GB at batch size 1."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,is_blank,is_blank,"Doc [luccioni2024#0034]: 'used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference'.  Doc [morrison2025#0042]: 'measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking.'","The question asks for the software package used to measure energy consumption during inference runs. Both Doc [luccioni2024#0034] and Doc [morrison2025#0042] explicitly state that the CodeCarbon package was used for this purpose, so the answer is CodeCarbon."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,score,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0,"The OpenLLM Leaderboard table in Doc [shen2024] lists the average scores for four models; the fourth column, corresponding to JetMoE-8B, shows an average of 53.0. This value directly answers the question about JetMoE-8Bâ€™s final average score on the leaderboard."
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,Doc [luccioni2025c]: '75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it con',The analysis of 100 news articles reported that 53% of them cited the contested 3â€¯Wh (or 10â€‘timesâ€‘Googleâ€‘search) estimate. The supporting passage from Doc [luccioni2025c] directly gives this percentage.
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.'","The passage from Doc [samsi2024] directly states that increasing the number of GPU shards increases the energy cost per response for LLaMAâ€‘65B, so the correct answer is True."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",almost 30%,30,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]','","The passage in Doc [luccioni2025a] explicitly states that Amazon, Microsoft, Meta, and Google accounted for almost 30% of all corporate PPAs in 2020, which directly answers the question."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: 3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The passage from Doc [ebert2024] states that openâ€‘source generalâ€‘purpose AI models are largely excluded from transparency requirements, meaning they are not required to report energy consumption under current EU rules. Therefore the statement is false."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].","The question asks whether public health costs of AI are evenly distributed. The cited passage explicitly states the impacts are highly unevenly distributed, making the statement false."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"Doc [khan2025#0033]: 'We apply quantization through Ollama [19], an openâ€‘source platform known for its support of edge computing principles and privacyâ€‘centric deployments.'","The question asks for the openâ€‘source tool used for 4â€‘bit quantization and local deployment in the financial sentiment case study. Doc [khan2025#0033] explicitly states that the quantization was applied through Ollama, providing the required answer."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: Figure 2: As a result of Mooreâ€™s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The question asserts that GPU theoretical performance per watt doubles every 3â€‘4 years; Doc [wu2021b] explicitly states this observation, confirming the statement is true."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 tCO2e,26,tCO2e,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: â€¦ net tCO2e 552 380 271 257 291 26,"The table in Doc [li2025a] lists the net carbon emissions for each model size; the entry for FLMâ€‘101B (the 101B column) shows a net of 26 metric tons of CO2e, which is the required total estimated net emissions for its preâ€‘training."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024]: Table 1 shows the public health cost of U.S. data centers from 2019 to 2023 as a reference. Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.","The question asks for the 2023 total public health cost based on the average attribution method. The cited passage from Doc [han2024] directly states that the cost was about $6.7â€¯billion, which provides the requested value."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: '... is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024...',"The passage from Doc [ebert2024] explicitly names Senator Edward J. Markey as the introducer of the AI Environmental Impacts Act bill on 1â€¯Febâ€¯2024, directly answering the question."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,4 years,4,years,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'me a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq ...'","The question asks for the estimated average GPU lifetime in AI data centers in 2024. Doc [morrison2025] explicitly states a 4â€‘year lifespan for GPUs, providing the required numeric value and unit."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, which is over 4 times the five cars estimate.",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the â€œfive carsâ€ number.'","The context states that Gemmaâ€™s preâ€‘training emissions were 1247.61 tCO2e and explicitly notes that this is more than four times the ""five cars"" benchmark, so the answer reflects the reported value and its comparison."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",$11.06/hr,11.06,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,Doc [chen2024]: 'Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr',"The question asks for the hourly price of an NVIDIA H100 according to Chen et al. The passage from Doc [chen2024] states that the H100 is priced at $11.06 per hour, which directly answers the question."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 V100 GPUs,2,V100 GPUs,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: Model Size V100 32GB A100 80GB 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128,"The table in Doc [samsi2024] lists the bare minimum hardware for each LLaMA size; for LLaMAâ€‘13B it specifies 2 V100 GPUs (32â€¯GB each), so the minimum number of V100 GPUs required for inference without compression or quantization is two."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the documents mention the training duration of ELMo on 3 NVIDIA GTX 1080â€¯Ti GPUs, so the answer cannot be determined from the provided context."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: 'Research shows that in North America, AWS can lower its customersâ€™ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energyâ€”a goal that Amazon, including AWS, achieved in 2023.'","The document states that moving workloads to AWS in North America can lower customersâ€™ carbon footprints by up to 96%, which directly answers the question."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,SpotLake,SpotLake,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,Doc [erben2023]: SpotLake: Diverse Spot Instance Dataset Archive Service.,"The context cites SpotLake as a dataset archive service for spot instances, implying it was used to shard and stream datasets for spot VMs that can terminate at any time."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: for electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The question asks for the U.S. national average water consumption for electricity generation. Doc [li2025b] explicitly states this value as 3.1 L/kWh, which is used as the answer."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10 PUE,1.1,PUE,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,"Doc [dodge2022]: 'Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021'; Doc [wu2021b]: 'Figure 1: PUE of hyperscalar datacenters, such as Googleâ€™s, has improved from 1.21 (2008) to 1.10 (2021)'","Both documents report that Googleâ€™s hyperscale data centers had a PUE of 1.10 in 2021, so the answer is 1.10 PUE."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents explicitly state that the relationship between runtime and energy consumption is nearly linear, so I cannot answer confidently based on the available context."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,H100 GPUs,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'We conduct training on a cluster containing 12 nodes and 96 H100s.',"The question asks for the total number of H100 GPUs used. Doc [shen2024] explicitly states that the training cluster had 12 nodes and 96 H100 GPUs, so the total is 96."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,3460 $,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'fineâ€‘tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.'; Doc [xia2024#0093]: 'net cost of $3460.',"The question asks for the net cost of fineâ€‘tuning a sparse Mixtral model with 2â€¯million queries on an NVIDIA H100 GPU. Both Doc [xia2024] and Doc [xia2024#0093] explicitly state that this cost is $3460, providing the required numeric value and unit."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50â€“70%,"[50, 70]",%,is_blank,is_blank,"Doc [chung2025#0014]: 'GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50â€“70% of the total provisioned power in the datacenter [52â€“54, 58].'","The question asks for the proportion of total provisioned power that GPUs consume in a typical datacenter. Doc [chung2025#0014] explicitly states that GPUs account for 50â€“70% of that power, so the answer is the 50â€“70% range."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",0.000022 kL,2.18e-05,kL,morrison2025,https://arxiv.org/pdf/2503.05804,Doc [morrison2025]: 'Mining 1 kg of rare earth materials consumes about 11 kL of water ... and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.',"The passage states 11 kL of water per kg of rare earths. An H100 contains 0.1% rare earth by mass. The wafer weight (125 g) divided by 63 H100s gives ~1.984 g of material per GPU. 0.1% of this is 0.001984 g = 1.984Ã—10â»â¶ kg of rare earths. Multiplying by 11 kL/kg yields 2.18Ã—10â»âµ kL of water per GPU, which is 0.000022 kL."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Approximately 41 times greater,40.9,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"Doc [jegham2025#0068]: 'GPTâ€‘4.1 nano remains among the most efficient proprietary models at 0.827 Wh' and Doc [jegham2025]: 'a long, highâ€‘reasoning query reaches an average of 33.8 Wh'","The question asks for the factor by which the o3 modelâ€™s energy consumption exceeds that of GPTâ€‘4.1 nano for a long prompt. GPTâ€‘4.1 nano uses 0.827 Wh, while the longâ€‘prompt energy for the o3 model is reported as 33.8 Wh. Dividing 33.8 by 0.827 gives approximately 40.9, so the o3 model consumes about 41 times more energy than GPTâ€‘4.1 nano for a long prompt."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 pounds",11023,pounds,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'Human life, avg, 1 year 11,023'","The table in Doc [strubell2019] lists the average annual CO2e consumption for a human life as 11,023 pounds globally."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"Doc [kim2025]: 'Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSaveâ€™s top choice.'","The question asks for the percentage difference in cost between Maxâ€‘Performanceâ€™s g6e.xlarge and InferSaveâ€™s top choice for the 400â€¯TPS online workload. Doc [kim2025] explicitly states that the Maxâ€‘Performance instance is about 280% more expensive, which directly answers the question."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a clear numeric value for the total execution time (in seconds) of a sparse Mixtral model with a batch size of 1 fine-tuned on an NVIDIA A40-48â€¯GB GPU.
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25x,1.25,x,samsi2024,https://arxiv.org/pdf/2310.03003,"Doc [samsi2024]: 'particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.'","The passage from Doc [samsi2024] states that for LLaMAâ€‘13B, inference on A100 GPUs is roughly 1.25 times faster than on V100 GPUs, indicating a 1.25Ã— speedup in throughput."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"Doc [morrison2025]: 'When actively training, the average GPU power is over 600W, over 85% of an H100â€™s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.'","The question asks for the average GPU power during active training in the first 300 logging steps of OLMoâ€¯2â€¯7B. Doc [morrison2025] explicitly states that the average GPU power is over 600â€¯W, so that is the reported value."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 Watts,104,Watts,patterson2021,https://arxiv.org/pdf/2104.10350,Doc [patterson2021]: 'Processor Average (Watts) ... TPU v2 221 ... V100 GPU 325',The table in Doc [patterson2021] gives the average system power per processor as 221â€¯W for TPUâ€¯v2 and 325â€¯W for V100 GPU; subtracting 221â€¯W from 325â€¯W yields a difference of 104â€¯Watts.
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,is_blank,is_blank,Doc [jegham2025#0079]: 'A single short GPT-4o query consumes 0.42 Wh (Â±0.13 Wh)',"The question asks for the energy consumption of a single short GPTâ€‘4o query. Doc [jegham2025#0079] explicitly states that such a query consumes 0.42â€¯Wh, and Doc [jegham2025#0077] corroborates this estimate. Therefore the answer is 0.42â€¯Wh."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: AIâ€™s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The 2025 paper (Doc [luccioni2025a]) states that worldwide electronic waste generation reached 62 million tonnes in 2022, which directly answers the question."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters of clean freshwater",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'ning the GPT-3 language model in Microsoftâ€™s state-of-theâ€‘art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.'","The passage from Doc [li2025b] explicitly states that training GPTâ€‘3 in Microsoftâ€™s U.S. data centers directly evaporates 700,000 liters of clean freshwater, which directly answers the question."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",True,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Doc [schwartz2019]: 'In addition, we propose reporting the financial cost or â€œprice tagâ€ of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.'","The question asks whether Green AI includes providing the financial cost of finding, training, and running models. Doc [schwartz2019] explicitly states that reporting the financial cost or ""price tag"" of developing, training, and running models is a key Green AI practice, confirming the statement is true."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,A10G,A10G,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"Doc [griggs2024#0033]: 'smaller sizes are best served on A10G, and larger sizes are best served on A100.'","The question refers to generating only a single classification token, which is a very small request size. Doc [griggs2024#0033] explicitly states that smaller sizes are best served on A10G, indicating that the A10G architecture is the most energyâ€‘efficient for this scenario."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,Doc [jegham2025]: 'OpenAIâ€™s reasoning models dominate the ecoâ€‘efficiency frontier. o3â€‘mini achieved the highest crossâ€‘efficiency score (0.884) ...',"The question asks for the model that ranked highest in a recent ecoâ€‘efficiency analysis using DEA. Doc [jegham2025] states that o3â€‘mini achieved the highest crossâ€‘efficiency score (0.884), making it the top-ranked model in that analysis."
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20Ã—,20,times,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: Facebookâ€™s recommendation model sizes have increased by 20Ã— between 2019 and 2021.,"The question asks for the increase factor of Facebookâ€™s recommendation and ranking model sizes between 2019 and 2021. Doc [wu2021a] explicitly states a 20Ã— increase, so the answer is 20 times."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048 tokens,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"Doc [li2025a]: 'FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.'","The passage from Doc [li2025a] explicitly states that the FLM-101B context window is 2,048 tokens, so the answer is 2048 tokens."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"Doc [wu2021b]: 'current averages of less than 3 years for cell phones [Cordella et al., 2020]'.","The question states that smartphone lifetimes are less than 3 years. Doc [wu2021b] explicitly says that the current averages for cell phones are less than 3 years, confirming the claim."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 â€“ 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 1]. AI represents the fastest expanding workloads in data centers [1, 4]. For example, a recent study suggests that the global AI could consume 85 â€“ 134 TWh of electricity in 2027 [7], whereas a more aggressive projection by the recent U.S. data center energy report predicts that AI serversâ€™ electricity consumption in the U.S. alone will surpass 150 â€“ 300 TWh in 2028 [1].","The question asks for the projected electricity consumption range for global AI in 2027. Doc [li2025b] explicitly states a range of 85â€“134 TWh, which is the answer."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61â€“76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"Doc [cottier2024]: 'Computing hardware makes up 47â€“64%, while energy comprises only 2â€“6%. However, if we exclude equity the fraction for R&D staff drops to 19â€“33%, and the fractions of computing hardware costs and energy rise to 61â€“76% and 2â€“7% respectively.'","The question asks for the percentage range of total amortized cost attributed to computing hardware when equity is excluded. Doc [cottier2024] explicitly states that this range is 61â€“76%, providing the required numeric answer with the unit ""%""."
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Doc [ebert2024]: The AI Act fails to address indirect greenhouse gas emissions from AI applications, such as those used for oil and gas exploration.","The question asks whether the AI Act requires disclosure of GHG emissions from AI applications. Doc [ebert2024] explicitly states that the AI Act fails to address indirect emissions from such applications, indicating no such disclosure requirement."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million packages,150,packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: Europe â€¢ We deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles. â€¢ We delivered 150 million packages via EVs.","The passage from Doc [amazon2023] explicitly states that Amazon delivered 150 million packages via electric vehicles in Europe in 2023, so 150 million is the required value."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"Doc [dodge2022#0081]: 'For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;'","The question asks for the maximum potential CO2 reduction for DenseNet 201 in the West US region using Flexible Start. Doc [dodge2022#0081] states the reduction can be up to 80% in West US, which is the maximum mentioned."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,True,1,is_blank,is_blank,is_blank,"Doc [chung2025#0064]: 'It can be seen that the LLMâ€™s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion modelâ€™s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations per byte of memory loaded is low [37, 58]. This leads to the GPUâ€™s computation throughput being bottlenecked by VRAM bandwidth and results in the GPUâ€™s computation units being ...' 
Doc [chung2025#0192]: 'Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.'","The documents state that LLM decoding has low compute intensity and is VRAMâ€‘bandwidth bound, leading to power consumption well below the GPUâ€™s maximum, whereas diffusion models are computeâ€‘intensive and draw close to maximum GPU power. These statements directly support that LLMs generally have lower power draw during inference than diffusion models, confirming the statement as true."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"Doc [strubell2019]: 'The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO 2 emissions: CO2e = 0.954pt'","The question asks for the EPAâ€™s average CO2 in pounds per kWh. Doc [strubell2019] states the EPA value is 0.954 lbs per kWh, so that is the correct answer."
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,is_blank,is_blank,"Doc [kim2025#0089]: 'time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.'","The document states each request uses 128 input and 512 output tokens, and there are 3000 requests. 128â€¯+â€¯512â€¯=â€¯640 tokens per request; 640â€¯Ã—â€¯3000â€¯=â€¯1,920,000 tokens total."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,is_blank,is_blank,"Doc [khan2025#0046]: 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential tradeâ€‘off between energy efficiency and overall predictive perforâ€‘mance.' Doc [khan2025#0053]: 'Metrics such as F1 score and overall accuracy may decline slightly postâ€‘optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.'","The documents state that after optimization, accuracy and F1 score can decline rather than always improve, so the claim is false."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244 projects,244,projects,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"Doc [amazon2023]: United States 244 17,706","The table in Doc [amazon2023] lists the number of Amazon Renewable Energy Projects by country, showing 244 projects announced in the United States as of Januaryâ€¯2024."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the percentage of power usage covered by renewable energy for AWS in 2018, so I cannot answer this question based on the available context."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any statements about Yelp sentiment analysis benchmarks or a comparison of traditional models to large language models, so the truth value cannot be determined from the context."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the amount of fiber optic cable installed globally to support AI workloads in 2023, so the answer cannot be determined from the given context."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",over 1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: 'the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.'  Doc [luccioni2024]: 'mean consumption of 0.002 kWh per 1,000 inferences' for text classification and 'mean consumption of 2.9 kWh' for image generation.","The document states the energy consumption for image generation (2.9â€¯kWh) versus text classification (0.002â€¯kWh), yielding a ratio of 2.9/0.002â€¯â‰ˆâ€¯1450. It explicitly says the factor is over 1450, so the answer is that image generation requires more than 1450 times the energy of text classification."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2 NVIDIA A100-80GB GPUs,2,GPUs,is_blank,is_blank,"Doc [griggs2024#0005]: 'For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.'","The question asks how many NVIDIA A100-80GB GPUs are needed to serve Llama2-70b at BF16 precision. Doc [griggs2024#0005] explicitly states that 2 such GPUs are required, so the answer is 2 GPUs."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5% operational energy footprint reduction,28.5,%,wu2021a,https://arxiv.org/pdf/2111.00364,Doc [wu2021a]: 'Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B)'.,"The question asks for the total operational energy footprint reduction achieved over the twoâ€‘year period. Doc [wu2021a] explicitly states that the iterative optimization process led to a 28.5% reduction, which is the figure the question seeks."
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,Doc [dodge2022]: The GPU alone accounts for 74% of the total energy consumption.,"The question asks for the percentage of total electricity consumption attributed to the GPU. Doc [dodge2022] explicitly states that the GPU accounts for 74% of the total energy consumption, so the answer is 74%."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48% increase,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"Doc [luccioni2025a]: 'For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to â€œincreases in data center energy consumptionâ€'","The question asks for the percentage increase in GHG emissions reported by Google in its 2024 environmental report. Doc [luccioni2025a] explicitly states a 48% increase, providing the required numeric value and unit."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: The umbrella term â€˜Sustainable AIâ€™ was initially proposed by van Wynsberghe as a ï¬eld of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The passage from Doc [luccioni2025b] clearly states that the term was proposed to include both using AI in climateâ€‘positive applications and improving the sustainability of AI itself, so it was not limited to only climateâ€‘positive uses."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping,Swapping,is_blank,is_blank,is_blank,Doc [chung2025#0177]: 'Swapping consistently consumes less energy.',"The question asks which preemption mechanism consumes less energy when the server is overloaded. The cited passage explicitly states that Swapping consistently consumes less energy compared to Recomputation, so the answer is Swapping."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",26.1%,26.1,%,is_blank,is_blank,Doc [amazon2023#0963]: '16.6% 31.9% 26.1% 23.5%' â€“ the 2023 percentage for Women among People Managers is 26.1%.,"The question asks for the 2023 percentage of Amazonâ€™s People Managers who identified as women. In Doc [amazon2023#0963] a table lists the Women percentage for 2023 as 26.1%, which directly answers the question."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: 'Energy consumption should be reported at the cumulative server level (see also [4]).' Doc [ebert2024#0188]: 'Cumulative server energy reporting: Require energy consumption to be measured and reported at the cumulative server level.',"The question asks for the measurement level recommended for reporting AI energy consumption. The cited passages from Doc [ebert2024] and its subâ€‘documents explicitly state that energy should be reported at the cumulative server level, which balances accuracy with feasibility."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, MÃ©lange achieved cost reductions in what percentage range compared to single-GPU baselines?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800M,800,USD,is_blank,is_blank,"Doc [cottier2024#0047]: 'we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.'","The question asks for the estimated upfront hardware acquisition cost for GPTâ€‘4. Doc [cottier2024#0047] explicitly states that the acquisition cost is $800M, so this value is used as the answer."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided documents give a McKinsey projection for data center electricity consumption in 2030.
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28 samples,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Doc [xia2024]: 20GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The question asks for the ground truth maximum batch size for fineâ€‘tuning Mixtral on an NVIDIA A100â€‘40GB GPU. The cited passage from Doc [xia2024] states that for the 40GB A100 the predicted (and experimentally validated) batch size is 28 samples, while the 80GB A100 supports 35. Thus the answer is 28 samples."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'reducing inference computation by about 70% compared to Llama2-7B.',"The question asks for the approximate percentage reduction in inference computation. Doc [shen2024] explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, so the answer is 70%."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",Approximately 25%,25,%,is_blank,is_blank,Doc [dodge2022#0092]: 'doubling the duration can lead to significant savings up to about 25%.',"The document explains that for the 6B parameter transformer, using Pause and Resume and allowing the job duration to double can yield a maximum emissions saving of about 25%."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,year,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Doc [luccioni2025c]: 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.'","The question asks for the year when direct environmental disclosures for notable models were highest. The cited passage from Doc [luccioni2025c] explicitly states that 2022 was the peak year, so the answer is 2022."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,is_blank,is_blank,"Doc [li2025b#0046]: 'GPT-3 was trained and deployed by OpenAI in Microsoftâ€™s data centers, with an estimated training energy of 1287 MWh [29].' 
Doc [jegham2025#0005]: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity...'","Both documents explicitly state that the full GPTâ€‘3 model required approximately 1,287 MWh of training energy, so the answer is 1287 MWh."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.',"The context explicitly states that the MoE layer is the costliest and a prime target for optimization in LLM fine-tuning, supporting the claim that it is often a targeted layer for performance enhancement."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 gCO2eq,0.32,gCO2eq,luccioni2024,https://arxiv.org/pdf/2311.16863,"Doc [luccioni2024]: . The difference is much more drastic if comparing BERT-based models for tasks such as text classification with the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of ð¶ð‘‚2ð‘’ð‘žper 1,000 queries","The question asks for the CO2eq emissions per 1,000 text classification queries for bert-base-multilingual-uncased-sentiment. Doc [luccioni2024] explicitly states the emission as 0.32â€¯g per 1,000 queries, so the answer is 0.32â€¯gCO2eq."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided documents mention the freshwater consumption of Google's DeepMind AlphaFold servers in 2023, so I cannot answer the question based on the available context."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",7-26 thousand grams,"[7, 26]",thousand grams,is_blank,is_blank,"Doc [dodge2022#0062]: can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The question asks for the approximate range of CO2 emissions between the most and least efficient regions. Doc [dodge2022#0062] gives the emissions as 7k grams for the most efficient region and 26k grams for the least efficient region, yielding a range of 7â€“26 thousand grams."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,False,0,is_blank,is_blank,is_blank,"Doc [erben2023#0073]: 'Figure 7 shows the result of the intra-zone experiments, which we used as a baseline to compare geoâ€‘distributed deployments to.'","The context does not report that intraâ€‘zone scaling with T4 GPUs achieved nearly linear perâ€‘GPU speedup for CV models. The only explicit mention of nearly linear perâ€‘GPU speedup is for transatlantic scaling in Doc [erben2023#0076], so the statement about intraâ€‘zone scaling is unsupported and thus false."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,is_blank,is_blank,Doc [xia2024#0100]: 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.',"The question claims that adding compute resources to accelerate MoE layers can increase costs. The cited passage explicitly states that adding compute resources reduces cost, contradicting the claim, so the correct answer is FALSE."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MkWh,40,MkWh,li2025a,https://arxiv.org/pdf/2309.03852,Doc [li2025a]: 'Energy (MkWh) 1171 1066 3179 444 688 40',"The carbon footprint analysis table in Doc [li2025a] lists the energy consumption for each model; for FLM-101B the value is 40â€¯MkWh, which is the total energy used for training."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",6205 days,6205,days,is_blank,is_blank,"Doc [morrison2025#0057]: 'Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years.'","The passage states the water usage for the small models (including the 60M model) is about 17 years of water use for one U.S. person. Converting 17 years to days (17 Ã— 365) gives 6205 days, which is the equivalent water usage in days."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,$32.7,32.7,$,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: 'TABLE IV ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7',"The question asks for the estimated total cost of fineâ€‘tuning a Mixtral model on GSM8K using sparse MoE on an NVIDIA A40â€‘48GB GPU. Doc [xia2024] provides Tableâ€¯IV, showing the total cost column for A40 as 32.7â€¯USD, which directly answers the question."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?",626155 lbs CO2e,626155,lbs CO2e,strubell2019;luccioni2025b,https://arxiv.org/pdf/1906.02243;https://arxiv.org/pdf/2504.00797,"Doc [luccioni2025b]: ... as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The question asks for the BERT training emissions in pounds CO2e from the 2019 Strubell et al. study. The Context provides the exact figure of 626,155 pounds in Doc [luccioni2025b] (which cites the Strubell study) and also in Doc [strubell2019] where the same number appears. Therefore the answer is 626,155 lbs CO2e."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",TRUE,1,is_blank,is_blank,is_blank,"Doc [erben2023#0093]: 'rity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.'","The documents state that for highâ€‘granularity CV models, intercontinental training slows performance by only 7% compared to local training, so the statement is true."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 GPU,1,GPU,samsi2024,https://arxiv.org/pdf/2310.03003,Doc [samsi2024]: 'Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64',"The question asks for the minimum number of V100 32GB GPUs needed to run LLaMAâ€‘7B inference. Doc [samsi2024] lists the bareâ€‘minimum hardware requirements and shows that a single V100 (Count = 1) suffices for the 7B model, so the answer is 1 GPU."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain a specific numeric value for the total execution time of a sparse Mixtral model fine-tuned on an NVIDIA A40-48GB with a batch size of 10, so I cannot answer confidently."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,Doc [ebert2024]: We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements. Energy consumption should be reported at the cumulative server level.,"The statement claims GPU-level monitoring is recommended, but Doc [ebert2024] explicitly states that GPU-level tracking is discouraged and that cumulative server-level reporting is preferred, so the correct answer is False."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided documents do not contain a specific figure for the hectares of land occupied by new AI data centers globally in 2022.
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,Doc [luccioni2025b]: ... training accounted for only half of the modelâ€™s overall emissions [121],"The 2023 study cited in Doc [luccioni2025b] states that training made up half of the BLOOM modelâ€™s total emissions, i.e., 50%."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,Doc [shen2024]: 'GSM8k 14.5 17.3 16.9 27.8',The GSM8k column in TableÂ 3 lists scores for four models; the last value corresponds to JetMoEâ€‘8B and is 27.8.
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10â€“50 requests,"[10, 50]",requests,li2025b,https://arxiv.org/pdf/2304.03271,"Doc [li2025b]: 'GPT-3 needs to â€œdrinkâ€ (i.e., consume) a 500ml bottle of water for roughly 10 â€“ 50 medium-length responses, depending on when and where it is deployed.'","The question asks how many user requests correspond to consuming a 500â€¯ml bottle of water during GPTâ€‘3 training. Doc [li2025b] states that a 500â€¯ml bottle is used for roughly 10â€“50 mediumâ€‘length responses, which are equivalent to user requests. Thus the answer is a range of 10 to 50 requests."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,is_blank,is_blank,"Doc [dodge2022#0065]: 'one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)'","The question asks for the CO2 emission in metric tons for one year of average US home energy use. Both Doc [dodge2022#0065] and Doc [dodge2022#0069] state that this value is 8.3 metric tons, so the answer is 8.3 metric tons."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",$20 billion,20,billion dollars,han2024,https://arxiv.org/pdf/2412.06288,"Doc [han2024#0002]: 'â€¦ could push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028, rivaling that of on-road emissions of California.'","The question asks for the projected value of the public health burden. Doc [han2024#0002] states the burden could reach more than $20â€¯billion (in 2028), and Doc [han2024] repeats the same figure, so the answer is $20â€¯billion."
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",30 samples,30,samples,is_blank,is_blank,Doc [xia2024#0039]: 'ense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)',"The figure and accompanying text in Doc [xia2024#0039] report that the execution time breakdown for a sparse Mixtral model on an NVIDIA A40-48â€¯GB GPU was measured at batch size 30, where the MoE layer was the longestâ€‘running component of the pipeline. Thus the batch size of the longestâ€‘running MoE layer is 30 samples."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2 samples,2,samples,xia2024,https://arxiv.org/pdf/2408.04693,Doc [xia2024]: TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20,"Table III in Doc [xia2024] reports the maximum batch size for Mixtral dense on the Hellaswag dataset (denoted CS) when fineâ€‘tuning on an NVIDIA A40 GPU with 48â€¯GB memory, showing a value of 2 samples."
