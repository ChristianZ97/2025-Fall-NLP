id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2t CO2e,1.2,t,patterson2021,https://arxiv.org/pdf/2104.10350,"""single passenger round trip SF-NY is ~1.2t CO 2 e (Table 2)""","The passage explicitly states that a single passenger round trip from San Francisco to New York emits approximately 1.2 tons of CO₂e, which is the required value."
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,4%,4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""total energy consumption of the US data centers increased by about 4% from 2010-2014""","The provided passage states that U.S. data center energy consumption rose by approximately 4% between 2010 and 2014, which is the requested average increase."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,is_blank,is_blank,"""3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU."" (Doc morrison2025#0049)","The embedded carbon emissions are taken from the Luccioni et al. estimate of 3700 kg CO₂eq for an 8‑GPU server node. Dividing 3700 kg by 8 GPUs gives 463 kg per GPU, which is directly stated in the cited context."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72%,72,%,li2025a,https://arxiv.org/pdf/2309.03852,"Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The document explicitly states that using the growth strategy saved 72% of the training time for the 101B model compared to a from-scratch approach, so the percentage saved is 72%."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,24 data centers,24,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The quoted passage directly states that in 2023 AWS began using recycled water for cooling in 24 data centers, providing the numeric answer."
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,0.036 kWh,0.036,kWh,is_blank,is_blank,"""Llama 3.2 1B 8 0.036 12.0 0.054 12.64 21.5 bil.""","The table in the SGLang benchmarking results lists the GPU Power Usage for Llama 3.2 1B at an 8 request/s frequency as 0.036 kWh, which directly answers the question."
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context gives total permitted NOx limits for all data center backup generators but does not specify a separate figure for northern Virginia alone. Therefore the documents do not contain sufficient information to answer the question with confidence.
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","13,900 cars",13900,cars,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.""","The report states that the avoided emissions are equivalent to taking more than 13,900 cars off the road, so the estimated car‑equivalent is 13,900 cars."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,6750,6750,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""Int… 4004 – 740kHZ"" and ""typical microprocessor … 5,000,000kHz"" and ""more than 6,750 fold improvement in processor clock speed""","The document compares the Intel 4004’s 740 kHz clock to a typical 2021 processor’s 5,000,000 kHz clock, explicitly stating a more than 6,750‑fold improvement, which provides the fold increase required."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",14.8 days,14.8,days,patterson2021;luccioni2023,https://arxiv.org/pdf/2104.10350;https://arxiv.org/pdf/2302.08476,"""It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS."" – Patterson et al.; ""over 3.5 million hours (14.8 days with 10,000 GPUs)"" – Luccioni et al.; Table in Patterson et al. shows Training time (days) 14.8 for 10,000 V100 GPUs.","Both Patterson et al. and Luccioni et al. state that training GPT‑3 on 10,000 V100 GPUs at 24.6 TFLOPS/sec requires about 14.8 days, which matches the table entry in Patterson’s document. Therefore the training duration is 14.8 days."
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give premature death estimates for 2028 (approximately 1,300 deaths) but do not include any data or projections for 2030, so the answer cannot be determined with confidence from these sources."
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,400,400,is_blank,li2025a,https://arxiv.org/pdf/2309.03852,"""budget of $100K"" (doc li2025a) and ""largest amortized hardware and energy cost, at $40M"" (doc cottier2024#0044).","The amortized training cost of GPT‑4 is $40 million, while the total training budget for FLM‑101B was $100 thousand. Dividing $40 million by $100 thousand yields a factor of 400."
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88,88,is_blank,is_blank,is_blank,"""We sampled 88 models""","Both the 2024 Power Hungry Processing study documents state that 88 models were sampled and analyzed, confirming the count."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.94,43.94,is_blank,is_blank,is_blank,"""On average, FLM-101B achieves a score of 43.94,""","The context explicitly states that the final average performance score of FLM-101B on the Open LLM Leaderboard is 43.94, which is the value used in the answer."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation...""","The supporting quote explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, making 2014 the correct year of launch."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts,8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"""set the same number of experts to 8 and top-k to 2 for every layer."" (Table 1: Nexperts 8)","The JetMoE-8B hyperparameters specify that each MoE layer contains 8 experts, as shown in the text and in Table 1 of the JetMoE‑8B paper."
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22%,22,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling""","The quoted sentence from Doc [luccioni2025a] explicitly states that 22% of e‑waste is formally collected and recycled, answering the question directly."
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","Command‑R Plus (≈3,426 Wh)",3426,Wh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command‑R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).""","The appendix lists a range of GPU energy consumptions for 1,000 queries, with the maximum value reported as 3,426 Wh for the Command‑R Plus model. This is the highest value among the models listed."
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,The document explicitly gives the English portion of FLM‑101B’s training cost as 28.22 zettaFLOPs.
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,is_blank,is_blank,"""This model was not trained to completion, but only until 13%; a full training run would take 60 days.""","The cited passage from Dodge et al. (2022) explicitly states that a full training run of the 6.1 billion‑parameter model would take 60 days, providing the numeric answer."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.25%,0.25,%,is_blank,is_blank,Percent of model activated on every token 100% 100% 100% 0.25% 0.10% 100%,"The table in Doc patterson2021#0065 lists the Switch Transformer with 1500 B parameters and reports that only 0.25 % of the model’s parameters are activated per token, which directly answers the question."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU. The maximum supported batch size for dense BlackMamba on the A40 is 6 (CS dataset) or 2 (MATH dataset), so batch size 30 is not supported and no execution time is reported for that configuration."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,li2025a,https://arxiv.org/pdf/2309.03852,"""Under this growth schedule, the total time cost for training FLM-101B is 21.54 days""","The document explicitly states that the total wall‑clock training time for FLM‑101B using the growth strategy is 21.54 days, which directly answers the question."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.""",The paper explicitly cites Jevons’ Paradox as the economic principle explaining why efficiency gains could lead to higher overall consumption and thus negate environmental benefits.
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",FALSE,0,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%"",""A significant portion of machine learning model experimentation utilizes GPUs at only 30-50%""","The provided document states that most experimentation workflows use GPUs at only 30‑50% capacity, which is far below the 80% threshold. Hence the claim that a majority use GPUs at over 80% capacity is false."
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 – 6.6 billion cubic meters,"[4.2, 6.6]",billion cubic meters,li2025b,https://arxiv.org/pdf/2304.03271,"""global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027""","The cited passage from doc li2025b explicitly states the projected water withdrawal range for global AI demand in 2027 as 4.2 to 6.6 billion cubic meters, which directly answers the question."
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets."" 
""These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models 22."",","The provided passages explicitly name the collaborative project as ""AI Energy Score"" and state that it is designed to offer a standardized method for comparing inference efficiency among AI models."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).""","The cited passage states that Red AI is on the rise, contradicting the claim that it is on the decline."
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,Approximately 104 MWh,104,MWh,is_blank,is_blank,"""We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/… )"" and ""We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.""","The cited passages state that 13.8 MWh were used for 13 % of the training, implying a full run of about 103–106 MWh. Both documents provide the same estimate, supporting the answer."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Intel Xeon Platinum 8358,Intel Xeon Platinum 8358,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Intel Xeon Platinum 8358 503 240 A100 80 300""","The experimental setup table lists the CPU used for inference evaluations as Intel Xeon Platinum 8358, indicating this processor was reported for the energy‑efficient local inference experiments."
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,1.2x to 4x,"[1.2, 4]",is_blank,is_blank,is_blank,"""a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.""","The cited passage states that the ratio of total compute for model development to the final training run ranges from 1.2× to 4×, providing the required multiplier range."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,3×,3,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.",The cited passage from Wu et al. states that raising GPU utilization to 80 % results in a three‑fold reduction in the overall carbon footprint for LM training.
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,6.4%,6.4,%,wu2021b,https://arxiv.org/pdf/2108.06738,"""in addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].""","The cited passage from Wu 2021b explicitly states that global carbon emissions fell by 6.4% in 2020, attributing part of this decline to reduced vehicle transportation during the COVID‑19 pandemic."
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",22,22,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy""; ""22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.""","Both passages state that in 2023, 22 AWS data center regions had their electricity consumption matched 100% with renewable energy. Thus the answer is 22 regions."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,"1,287 MWh",1287,MWh,jegham2025;li2025b;patterson2021,https://arxiv.org/pdf/2505.09598;https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2104.10350,"""Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity"" (Doc [jegham2025])","All three cited documents report that GPT‑3’s training energy consumption is 1,287 MWh, which directly answers the question."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for the execution time of the longest kernel in the MoE layer for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU. Therefore, the answer cannot be determined from the documents."
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts,2,is_blank,shen2024;xia2024,https://arxiv.org/pdf/2404.07413;https://arxiv.org/pdf/2408.04693,"""set the same number of experts to 8 and top-k to 2 for every layer.""\n""Only the top two experts are selected for each token.""","The JetMoE‑8B hyperparameters specify Nexperts = 8 and Top‑k = 2, and the text explicitly states that only the top two experts are activated for each token in every layer."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The provided document reports a 300,000‑fold increase in compute over six years (2012‑2018), not the 200,000‑fold increase stated. Therefore the claim is false."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide a 44% energy reduction at 77 ms TPOT, but do not give a specific percentage for a 100 ms TPOT target. Therefore the requested value cannot be determined with confidence from the provided context."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW,1.3,GW,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"In 2023, we added seven solar projects paired with battery energy storage systems to our portfolio in the U.S. We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The cited passage explicitly states that Amazon’s energy storage capacity in 2023 was 1.3 gigawatts, providing direct evidence for the answer."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context and metadata do not contain any information about the percentage of AI inference workloads in Asia powered by coal in 2023, so the answer cannot be determined with confidence."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS),Neural Architecture Search,is_blank,luccioni2025c;luccioni2023,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2302.08476,"""training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 626,155 lbs (626,155 pounds) of CO₂, similar to the lifetime emissions of five US cars"" (Luccioni et al., 2025)","The 2019 study’s five‑cars estimate comes from the Strubell et al. analysis of training a large Transformer model using Neural Architecture Search (NAS), a process that is rarely performed in typical AI training. The quoted passage directly states that NAS was the basis for the 626,155‑lb figure underpinning the five‑cars carbon footprint claim."
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,approximately 60 years,60,years,stone2022,https://arxiv.org/pdf/2211.06318,"""since the field’s inception sixty years ago"" (Doc [stone2022#0021]) and ""the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop"" (Doc [stone2022]).","Both passages indicate that the field began around 1955‑1956, making its age about 60 years in 2025."
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,"2,300 transatlantic flights",2300,flights,jegham2025,https://arxiv.org/pdf/2505.09598,"""These figures are comparable to the annual emissions of 30,000 gasoline‑powered cars or the cumulative emissions of 2,300 transatlantic flights between Boston and London.""","The document states that GPT‑4o’s annual carbon emissions are comparable to 2,300 transatlantic flights, directly supporting the answer."
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.""","The provided passage explicitly states that the field was officially christened in 1956 at the Dartmouth workshop, which is the answer to the question."
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,8,8,samples,xia2024,https://arxiv.org/pdf/2408.04693,"Table III: ""MATH 1 3 2 8"" – indicating a maximum batch size of 8 for BlackMamba‑S on the GSM8K (math) dataset.","The table lists the maximum batch sizes for each model/dataset combination. For the GSM8K dataset (referred to as MATH in the table), BlackMamba in sparse mode (BlackMamba‑S) has a maximum batch size of 8 samples on the NVIDIA A40 GPU with 48 GB memory."
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,is_blank,is_blank,"""Llama 7B 63 Meta 356 14"" from Table 1 of luccioni2025c#0096",The table lists Llama 7B with a reported GHG emission of 14 tCO2e for its pre‑training phase.
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context includes specific WUE figures for two Google clusters (Jupiter 1.29 L/kWh and Augusta 3.1 L/kWh) and an on‑site efficiency of 1 L/kWh, but does not give a single average WUE value for all Google AI‑dedicated data centers in 2024. Therefore the documents do not provide sufficient information to answer the question confidently."
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,3-4 Joules,"[3, 4]",J,samsi2024,https://arxiv.org/pdf/2310.03003,"""with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.""","The context explicitly states that at a maximum generation length of 512 tokens the LLaMA‑65B consumes roughly 3–4 Joules per output token, which directly answers the question."
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,TRUE,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity""","The quoted passage from Doc wu2021b directly states that roughly 770 million people worldwide lack a stable electricity supply, confirming the statement as true."
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B parameters,2,B,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.",The context explicitly states that 2B of JetMoE-8B’s 8B total parameters are activated per input token during inference.
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,15%,15,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%""","The quoted sentence explicitly states that quantizing RM2 from 32-bit to 16-bit reduces its model size by 15%, which is the requested percentage."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,False,0,is_blank,morrison2025,https://arxiv.org/pdf/2503.05804,"""sions arising from energy sources used to power model training and deployment, including servers and data center cooling. We base our analysis of operational emissions around the following equation introduced by Schwartz et al. (2020) to describe the amount of computation required to produce a machine learning artifact, such as an LLM:"" 

""Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training a""","The passage explicitly states that operational environmental impacts cover GHG emissions from servers and data center cooling, contradicting the claim that they do not include such emissions. Therefore the statement is false."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",TRUE,1,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy""","The quoted sentence from the Patterson 2021 document explicitly states that sparsely activated DNNs use less than one-tenth the energy of large dense DNNs while maintaining accuracy, confirming the claim."
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context passages contain a specific numeric value for the energy consumption in MWh for pre‑training the BLOOM model. The available excerpts give general ranges for LLM pre‑training or detailed figures for other models, but no explicit figure for BLOOM is present. Therefore the answer cannot be given with confidence from the supplied documents."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,The reasoning behind the 5‑10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience.,"The context states that the 5‑10% figure is widely cited but the calculations are not publicly detailed or scientifically grounded, supporting a false claim that it is supported by clear, publicly available calculations."
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.1,1.1,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""we conservatively assume a power usage effec‑tiveness (PUE) of 1.1, which is a fairly low value even for state‑of‑the‑art data center facilities [4].""","The Li 2025b document states that for AI‑dedicated data centers the assumed PUE in 2023 is 1.1, providing the requested global average PUE for these centers."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear, explicit statement of the energy consumption (in Wh) for the o3 model when processing a long prompt. Therefore, the answer cannot be determined with confidence from the documents alone."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.""","The passage from wu2021a explicitly states that manufacturing (embodied) carbon accounts for 74% of a client device’s total carbon footprint, providing the required percentage."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any specific information about the metric tons of CO₂ emitted by OpenAI’s API requests in January 2024, so the answer cannot be determined from the given context."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,17 members,17,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry , and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015."",","The context explicitly states that the inaugural 2015 Study Panel consisted of seventeen members, which is the required count."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].""","The quoted sentence from Doc ebert2024 explicitly states that the average global PUE in 2023 was 1.58, providing the required numeric value."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","$25,000",25000,USD,is_blank,is_blank,"""Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.""","The context explicitly states that Grover’s training on 256 TPU chips over two weeks incurred an estimated cost of $25,000, which matches the requested figure."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,FALSE,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.""","The quoted passage from the 100‑Year Study on AI explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, directly contradicting the claim that the panel is concerned about an imminent threat."
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,53%,53,%,xia2024,https://arxiv.org/pdf/2408.04693,"""The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)""",The cited sentence directly states that the optimizer stage accounts for 53% of the running time for BlackMamba sparse fine-tuning on an NVIDIA A40‑48GB GPU with batch size = 1.
q066," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"Table 3 shows Flan‑T5‑xxl has an average energy usage of 0.083 kWh for 1,000 inferences.","The per‑query energy is 0.083 kWh/1,000 = 8.3×10⁻⁵ kWh. For 1 billion queries: 1 ×10⁹ × 8.3×10⁻⁵ = 8.3×10⁴ kWh, which is 83 MWh."
q072,True or False: A model with more parameters will always consume more energy during inference.,FALSE,0,is_blank,is_blank,is_blank,"Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi‑3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.","The cited passage explicitly states that more parameters do not always lead to higher inference energy, providing a counterexample where a larger model consumes less energy. Thus the claim that a model with more parameters will always consume more energy during inference is false."
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",49%,49,%,cottier2024,https://arxiv.org/pdf/2405.21015,"""Gemini Ultra has the highest fraction of R&D staff cost at 49%""","The cited passage directly states that Gemini Ultra’s R&D staff cost (including equity) constitutes 49% of its total model development cost, giving the required percentage."
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","11,390 tCO2e, which is over 40 times the ‘five cars’ estimate.",11390,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the ‘five cars’ estimate.","The supporting quote states the exact emissions figure and the comparison to the five‑cars benchmark, confirming that 11,390 tCO2e is more than 40 times larger."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context does not contain any statement or figure giving the distance from Earth to the Sun in miles.
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","1,594 grams",1594,grams,luccioni2024,https://arxiv.org/pdf/2311.16863,"""most carbon-intensive image generation model (stable‑diffusion‑xl‑base‑1.0) generates 1,594 grams of CO2eq for 1,000 inferences""","The context explicitly states that stable‑diffusion‑xl‑base‑1.0 emits 1,594 g CO2eq per 1,000 inferences, which is the requested value."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,continuous batching,continuous batching,is_blank,is_blank,is_blank,"""Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time.""",The cited passages explicitly describe continuous batching as a strategy that replaces finished requests with new ones to reduce idle GPU time and improve utilization. Hence the name of the strategy is continuous batching.
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",2.9×,2.9,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.,"The document states that the explosive growth in AI use cases drove a 2.9‑fold increase in AI training infrastructure capacity between Yr1‑Q1 and Yr2‑Q2 (2019‑2021), which directly answers the factor requested."
q080,True or False: The AlphaGo program defeated the human Go champion.,TRUE,1,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"""the recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match""","The quotation from the Stone 2022 passage explicitly states that AlphaGo beat the human Go champion, confirming the statement as true."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,0.429 kg CO2e per kWh,0.429,kg CO2e/KWh,patterson2021,https://arxiv.org/pdf/2104.10350,The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].,"The context from document patterson2021 explicitly states that the gross carbon intensity of energy for the U.S. average mix in 2021 is 0.429 kg CO2e per kWh, providing the required numeric value and unit."
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,is_blank,is_blank,"""Hivemind [39] is a PyTorch-based [32] framework..."" and ""We used the Hivemind framework for all of our experiments.""","The first quote identifies Hivemind as a PyTorch-based decentralized framework, and the second quote confirms it was the framework used for the distributed spot instance training across clouds and continents."
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"""The entire alignment process takes 60 H100 GPU hours.""","The document explicitly states that the full alignment procedure, which includes both dSFT and dDPO fine‑tuning, requires 60 H100 GPU hours. This directly answers the question."
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided passages contains any statement giving the number of wind turbines that Microsoft directly contracted to power Azure AI clusters in 2023. Therefore the answer cannot be determined from the supplied documents.
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist) – but by adopting a multitude of endeavors such as the ones described in the paragraphs above can help involve different actors and""","The cited passage explicitly states that researchers do not believe a universal, one‑size‑fits‑all approach to AI ethics and sustainability can be developed, contradicting the proposition that they do. Hence the correct answer is FALSE."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","0.06 Wh to 3,426 Wh","[0.06, 3426]",Wh,is_blank,is_blank,"""GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus)""","The appendix of the 2025 study lists the lowest and highest GPU energy consumption for 1,000 inference queries as 0.06 Wh and 3,426 Wh respectively, establishing the stated range."
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Carbon Intensity gCO2/kWh – CO₂ emissions per unit of electricity consumed""",The metric described in the framework is explicitly named “Carbon Intensity” in the table from Doc [khan2025].
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,The provided context passages do not contain information about classification experiments on German public administration texts or the accuracy of models using sentence embeddings for that task.
q093,How many parameters does the largest T5 model have?,11B,11,B,patterson2021,https://arxiv.org/pdf/2104.10350,"The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO2e.","The passage explicitly states that the largest T5 variant contains 11 billion parameters, which is the numeric answer requested."
q094,What is the total number of parameters in the JetMoE-8B model?,8B,8,B,shen2024,https://arxiv.org/pdf/2404.07413,JetMoE-8B has 8B parameters while only activating 2B for each input token,"The cited passage states that JetMoE‑8B has 8B total parameters, which directly answers the question about its total parameter count."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,%,li2025b;luccioni2025a,https://arxiv.org/pdf/2304.03271;https://arxiv.org/pdf/2501.16548,"""the company’s data center water consumption increased by ∼20% from 2021 to 2022"" (li2025b) | ""Google observed a 20% uptick in the same period"" (luccioni2025a)",Both documents report that Google’s data center water consumption rose by approximately 20% from 2021 to 2022.
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",810×,810,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer‑based universal translation model by 810×.""","The quoted passage directly states that full‑stack optimization reduces the operational carbon footprint by a factor of 810 compared to a CPU server baseline, which is the required factor in the answer."
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems""","The passage defines ""social transparency"" as the expanded form of AI transparency that includes socio-technical aspects and the societal/environmental footprint, directly answering the question."
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",52.88%,52.88,%,li2025a,https://arxiv.org/pdf/2309.03852,"""101 4 4 12 192 2160 165 52.88%""","The table for the 101B growth stage lists a FLOPs utilization of 52.88%, which is the final stage of FLM-101B training."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,is_blank,is_blank,"""We develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster."" (Doc [chen2024#0003])","The excerpts from the 2024 Chen et al. documents explicitly name the system as Lamina and state that it uses model-attention disaggregation, which directly answers the question."
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,0.64,0.64,is_blank,is_blank,is_blank,"""for the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs"" (Doc erben2023#0088).  ""slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4)"" (Doc erben2023#0091).","The documents state that NLP throughput when distributed across four continents (C‑4) is 36% slower than the local (A‑4) experiment.  A 36% slowdown corresponds to 64% (1‑0.36) of the local throughput, giving a fraction of 0.64."
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,3.5B liters,3500000000,liters,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""3.5B Liters of water returned to communities from replenishment projects in 2023""","The provided Amazon 2023 report states that 3.5 billion liters of water were returned to communities from replenishment projects in 2023, which is the value requested."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3.7,GPUs,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023)""","The 2025 paper states that NVIDIA shipped 3.7 million data‑center GPUs in 2024, directly answering the question."
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,$40M,40,M,cottier2024,https://arxiv.org/pdf/2405.21015,"""Currently, GPT‑4 has the largest amortized hardware and energy cost, at $40M.""","The cited passage from the Cottier 2024 study explicitly states that the amortized hardware and energy cost for GPT‑4 is $40 million, which directly answers the question about its estimated amortized training costs."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",44%,44,percent,is_blank,is_blank,"""Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.""","The quoted passage from the cited documents explicitly states that, on average, 44% of the total amortized hardware and energy cost is attributed to AI accelerator chips."
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",TRUE,1,is_blank,rubei2025,https://arxiv.org/pdf/2501.05899,Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.,"The cited passage from rubei2025 explicitly states that custom tags reduce energy consumption for zero‑shot, one‑shot, and few‑shot prompt engineering techniques in source code completion tasks, confirming the statement is true."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.10,1.1,is_blank,wu2021a;wu2021b,https://arxiv.org/pdf/2111.00364;https://arxiv.org/pdf/2108.06738,"""Achieving a Power Usage Effectiveness (PUE) of about 1.10,"" (wu2021a) and ""the PUE of Facebook datacenters is 1.10 (2020)"" (wu2021b).","Both documents explicitly state that Facebook’s data centers have a Power Usage Effectiveness of approximately 1.10, indicating the ratio of total energy to IT energy consumption."
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 lbs",36156,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Human life, avg, 1 year 11,023  American life, avg, 1 year 36,156","The Strubell et al. table lists the estimated CO2e emissions for an average American life over one year as 36,156 pounds of CO2e, directly answering the question."
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 books,115,books,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""a life cycle assessment (LCA)... has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].""","The context explicitly states that 115 physical print books produce the same CO₂ emissions as one Kindle, providing the required numeric answer."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter‑disciplinary governance of AI systems""","The context explicitly states that the Finnish project’s acronym is ETAIROS, confirming it as the answer."
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"None of the provided context documents mention the energy consumption of the DS Llama 70B model on the FKTG dataset, so the answer cannot be determined from the available information."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",200 times,200,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""per-household health burden could be 200x more than that in less-impacted communities."" (Doc han2024#0003)","Both documents state that disadvantaged communities could experience a per‑household health burden up to 200 times higher than in less‑impacted areas, so the factor is 200."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,9 µg/m³,9,µg/m³,han2024,https://arxiv.org/pdf/2412.06288,"""EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3"" and ""NAAQS primary standards set the annual average PM2.5 concentration at 9µg/m 3""",The cited passages from the Han 2024 document state that the EPA’s tightened primary standard for PM2.5 is an annual average limit of 9 µg/m³.
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",2.907 kWh,2.907,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,image generation 2.907 3.31,"Table 2 of the 2024 study lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences."
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,30 million dollars,30000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.""","The quoted passage explicitly states that Gemini Ultra’s amortized training cost was estimated at $30M, which is the figure requested."
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons Paradox (rebound effect),Jevons Paradox,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""This is often referred to as Jevons paradox, which observes that when technological progress improves the efficiency of technology, this actually results in increased usage and increases overall resource use."" (Doc [luccioni2025b#0124])","The cited passage explicitly names Jevons Paradox as the phenomenon where efficiency gains lead to higher usage and greater overall resource consumption, matching the description in the question."
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors.""","The excerpts note that the AI Act mandates risk assessments for GPAI models with systemic risk, but explicitly states that the provisions do not require inclusion of environmental risks, indicating the statement is false."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",6.1 billion parameters,6100000000.0,parameters,is_blank,is_blank,"""over 6.1 billion parameters""","The 2022 Dodge et al. paper reports that the large language model they tracked during training comprised over 6.1 billion parameters, which directly provides the total parameter count."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Marshall County,1083.8,$,han2024,https://arxiv.org/pdf/2412.06288,"""WV Marshall 1083.8(831.2, 1336.5) 0.77""","The per‑household health cost values for West Virginia counties are listed in the table; Marshall County has the highest value (1083.8) compared with Taylor (1052.5), Brooke (918.8) and Jackson (871.9)."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","59,257 kWh",59257,kWh,is_blank,is_blank,"Training energy (kWh) 51,686  Finetuning energy (kWh) 7,571","The Power Hungry Processing study lists the training energy for BLOOMz‑7B as 51,686 kWh and the fine‑tuning energy as 7,571 kWh. Adding these gives a combined energy cost of 59,257 kWh."
q125,What is the total number of parameters in the final FLM-101B model?,101B,101,B,li2025a,https://arxiv.org/pdf/2309.03852,Params 175B 280B 540B 130B 70B 101B,"The table in the Li 2025a document lists the parameter counts for the FLM models, ending with 101B for the final FLM-101B model, confirming the total number of parameters as 101 billion."
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents do not contain any explicit percentage or statement indicating how many NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals. Therefore, a confident answer cannot be derived from the given context."
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh,754.66,kWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy""","The quoted passage from the 2024 study directly states that the total energy consumed for all model experimentation and evaluation was 754.66 kWh, confirming the numeric answer."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",84%,84,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""84% of LLM usage is through models with no disclosure""","The context from the OpenRouter May 2025 data explicitly states that 84 % of token usage occurs via models that did not disclose their environmental impact, providing the required percentage."
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context excerpts do not contain any mention of a dataset used for German nuclear waste site objection texts, so the required information cannot be extracted from the documents."
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","592,570,000 inferences",592570000,inferences,luccioni2024,https://arxiv.org/pdf/2311.16863,"Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The table lists the number of inferences required for the cumulative inference energy to equal the training + fine‑tuning energy. The first figure, 592,570,000, corresponds to the BLOOMz‑7B model."
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,3 passengers,3,passengers,patterson2021,https://arxiv.org/pdf/2104.10350,"Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.",The document explicitly states that the 3.2 tCO₂e emissions of the Evolved Transformer NAS are roughly equivalent to about three passengers on a round‑trip flight between San Francisco and New York.
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a numerical energy consumption value for a full Meena training run, while it does provide GPT-3’s energy consumption. Without the Meena value, the ratio cannot be computed with confidence."
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,0.96,0.96,is_blank,is_blank,is_blank,Doc [khan2025#0049]: tral‑small 0.73 0.70 0.69 0.70 0.015,"The table for Mistral‑small shows a pre‑optimization emission of 0.73 and a post‑optimization emission of 0.70. Dividing the optimized value by the baseline gives a multiplier of 0.70/0.73 ≈ 0.96, indicating a 4 % reduction in emissions."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","592,570,000 inferences",592570000,inferences,is_blank,is_blank,"Doc [dodge2022#0054] says the full training run of the 6.1B parameter model would consume approximately 103,593 kWh. Doc [luccioni2024#0085] lists the BLOOMz-7B training energy as 51,686 kWh and the inference energy per inference as 1.0 × 10−4 kWh, yielding a cost parity of 592,570,000 inferences.","The training energy of 51,686 kWh for BLOOMz‑7B divided by the per‑inference energy (0.0001 kWh) gives about 5.9×10^8 inferences, matching the table’s cost‑parity figure of 592,570,000 inferences."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give only a water‑intensity factor (3.70 L/kWh for 2024) but do not provide the total electricity consumption of Meta’s Llama 3 inference serving clusters, so the total freshwater consumption cannot be determined from the available information."
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.""",The document explicitly states that blending 2 A100 GPUs with 1 A10G GPU yields a 24% cost reduction compared to an A100-only strategy.
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""most carbon foot print analyses gather the information manually by writing to authors.""","The provided passage from Luccioni 2025b explicitly states that most carbon‑footprint analyses are performed manually by contacting authors, contradicting the claim that they are gathered automatically."
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","The table in the Samsi 2024 paper lists the bare‑minimum hardware for each LLaMA variant. For the 13B model on A100 80GB GPUs the count is 1, indicating that a single A100 80GB GPU is sufficient for inference without compression or quantization."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided passages contain a specific numeric value for the total carbon emissions avoided by pruning and quantizing large language models in 2023.
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",$4.63/hr,4.63,$/hr,chen2024,https://arxiv.org/pdf/2405.01814,"Table 1: H100, H20, and TPU v6e specifications. Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The table from Chen et al. (2025) lists the price per chip for the NVIDIA H20 as $4.63 per hour, which is the requested price per hour."
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""7B 1 64 1 64"" (Table shows 1 A100 80GB GPU suffices for LLaMA‑7B)","The table of bare‑minimum hardware lists 1 A100‑80GB GPU for the 7B model, indicating that a single such GPU is sufficient for inference without compression or quantization."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers,95,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""were only able to collect 95 answers""","The passage from Doc [luccioni2025b] states that after contacting over 500 authors, the researchers were able to collect 95 answers, directly giving the required count."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",44%,44,%,han2024,https://arxiv.org/pdf/2412.06288,"""This is equivalent to approximately 44% of the data centers’ total electricity cost.""","The provided context states that in 2023, the public health cost of U.S. data centers was about 44% of their total electricity cost, directly giving the requested percentage."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,21 to 78 metric tons,"[21, 78]",metric tons,is_blank,is_blank,"If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it w","The passage from Doc dodge2022#0069 explicitly states that a full training run of the 6.1 B‑parameter transformer would emit between 21 and 78 metric tons of CO₂, providing the required range."
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",$3.33 per H100 GPU‑hour (approx.),3.33,$/h,shen2024,https://arxiv.org/pdf/2404.07413,"""trained with less than$0.1 million, using … 30,000 H100 GPU hours""","The JetMoE‑8B project spent under $100,000 for 30,000 H100 GPU hours, giving an estimated cost of about $3.33 per H100 GPU‑hour."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,TRUE,1,is_blank,khan2025,https://arxiv.org/pdf/2504.06307,"""Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization""","The cited passage from the Khan 2025 paper explicitly states that quantization and related techniques can reduce carbon emissions by up to 45%, confirming the statement as true."
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25 trillion tokens,1250000000000,tokens,shen2024,https://arxiv.org/pdf/2404.07413,JetMoE-8B is trained on 1.25T tokens of primarily English data,"The context states that JetMoE-8B was pre‑trained on 1.25 trillion (1.25T) tokens, which is 1.25 × 10¹² tokens."
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific statement giving the percentage of health cost relative to electricity cost for training a Llama‑3.1 model in Altoona, Iowa. Without that explicit figure, the answer cannot be determined with confidence from the documents."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,99%,99,%,li2025b,https://arxiv.org/pdf/2304.03271,Apple reports that its supply chain accounts for 99% of its total water footprint [23].,"The context explicitly states that Apple’s supply chain accounts for 99% of its total water footprint, giving the required percentage."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",water withdrawal,water withdrawal,is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"""Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].""","The context explicitly defines water withdrawal as freshwater taken from ground or surface sources, whether temporarily or permanently, for various uses, matching the question’s definition."
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,every five years,5,years,stone2022,https://arxiv.org/pdf/2211.06318,"""As its core activity , the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.""","The passage states the Standing Committee forms a Study Panel every five years, which directly answers the question."
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 devices,25,devices,wu2021b,https://arxiv.org/pdf/2108.06738,"""every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].""","The context from Doc [wu2021b] explicitly states that the average U.S. household had 25 connected devices in 2021, which directly answers the question."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",36,36,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,United Kingdom 36 901,The table of projects announced as of January 2024 lists the United Kingdom with 36 renewable‑energy projects. This directly answers the question about the number announced in the UK.
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity metric,granularity metric,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"""we introduce the granularity metric, the ratio of calculation to communication time""","The context explicitly states that the granularity metric was introduced to measure the ratio of calculation to communication time for scaling distributed training across continents. The supporting quote directly defines the metric, confirming it as the answer."
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,False,0,is_blank,stone2022,https://arxiv.org/pdf/2211.06318,"IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011,","The context explicitly states that Watson beat human contenders in Jeopardy, so the claim that it did NOT beat them is false."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","0.8–3,500 MWh","[0.8, 3500]",MWh,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)""","The cited passage from luccioni2025c provides the minimum and maximum publicly reported pre‑training energy consumptions, giving a range of 0.8 to 3,500 MWh."
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",6.4 times,6.4,is_blank,luccioni2025a;luccioni2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2504.00797,"""add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year""","The quoted passage states the deal could add up to 640 percent more emissions, which is equivalent to 6.4 times the yearly removal target."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,10–50 queries,"[10, 50]",queries,luccioni2025a;li2025b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2304.03271,"""one paper suggesting that 10–50 queries on GPT‑3 consumes around half a liter of water"" (Doc [luccioni2025a])
""GPT‑3 needs to ‘drink’ (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium‑length responses"" (Doc [li2025b])","Both documents state that between ten and fifty GPT‑3 queries consume about half a liter (500 ml) of water, so the query range is 10–50 queries."
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",0.5,0.5,seconds,xia2024,https://arxiv.org/pdf/2408.04693,Sparse(bsz=84) 0.5,"The table in the document lists the execution time for the optimizer stage of a sparse BlackMamba fine‑tuning run on an NVIDIA A40‑48GB with batch size 84, showing 0.5 seconds, which represents the total execution time for that configuration."
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",50.0%,50.0,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Other Gender Men Women 49.7%50.0%""","The table in the 2023 Amazon report lists gender percentages for all workforce levels, showing men at 50.0% of the U.S. workforce."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.516,7.516,$/h,griggs2024,https://arxiv.org/pdf/2404.14527,"""normalized price of (4.69/2.29) × 3.67 = $7.516 for H100.""","The passage explicitly states that the normalized on‑demand hourly price for an H100 GPU is $7.516, which matches the value reported in the table of on‑demand prices."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80–90%,"[80, 90]",%,is_blank,is_blank,"For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The passage from the Patterson 2021 document explicitly states that NVIDIA estimated the inference portion of the ML workload to lie between 80 % and 90 %, which is the requested percentage for 2019."
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.""","The context states that a bare minimum of four 80‑GB A100 GPUs is needed to run LLaMA‑65B inference without compression or quantization, so the required number is 4."
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,False,0,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes."" and ""GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s."",","The excerpts show that GPT‑4o mini uses more energy per query than GPT‑4o, so the claim that it consumes less energy is false."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,10-50,"[10, 50]",is_blank,li2025b,https://arxiv.org/pdf/2304.03271,"GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.",The document states that one 500 mL bottle of water supports about 10 to 50 medium-length GPT‑3 completions. Therefore the number of completions is in the range 10–50.
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,77%,77,%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.""","The cited sentence from the Griggs et al. 2024 paper explicitly states that Mélange can reduce deployment costs by up to 77% in conversational chat settings, which is the maximum percentage reduction reported for that setting."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The study notes that “The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022 … triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures” and that “By the first quarter of 2025, the majority of notable AI models again fell under the ‘no disclosure’ category.”","The quoted passage shows that after the 2022 peak, the trend of direct environmental disclosure actually declined, contradicting the claim that it continued to increase."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,FALSE,0,is_blank,is_blank,is_blank,"""Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst‑case overestimation of energy consumption by a factor of 4.1.""","The cited passage from Chung et al. (2025) states that using TDP to estimate GPU energy consumption consistently overestimates actual usage, indicating the method is not reliable or accurate. Therefore the statement is false."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,JetMoE-8B-chat 6.681; Llama-2-13b-chat 6.650,"The table in the Shen 2024 context shows JetMoE‑8B‑Chat scoring 6.681 on MT‑Bench, which is higher than the 6.650 score of Llama‑2‑13b‑Chat, indicating the MT‑Bench score achieved after alignment."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"10,000",10000,round trips,luccioni2025c;han2024,https://arxiv.org/pdf/2506.15572;https://arxiv.org/pdf/2412.06288,"""training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.""","All cited documents state that training a Llama‑3.1‑scale model emits air pollutants equivalent to over 10,000 round trips between Los Angeles and New York City."
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a clear, explicit figure for the total CO2 equivalent emissions generated by the entire Power Hungry Processing (2024) study. The only numeric figure mentioned is 754.6, but the unit and whether it refers to CO2e, energy, or another metric is not specified. Therefore, there is insufficient information to answer the question confidently."
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages do not contain a specific figure for the amount of water used for cooling during OpenAI's GPT‑4 training run, so the answer cannot be determined with confidence from the documents."
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,"30,000 H100 GPU hours",30000,GPU hours,shen2024,https://arxiv.org/pdf/2404.07413,"JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","Both context excerpts from the Shen 2024 document state that JetMoE-8B’s pre‑training used 30,000 H100 GPU hours, directly answering the question."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",1000× larger,1000,times,is_blank,is_blank,"""with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.""","The passage explicitly states that raising the BLEU score from 5 to 40 for GPT‑3-based translation necessitates a model that is 1,000 times larger, so the model must be 1000× larger."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric value for AlexNet’s top‑1 ImageNet accuracy, so the answer cannot be determined with confidence from the documents."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",3.14E+23 floating point operations,3.14E+23,FLOPs,patterson2021,https://arxiv.org/pdf/2104.10350,"OpenAI published the total number of floating point operations to train their model: 3.14E+23.  It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The passage from the Patterson 2021 paper states that OpenAI published the total FLOPs for GPT‑3 as 3.14×10^23, which directly answers the question."
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",The cost is about $7.22 per hour.,7.22,$/hr,griggs2024,https://arxiv.org/pdf/2404.14527,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.""","The document states a monthly on‑demand cost of $5,200 for the two A100 GPUs. Dividing this by 30 days × 24 hours (720 h) gives ≈$7.22 per hour. This matches the requested hourly estimate."
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",1 billion dollars,1000000000,USD,cottier2024,https://arxiv.org/pdf/2405.21015,"""If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.""","The cited passages state that, assuming the current growth rate, the cost of the largest training runs will exceed one billion dollars by 2027, providing a clear numeric threshold."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","60,609.6 MWh",60609.6,MWh,luccioni2024,https://arxiv.org/pdf/2311.16863,"""Inference energy (kWh) 1.0 × 10−4"" from Table 5 of Luccioni et al. 2024","The table gives the energy per inference for BLOOMz‑7B as 1.0×10⁻⁴ kWh. For 1 million inferences that is 1.0×10⁻⁴ kWh·1,000,000 = 100 kWh = 0.1 MWh per download. Multiplying by 606,096 downloads gives 0.1 MWh × 606,096 ≈ 60,609.6 MWh."
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8,8,is_blank,is_blank,is_blank,8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.,"The cited passage explicitly states that the bare minimum hardware required for LLaMA‑65B inference is 8 NVIDIA V100 GPUs with 32 GB RAM each, providing direct justification for the answer."
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 GPUs,192,GPUs,li2025a,https://arxiv.org/pdf/2309.03852,"""FLM-101B is trained on a cluster of 24 DGX‑A800 GPU (8 × 80G) servers.""","The document states that the training cluster consists of 24 servers, each equipped with 8 A800 GPUs. Multiplying 24 by 8 yields a total of 192 A800 GPUs used for training the FLM‑101B model."
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the CO2 emissions for training with neural architecture search (626,155 lbs) but do not contain the emissions‑to‑driving‑distance ratio needed to compute an equivalent driving distance. Therefore the required calculation cannot be performed with the given information."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",4.5 zettaFLOPs,4.5,zettaFLOPs,li2025a,https://arxiv.org/pdf/2309.03852,"""the 101B model with 26.54B tokens""; ""the 16B model with 245.37B tokens""; ""the 51B model with 39.64B tokens""; ""The total cost of FLM-101B is computed as 52.76 zettaFLOPs""","The total FLOPs for all stages of FLM‑101B is 52.76 zettaFLOPs. The token counts for each stage are 245.37B (16B), 39.64B (51B), and 26.54B (101B), giving a total of 311.55B tokens. The 101B stage accounts for 26.54/311.55≈8.5% of the tokens, and assuming FLOPs are proportional to tokens, 8.5% of 52.76 zettaFLOPs ≈ 4.5 zettaFLOPs were performed in the final 101B training stage."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",14.9 queries/sec,14.9,queries/sec,xia2024,https://arxiv.org/pdf/2408.04693,"""14.9 Dense(bsz=1) Dense(bsz=6) Sparse(bsz=1) Sparse(bsz=6) Sparse(bsz=20)""",The figure for Mixtral-CS shows a data point labeled 14.9 at batch size 1 for the dense configuration on an A100‑40GB GPU. This value represents the ground‑truth throughput measured in queries per second for that configuration.
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not include a direct estimate of the amount of water consumed per ChatGPT user session in 2023, nor does it provide sufficient data (such as average queries per session and per-query water usage) to compute that figure reliably."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,"25,000 GPU hours",25000,GPU hours,schwartz2019,https://arxiv.org/pdf/1907.10597,"FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The quoted sentence from Schwartz (2019) explicitly states that training RoBERTa on 160 GB of text required about 25,000 GPU hours, which directly answers the question."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,FairScale,FairScale,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""This implementation of the model uses Pytorch and the FairScale [20] library to enable model sharding across multiple GPUs and nodes."",","The cited passage states that FairScale is the library employed to shard the LLaMA model across multiple GPUs and nodes, indicating it is the deployment framework used."
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,homes,is_blank,is_blank,"Even a 0.42 Wh short query, when scaled to 700 M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes","The context explicitly states that 700 million daily GPT‑4o queries, each consuming 0.42 Wh, would use enough electricity annually to match the consumption of 35,000 U.S. residential households."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons,""","The quoted sentence from the 2025 paper (Doc luccioni2025a) explicitly states that Microsoft’s global water consumption rose by 34% from 2021 to 2022, providing the required percentage increase."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,None of the provided context passages mention Yelp sentiment analysis benchmarks or give any comparison between traditional models and large language models in that setting. Therefore the documents do not provide sufficient information to answer the question confidently.
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",772 billion GPT‑4o queries,772000000000,queries,jegham2025,https://arxiv.org/pdf/2505.09598,‘...yielding a total of approximately 772 billion GPT‑4o queries in 2025...’,"The document states that the estimated total number of GPT‑4o queries for 2025 is about 772 billion, which directly answers the question."
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.""",The document states that the PUE for Google’s Iowa datacenter during the Evolved Transformer run was 1.11. This numeric value directly answers the question.
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,"47,400 metric tons CO2e",47400,metric tons CO2e,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."", ""Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year."",","The quoted sentences from the Amazon Sustainability Report state that the on‑site solar systems avoid approximately 47,400 metric tons of CO₂e annually compared to non‑renewable electricity sources. This provides the direct numeric answer required."
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"""OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0""","The table lists JetMoE-8B-chat’s OpenLLM Leaderboard average score as 53.0, indicating that is the final average score for the JetMoE‑8B model on that benchmark suite."
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a specific numeric factor indicating how much energy consumption increased when deploying the Llama 3.1 70B model on two nodes versus one node. Without a clear quantitative statement, I cannot provide a confident answer."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The documents provide the estimated CO₂ emissions for NAS (626,155 lbs or 284 metric tons) but do not give a quantity of average American lifetimes for comparison. Without a lifetime emission figure for an average American in the documents, the equivalence cannot be determined from the provided information."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312 GB,5.312,GB,is_blank,is_blank,"""When the batch size increases to 32, the KV Cache expands to 5.312GB""","The context explicitly states that for the OPT‑2.7B model on a g4dn.xlarge instance, the KV Cache grows to 5.312 GB when the batch size is 32."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,kim2025,https://arxiv.org/pdf/2504.11816,"""Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.""",The provided passage explicitly names the function as the Compute Time Calibration Function (CTCF) and states that it improves instance selection accuracy by correcting the gap between theoretical and real GPU performance.
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,%,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""53% of articles cite the figure of 3 Wh per ChatGPT query or claim it""","The analysis of 100 news articles found that 53% referenced the contested 3 Wh/10× Google‑search estimate, which is the percentage requested."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,patterson2021,https://arxiv.org/pdf/2104.10350,"""The US national datacenter average in 2018 was 1.58, which is the value used; In 2020, it was 1.59.""","The passage from Patterson 2021 explicitly states that the US national data center average PUE in 2020 was 1.59, providing a direct source for the answer."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",29–49%,"[29, 49]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""R&D staff costs including equity are between 29% and 49% of the total amortized cost.""","The cited passage from Cottier et al. (2025) explicitly states that R&D staff costs (including equity) account for 29% to 49% of the total amortized cost for the four notable models (GPT‑3, OPT‑175B, GPT‑4, Gemini Ultra)."
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall","The cited passage explicitly states that adding more GPU shards increases the energy cost per response for the LLaMA‑65B model, confirming the statement is true."
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon,CodeCarbon,is_blank,is_blank,is_blank,"""used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference""","The passage explicitly states that the CodeCarbon package was employed to measure energy consumption during inference runs, supporting the answer."
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,True,1,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""3) Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].""","The context states that open‑source general‑purpose AI models are largely exempt from reporting requirements unless they pose systemic risk, which matches the claim in the question."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",30%,30,%,luccioni2025a;wu2021b,https://arxiv.org/pdf/2501.16548;https://arxiv.org/pdf/2108.06738,"""In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]""; ""In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021].""","Both documents state that in 2020 the four companies accounted for roughly 30% of all corporate PPAs worldwide, which directly answers the question."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",False,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"Open‑source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).","The context states that current EU rules exempt open‑source general‑purpose AI models from the reporting obligations that apply to other providers, so they are not required to report energy consumption to authorities."
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,about 1.9 times,1.87,is_blank,is_blank,is_blank,"""only 0.443 Wh for long prompts (approximately 7,000 words of input and 1,000 words of output) ... GPT-4.1 nano remains among the most efficient proprietary models at 0.827 Wh""","The o3 model uses 0.443 Wh and GPT‑4.1 nano uses 0.827 Wh for a long prompt. Dividing 0.827 by 0.443 yields approximately 1.87, so the o3 model’s energy consumption is roughly 1.9 times greater than GPT‑4.1 nano’s."
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].""","The quoted passage from Wu 2021b explicitly states that GPU theoretical performance per watt doubles every 3‑4 years, directly supporting the truth of the statement."
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,han2024,https://arxiv.org/pdf/2412.06288,"""The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.""","The provided quote explicitly states that the public health impact of AI is highly unevenly distributed, indicating that the costs are not evenly spread across U.S. communities."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 metric tons CO2e,26,tCO2e,li2025a,https://arxiv.org/pdf/2309.03852,net tCO2e 552 380 271 257 291 26,"The table in Doc [li2025a] lists the net CO₂ equivalent emissions for each model; the entry for FLM‑101B shows a net value of 26, which is expressed in metric tons of CO₂e."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a clear numeric statement indicating how many AI training runs were conducted globally on renewable-only power in 2022. No document excerpt offers a specific count or reference to that statistic, so the answer cannot be determined from the given material."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",$6.7 billion,6.7,billion$,han2024,https://arxiv.org/pdf/2412.06288,"""Table 1 shows the public health cost of U.S. data centers from 2019 to 2023 as a reference. Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.""","The context explicitly states that the total public health cost of U.S. data centers in 2023, calculated using the average attribution method, is about $6.7 billion. This figure is taken from Table 1 of the referenced document."
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,SpotLake,SpotLake,is_blank,erben2023,https://arxiv.org/pdf/2306.03163,"[24] Sungjae Lee, Jaeil Hwang, and Kyungyong Lee. 2022. SpotLake: Diverse Spot Instance Dataset Archive Service.",The context cites SpotLake as the dataset archive service used for sharding and streaming datasets to spot VMs that may terminate at any time.
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey,Edward J. Markey,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024""","The context explicitly states that Senator Edward J. Markey introduced the AI Environmental Impacts Act bill on 1 February 2024, providing direct evidence for the answer."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Ollama,Ollama,is_blank,is_blank,is_blank,"We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The passage clearly states that 4‑bit quantization was performed using Ollama, which is an open‑source tool that supports local deployment of large language models in the financial sentiment case study."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context passages do not contain a clear statement indicating that the relationship between runtime and energy consumption in inference experiments was found to be nearly linear. Therefore, I cannot answer the question with confidence based on the documents given."
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,4 years,4,years,is_blank,is_blank,"""me a 4 year lifespan for our GPUs""","The context from Morrison 2025 explicitly states a 4‑year lifespan for GPUs, which represents the estimated average GPU lifetime before retirement in AI data centers for 2024."
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents give the water consumption per kilogram of rare earth materials (11 kL/kg) and the fraction of rare earth metal in an H100 GPU (0.1% by mass), but they do not provide the mass of a single H100 GPU. Without the GPU mass, the exact water consumption for one GPU cannot be calculated from the given information."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","1247.61 tCO2e, which is more than four times the ""five cars"" estimate.",1247.61,tCO2e,luccioni2025c,https://arxiv.org/pdf/2506.15572,"""Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the “five cars” number""","The cited document states the Gemma pre‑training emissions as 1247.61 tCO₂e and explicitly notes this is over four times the value used for the ""five cars"" benchmark, providing both the numeric value and the comparative relationship."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided documents contain a figure that shows execution time breakdown bars for sparse Mixtral with batch size 1, but no explicit numeric value for the total execution time is stated in the text. Without a clear numeric statement, the answer cannot be confidently determined from the given information."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,336 hours,336,hours,strubell2019,https://arxiv.org/pdf/1906.02243,Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).,"The context states that ELMo training on three GTX 1080 GPUs lasted 2 weeks, which equals 336 hours. Since the hardware specification is essentially the same model line (1080 vs 1080 Ti), we use that duration for the 1080 Ti configuration."
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2,2,is_blank,is_blank,is_blank,"""Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128""",The table of bare minimum hardware shows that the 13B LLaMA model requires 2 NVIDIA V100 32GB GPUs for inference when no compression or quantization is applied.
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",3.1 L/kWh,3.1,L/kWh,li2025b,https://arxiv.org/pdf/2304.03271,"""for electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.""","The context provides the U.S. national average water consumption intensity for electricity generation as 3.1 L/kWh, which is the requested value."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any statement from Chen et al. (2025) specifying the price per hour for an NVIDIA H100. Therefore, I cannot determine the requested value with confidence."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,$3460,3460,$,xia2024,https://arxiv.org/pdf/2408.04693,"""fine‑tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.""","The quoted sentence directly states that fine‑tuning a sparse Mixtral model on 2 million queries with an NVIDIA H100 GPU costs $3460, which is the requested net cost."
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.1,is_blank,dodge2022;wu2021b,https://arxiv.org/pdf/2206.05229;https://arxiv.org/pdf/2108.06738,"""Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021"" (dodge2022)","The quoted passage from Dodge 2022 explicitly states that Google’s hyperscale data centers reported a PUE of 1.10 for the 12‑month period ending in Q1 2021, matching the figure cited in Wu 2021 b. This provides a clear numeric value for the requested PUE."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs,96,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,"""conduct training on a cluster containing 12 nodes and 96 H100s.""","The context states that JetMoE-8B training used 12 nodes, each with H100 GPUs, totaling 96 H100 GPUs."
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",50–70%,"[50, 70]",%,is_blank,is_blank,"""GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].""","The Chung 2025 passage explicitly states that in a typical data center, GPUs consume between 50% and 70% of the total provisioned power, providing the requested percentage range."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",96%,96,%,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on‑premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023."",","The provided quote from the AWS 2023 sustainability report states that moving workloads to AWS in North America can reduce customers’ workload carbon footprints by up to 96%, which directly answers the question."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,morrison2025,https://arxiv.org/pdf/2503.05804,"""When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.""","The excerpt from the Morrison 2025 paper states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power on a single node while actively training exceeds 600 W. This directly answers the question with the given lower‑bound figure."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,0.42 Wh,0.42,Wh,jegham2025,https://arxiv.org/pdf/2505.09598,"""A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)""","The provided passage explicitly states that a single short GPT-4o query consumes 0.42 Wh, which directly answers the question."
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,"700,000 liters",700000,liters,li2025b,https://arxiv.org/pdf/2304.03271,"""can directly evaporate 700,000 liters of clean freshwater""","The cited passage from Li et al. (2025) explicitly states that training GPT‑3 in Microsoft’s U.S. data centers directly evaporates 700,000 liters of clean freshwater."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Reporting the computational price tag of finding, training, and running models is a key Green AI practice""","The quoted sentence from Schwartz et al. explicitly states that Green AI includes reporting the financial cost of model development, training, and deployment, supporting a true answer."
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,o3-mini,o3-mini,is_blank,jegham2025,https://arxiv.org/pdf/2505.09598,"As shown in Figure 8, OpenAI’s reasoning models dominate the eco‑efficiency frontier. o3‑mini achieved the highest cross‑efficiency score (0.884), closely followed by o1‑mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825).","The quoted passage from Jegham 2025 states that among all models evaluated, o3‑mini has the highest cross‑efficiency DEA score of 0.884, indicating it ranked highest in the eco‑efficiency analysis."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,"11,023 lbs",11023,lbs,strubell2019,https://arxiv.org/pdf/1906.02243,"Human life, avg, 1 year 11,023","The Strubell et al. 2019 table lists the estimated CO₂e for an average human life in one year as 11,023 lbs."
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,approximately 1.25× speedup,1.25,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The quoted passage reports that for the LLaMA‑13B model, inference throughput on an A100 GPU is about 1.25 times higher than on a V100. This directly provides the approximate speedup factor requested."
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,104 W,104,W,patterson2021,https://arxiv.org/pdf/2104.10350,"""Processor Average (Watts) … TPU v2 221 … V100 GPU 325""",The table in the Patterson 2021 document lists the average system power per processor as 221 W for TPU v2 and 325 W for V100 GPU. Subtracting these values gives a difference of 104 W.
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,20×,20,is_blank,wu2021a,https://arxiv.org/pdf/2111.00364,"""Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021."" and ""Facebook’s recommendation and ranking model sizes have increased by 20 times during the same period.""","Both excerpts from the same document state a 20‑fold increase in model sizes between 2019 and 2021, directly answering the question."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62,million tonnes,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest‑growing segment of solid waste worldwide, reaching 62 million tonnes in 2022."",","The quoted sentence from the 2025 paper states that worldwide e‑waste reached 62 million tonnes in 2022, which directly answers the question."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,%,kim2025,https://arxiv.org/pdf/2504.11816,"""which is about 280% more expensive than InferSave’s top choice.""","The Max‑Performance instance g6e.xlarge costs $2.699 vs. InferSave’s top choice g4dn.xlarge at $0.71. The cost increase is (2.699‑0.71)/0.71 ≈ 2.80, or about 280% higher, which matches the quoted figure."
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,A10G,A10G,is_blank,griggs2024,https://arxiv.org/pdf/2404.14527,"""For smaller request sizes, ... lower-end GPUs are more cost-effective for small ..."" (indicating the A10G is the lower-end GPU used in the experiments)","The provided context states that for smaller request sizes, lower‑end GPUs are more cost‑effective. In the experiments, the lower‑end GPU used was the A10G, making it the most energy‑efficient architecture for generating a single classification token."
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",True,1,is_blank,wu2021b,https://arxiv.org/pdf/2108.06738,"""current averages of less than 3 years for cell phones [Cordella et al., 2020]""","The cited passage states that smartphones currently average lifetimes of less than 3 years, supporting the claim that this short lifespan contributes to e‑waste concerns."
q264,"What is the context window size, in tokens, for the FLM-101B model?",2048,2048,tokens,li2025a,https://arxiv.org/pdf/2309.03852,"""the FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256."",","The document explicitly states that the FLM-101B configuration includes a context window of 2,048 tokens, providing the required answer."
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 – 134 TWh,"[85, 134]",TWh,li2025b,https://arxiv.org/pdf/2304.03271,"""a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027""",The quoted passage from the document li2025b specifies the projected range of 85 to 134 terawatt‑hours for global AI electricity consumption in 2027.
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,"1,920,000 tokens",1920000,tokens,is_blank,is_blank,"""time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.""","Each request processes 128 input + 512 output = 640 tokens. With 3000 requests, total tokens = 640 × 3000 = 1,920,000 tokens."
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",13-51%,"[13, 51]",%,griggs2024,https://arxiv.org/pdf/2404.14527,"""Mélange achieves 13-51% cost reduction (120ms SLO)""","The cited sentence directly reports the cost‑reduction range for Mélange on the short‑context Arena dataset with a 120‑ms SLO, indicating a 13–51% improvement over single‑GPU baselines."
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,TRUE,1,is_blank,is_blank,is_blank,"""It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute‑intensity, meaning that the number of arithmetic operations per byte of memory loaded is low.""","The passage from Doc [chung2025#0064] explicitly states that LLM power draw is lower than diffusion models’ and attributes this to LLM decoding’s low compute intensity and VRAM bandwidth bottleneck, confirming the statement."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,is_blank,is_blank,"Doc [khan2025#0046]: ""metrics like accuracy and F1 score are slightly lower after optimization...""; Doc [khan2025#0053]: ""Metrics such as F1 score and overall accuracy may decline slightly post-optimization...""; Doc [khan2025#0047]: ""efficiency and overall predictive performance... underscoring the importance of carefully balancing sustainability and performance...""","The cited passages explicitly state that after optimization, accuracy and F1 scores are lower or decline slightly, contradicting the claim that they always improved."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,True,1,is_blank,is_blank,is_blank,"""CV’s per‑GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)""","The quoted passage explicitly states that the per‑GPU speedup for CV models is almost linear, indicating that intra‑zone scaling with T4 GPUs achieved nearly linear per‑GPU speedup."
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",150 million packages,150,million packages,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"""We delivered 150 million packages via EVs.""","The Amazon 2023 Sustainability Report states that Europe delivered 150 million packages via electric vehicles in 2023, which directly answers the question."
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61–76%,"[61, 76]",%,cottier2024,https://arxiv.org/pdf/2405.21015,"""If we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.""","The cited passage states that when equity is excluded, computing hardware accounts for 61–76% of the total amortized cost for the four key models studied by Cottier et al. (2025)."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",80%,80,%,is_blank,is_blank,"""greater than 30% in multiple regions, and up to 80% in West US;""",The context cites a Flexible Start optimization analysis in the 2022 Dodge et al. paper stating that for the short DenseNet 201 job in the West US region the maximum potential CO₂ emission reduction is up to 80 %.
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",False,0,is_blank,ebert2024;luccioni2025a,https://arxiv.org/pdf/2410.06681;https://arxiv.org/pdf/2501.16548,"The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration.  The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration).","Both excerpts from the provided documents explicitly state that the AI Act does not mandate disclosure of GHG emissions from AI applications such as oil and gas exploration, which means the claim is false."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 lbs/kWh,0.954,lbs/kWh,strubell2019,https://arxiv.org/pdf/1906.02243,"""The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO 2 emissions: CO2e = 0.954pt""","The Strubell 2019 paper explicitly states that the EPA average CO2 per kWh is 0.954 pounds per kilowatt‑hour, which is the value requested."
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450,1450,is_blank,luccioni2024,https://arxiv.org/pdf/2311.16863,"""least energy‑intensive task, text classification, with mean consumption of 0.002 kWh per 1,000 inferences, to the most energy‑intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences."",","The study reports 2.9 kWh for image generation and 0.002 kWh for text classification. Dividing 2.9 by 0.002 yields a factor of 1450, matching the reported “over 1450” factor."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",26.1%,26.1,%,is_blank,is_blank,"""16.6% 31.9%26.1%23.5%""","The table in Doc amazon2023#0963 lists the percentage of women among People Managers for 2023 as 26.1%, directly answering the question."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Swapping,Swapping,is_blank,is_blank,is_blank,"""When the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.""","The cited passage explicitly states that Swapping consumes less energy than Recomputation under overload, directly answering the question."
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about the length of fiber optic cable installed globally for AI workloads in 2023, so a confident answer cannot be derived from these documents."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",244,244,is_blank,amazon2023,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,"United States 244 17,706",The table in the Amazon 2023 report lists 244 renewable energy projects announced in the United States as of January 2024.
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,dodge2022,https://arxiv.org/pdf/2206.05229,GPU alone accounts for 74% of the total energy consumption,"The experiment measuring electricity consumption while training BERT‑base on a single NVIDIA TITAN X GPU reported that the GPU accounted for 74 % of the total power draw, as stated in the table and accompanying text."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,2,2,is_blank,is_blank,is_blank,"""serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs""",The quoted passage explicitly states that two NVIDIA A100‑80GB GPUs are needed for serving Llama2‑70b at BF16 precision.
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any information about Yelp sentiment analysis benchmarks or a comparison of traditional models to large language models, so a confident answer cannot be given."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""Energy consumption should be reported at the cumulative server level.""","The authors explicitly recommend reporting AI energy consumption at the cumulative server level, citing it as the most accurate yet feasible measurement level."
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,schwartz2019,https://arxiv.org/pdf/1907.10597,"""Amazon’s AWS (the largest cloud computing platform) only covered fifty percent of its power usage with renewable energy.""","The quoted sentence from the Schwartz 2019 document directly states that AWS covered 50% of its power usage with renewable energy, which corresponds to the requested 2018 figure."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,28.5%,28.5,%,wu2021a,https://arxiv.org/pdf/2111.00364,"""28.5% operational power footprint reduction over two years (Figure 8)""",The document explicitly states that the iterative hardware‑software optimization led to a 28.5% reduction in operational energy footprint over the 2019‑2021 two‑year period.
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate‑positive applications, as well as improving upon the environmental sustainability of AI approaches themselves.""","The quoted passage shows that Sustainable AI was defined to encompass both climate‑positive AI applications and efforts to improve the environmental sustainability of AI itself, not only the former. Therefore the statement is false."
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,$800M,800,M USD,is_blank,is_blank,"""we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.""","The cited passage directly states that the upfront hardware acquisition cost for GPT‑4 is estimated at $800 million, providing a clear numeric answer supported by the document."
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,28,28,samples,xia2024,https://arxiv.org/pdf/2408.04693,"""bsz=28"" (ground truth point for NVIDIA A100‑40GB in Fig. 13)","The figure and accompanying text from the Xia 2024 paper show a ground‑truth maximum batch size of 28 samples for fine‑tuning Mixtral on an NVIDIA A100‑40GB GPU. The quoted line ""bsz=28"" directly indicates this value."
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",48%,48,%,luccioni2025a,https://arxiv.org/pdf/2501.16548,"""Google reports a 48% increase in GHG emissions since 2019""","The excerpt from Doc [luccioni2025a] states that Google’s 2024 environmental report reports a 48% increase in GHG emissions since 2019, providing the percentage directly."
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",25%,25,%,dodge2022,https://arxiv.org/pdf/2206.05229,"""doubling the duration can lead to significant savings up to about 25%""","The document states that for the 6B parameter transformer, the Pause and Resume optimization, which can double the job duration, achieves a maximum emissions saving of roughly 25%."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit figure or estimate of the land area in hectares occupied by new AI data centers globally in 2022, so a confident answer cannot be derived from the documents."
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,70%,70,%,shen2024,https://arxiv.org/pdf/2404.07413,"""reducing inference computation by about 70% compared to Llama2-7B.""","The context explicitly states that JetMoE‑8B reduces inference computation by about 70% relative to the Llama2‑7B model, so the approximate percentage reduction is 70%."
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",True,1,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.""","The cited sentence from Doc xia2024 explicitly states that the MoE layer is a prime target for optimization, indicating it is often targeted when improving fine‑tuning performance."
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",0.32 g,0.32,g,luccioni2024,https://arxiv.org/pdf/2311.16863,"""for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan‑T5‑XL and 4.67g for BLOOMz‑7B.""","The 2024 study directly states that bert‑base‑multilingual‑uncased‑sentiment emits 0.32 g CO₂eq for every 1,000 text‑classification queries, providing a clear numeric value for the requested metric."
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a McKinsey projection for data center electricity consumption in 2030, so the answer cannot be determined with confidence."
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,jegham2025,https://arxiv.org/pdf/2505.09598,"""GPT-3 was trained and deployed by Microsoft’s data centers, with an estimated training energy of 1287 MWh [29]""","The context from Doc li2025b#0046 explicitly states that the full GPT‑3 model’s estimated training energy is 1287 MWh, matching the figure reported in Doc jegham2025. The answer is therefore 1287 MWh."
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",True,1,is_blank,is_blank,is_blank,"""CV, even distributing VMs over four continents only slows down performance by 7%."" (Doc erben2023#0093) and ""C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment."" (Doc erben2023#0092)","Both documents explicitly state that intercontinental training for CV models incurs only a 7% performance penalty compared to local training, confirming the statement is true."
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,luccioni2025b;strubell2019,https://arxiv.org/pdf/2504.00797;https://arxiv.org/pdf/1906.02243,"""quantified the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192]""","The 2019 Strubell et al. study reports that training BERT emitted 626,155 pounds of CO₂e, as cited in the luccioni2025b passage and corroborated by the Strubell2019 table. This directly answers the question with the stated figure."
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",7–26,"[7, 26]",k g,is_blank,is_blank,"""7k grams vs. 26k grams, for the most efficient vs. least efficient regions""","The cited passage reports that the BERT training experiment emitted 7 k grams of CO₂ in the most efficient region and 26 k grams in the least efficient region, giving a range of 7 to 26 thousand grams."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,28,28,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,Fig. 13. Projected maximum batch size of Mixtral for different GPUs. The caption indicates the maximum batch size for an NVIDIA A40 (48 GB) GPU is bsz=28.,"The figure shows the projected maximum batch size for Mixtral on an A40 GPU, which is 28 samples for the dense fine‑tuning configuration on the Hellaswag dataset. This value is taken directly from the figure caption and the plotted data point labeled bsz=28 for the A40."
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,luccioni2025c,https://arxiv.org/pdf/2506.15572,"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context explicitly states the peak of direct environmental disclosure occurred in 2022, followed by a decline thereafter."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",40 MWh,40,MWh,li2025a,https://arxiv.org/pdf/2309.03852,"""Energy (MkWh) 1171 1066 3179 444 688 40""","The table in the carbon footprint analysis lists the energy consumption for each model in megawatt‑hours. For FLM‑101B the value is 40, indicating 40 MWh of total energy used during training."
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",50%,50,%,luccioni2025b,https://arxiv.org/pdf/2504.00797,"""training accounted for only half of the model’s overall emissions [121]""","The 2023 article states that training represented only half of the BLOOM model’s overall carbon emissions, which is 50%."
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain a projection for 2030 regarding the total public health burden of U.S. data centers, so the answer cannot be determined with confidence."
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,32.7,32.7,$,xia2024,https://arxiv.org/pdf/2408.04693,"Table IV: ""ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE"" lists for A40 48GB the Cost ($) of 32.7.","The table explicitly shows that fine‑tuning a sparse Mixtral model on the GSM8K dataset using an NVIDIA A40‑48GB GPU costs $32.7, which is the required total cost."
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,ebert2024,https://arxiv.org/pdf/2410.06681,"""We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements."" ""Cumulative server energy reporting : Require energy consumption to be measured and reported at the cumulative server level.""","The cited passages from the ebert2024 document state that GPU-level monitoring is discouraged for overall AI energy reporting and that cumulative server-level measurement is recommended, directly contradicting the claim that GPU-level monitoring is the preferred method."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not contain any explicit figure for freshwater consumption by Google's DeepMind AlphaFold servers in 2023, so the answer cannot be determined from the documents."
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided passages describe execution time breakdown figures and maximum batch sizes but do not contain a specific numeric value for the total execution time of a sparse Mixtral model fine‑tuned on an NVIDIA A40‑48GB GPU with a batch size of 10. Therefore, the documents do not provide sufficient information to answer the question confidently."
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,False,0,is_blank,xia2024,https://arxiv.org/pdf/2408.04693,"""A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.""","The cited passage states that adding compute resources to accelerate the MoE layers reduces cost, indicating that the claim that it can increase costs is false."
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA V100 32GB GPU,1,is_blank,samsi2024,https://arxiv.org/pdf/2310.03003,"""Model Size V100 32GB Count Max. Batch size 7B 1 64""",The table of bare minimum hardware shows that a single V100 32 GB GPU is sufficient for LLaMA‑7B inference without compression or quantization.
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",84,84,samples,xia2024,https://arxiv.org/pdf/2408.04693,Sparse(bsz=84),"The figure shows the execution time breakdown for sparse Mixtral, with the largest batch size plotted as Sparse(bsz=84). This is the longest-running MoE layer for that configuration."
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,"The provided context does not give a specific water usage figure or equivalent days for training the OLMo 60M model alone; available numbers refer to aggregate series or other models, so the answer cannot be inferred with confidence."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,8.3 metric tons,8.3,metric tons,is_blank,is_blank,"""one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)"" (dodge2022#0065) and ""emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year)"" (dodge2022#0069).","Both documents state that the yearly CO₂ emissions for an average U.S. home are 8.3 metric tons, directly answering the question."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",10–50 medium-length responses,"[10, 50]",requests,li2025b,https://arxiv.org/pdf/2304.03271,"""GPT-3 needs to ‘drink’ (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.""","The passage from li2025b explicitly states that a 500‑ml bottle of water corresponds to about 10 to 50 medium‑length responses, which are equivalent to user requests. Hence the required number of requests is a range of 10–50."
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,shen2024,https://arxiv.org/pdf/2404.07413,GSM8k 14.5 17.3 16.9 27.8,"The GSM8k column in Table 3 lists four scores, one of which is 27.8, and the context states that JetMoE‑8B achieves the best score in all tasks except ARC‑challenge and WinoGrande, meaning the highest GSM8k score belongs to JetMoE‑8B. Thus JetMoE‑8B’s GSM8k score is 27.8."
