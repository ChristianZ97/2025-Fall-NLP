{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 105181,
     "databundleVersionId": 13704170,
     "sourceType": "competition"
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "I am sincerely grateful to the classmate whose code provided invaluable reference. If my publicly shared notebook inadvertently causes any issues, I am DEEPLY SORRY for any inconvenience caused. üòî\n\nI became aware of this competition and its associated code through a social chat group.\n\nWith the original author's kind permission, I have referenced code from KoHaKu-Lab(check in leaderboard)(Course Information: TAICA AIA5320).\nCode from https://github.com/KohakuBlueleaf/KohakuRAG",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!nvidia-smi\n!nvcc --version\n!pip install openai rank_bm25 sentence-transformers transformers[torch] faiss-cpu",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "base = \"/kaggle/input\"\n",
    "dirs = os.listdir(base)\n",
    "dirs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "base = \"/kaggle/input/WattBot2025\"\n",
    "\n",
    "\n",
    "def smart_read_csv(path):\n",
    "    encodings = [\"utf-8\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "    raise last_error\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(f\"{base}/train_QA.csv\")\n",
    "test_df = pd.read_csv(f\"{base}/test_Q.csv\")\n",
    "meta_df = smart_read_csv(f\"{base}/metadata.csv\")\n",
    "\n",
    "train_df.head(), test_df.head(), meta_df.head()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def load_metadata(path: Path) -> list[dict[str, str]]:\n",
    "    with path.open(newline=\"\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "\n",
    "def clean_url(url: str) -> str:\n",
    "    return (url or \"\").strip()\n",
    "\n",
    "\n",
    "def is_pdf_url(url: str) -> bool:\n",
    "    cleaned = clean_url(url)\n",
    "    if not cleaned:\n",
    "        return False\n",
    "    parts = urlsplit(cleaned)\n",
    "    base = f\"{parts.scheme}://{parts.netloc}{parts.path}\".rstrip(\"/\")\n",
    "    path_lower = parts.path.lower().rstrip(\"/\")\n",
    "    if base.lower().endswith(\".pdf\"):\n",
    "        return True\n",
    "    # arXiv-style URLs sometimes omit the .pdf suffix but still live under /pdf/.\n",
    "    if parts.netloc.endswith(\"arxiv.org\") and path_lower.startswith(\"/pdf/\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def has_pdf_mime(url: str) -> bool:\n",
    "    try:\n",
    "        resp = requests.head(url, allow_redirects=True, timeout=30)\n",
    "        ctype = resp.headers.get(\"Content-Type\", \"\").lower()\n",
    "        return \"pdf\" in ctype\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_pdf(url: str, dest: Path, *, force: bool = False) -> None:\n",
    "    if dest.exists() and not force:\n",
    "        return\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    headers = {\"User-Agent\": \"KohakuRAG/0.0.1\"}\n",
    "    resp = requests.get(url, headers=headers, timeout=120)\n",
    "    resp.raise_for_status()\n",
    "    dest.write_bytes(resp.content)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Fetch WattBot source PDFs and convert to structured JSON.\"\n",
    ")\n",
    "parser.add_argument(\"--metadata\", type=Path, default=Path(f\"{base}/metadata.csv\"))\n",
    "parser.add_argument(\"--pdf-dir\", type=Path, default=Path(\"artifacts/raw_pdfs\"))\n",
    "parser.add_argument(\"--output-dir\", type=Path, default=Path(\"artifacts/docs\"))\n",
    "parser.add_argument(\"--force-download\", action=\"store_true\")\n",
    "parser.add_argument(\n",
    "    \"--limit\", type=int, default=5, help=\"Process only N documents (for testing).\"\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "# Load document metadata\n",
    "rows = load_metadata(args.metadata)\n",
    "processed = 0\n",
    "skipped = 0\n",
    "\n",
    "# Process each document\n",
    "for row in rows:\n",
    "    doc_id = row[\"id\"]\n",
    "    title = row.get(\"title\") or doc_id\n",
    "    url = clean_url(row.get(\"url\") or \"\")\n",
    "\n",
    "    # Validate URL\n",
    "    if not url:\n",
    "        skipped += 1\n",
    "        print(f\"[skip] {doc_id}: missing URL\", file=sys.stderr)\n",
    "        continue\n",
    "\n",
    "    if not is_pdf_url(url) and not has_pdf_mime(url):\n",
    "        skipped += 1\n",
    "        print(f\"[skip] {doc_id}: URL does not look like PDF ({url})\", file=sys.stderr)\n",
    "        continue\n",
    "\n",
    "    pdf_path = args.pdf_dir / f\"{doc_id}.pdf\"\n",
    "    json_path = args.output_dir / f\"{doc_id}.json\"\n",
    "\n",
    "    try:\n",
    "        # Download PDF\n",
    "        download_pdf(url, pdf_path, force=args.force_download)\n",
    "\n",
    "        # Parse PDF into structured payload\n",
    "        payload = pdf_to_document_payload(\n",
    "            pdf_path,\n",
    "            doc_id=doc_id,\n",
    "            title=title,\n",
    "            metadata={\n",
    "                \"url\": url,\n",
    "                \"type\": row.get(\"type\"),\n",
    "                \"year\": row.get(\"year\"),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Save as JSON\n",
    "        json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        json_path.write_text(\n",
    "            json.dumps(payload_to_dict(payload), ensure_ascii=False),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "        processed += 1\n",
    "        print(f\"[ok] {doc_id} -> {json_path}\")\n",
    "\n",
    "    except requests.HTTPError as err:\n",
    "        skipped += 1\n",
    "        print(f\"[error] {doc_id}: download failed ({err})\", file=sys.stderr)\n",
    "    except Exception as exc:\n",
    "        skipped += 1\n",
    "        print(f\"[error] {doc_id}: conversion failed ({exc})\", file=sys.stderr)\n",
    "\n",
    "    # Respect limit for testing\n",
    "    if args.limit and processed >= args.limit:\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"Processed {processed} documents, skipped {skipped}. \"\n",
    "    f\"Structured docs saved under {args.output_dir}\"\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from pypdf.generic import DictionaryObject, IndirectObject\n",
    "\n",
    "from .text_utils import split_paragraphs, split_sentences\n",
    "from .types import (\n",
    "    DocumentPayload,\n",
    "    ParagraphPayload,\n",
    "    SectionPayload,\n",
    "    SentencePayload,\n",
    ")\n",
    "\n",
    "\n",
    "def _resolve(obj):\n",
    "    return obj.get_object() if isinstance(obj, IndirectObject) else obj\n",
    "\n",
    "\n",
    "def _extract_images(page) -> list[dict[str, Any]]:\n",
    "    images: list[dict[str, Any]] = []\n",
    "    resources = page.get(\"/Resources\")\n",
    "    if not resources:\n",
    "        return images\n",
    "    resources = _resolve(resources)\n",
    "    xobject = (\n",
    "        resources.get(\"/XObject\") if isinstance(resources, DictionaryObject) else None\n",
    "    )\n",
    "    if xobject is None:\n",
    "        return images\n",
    "    xobject = _resolve(xobject)\n",
    "    if not isinstance(xobject, DictionaryObject):\n",
    "        return images\n",
    "    for name, obj in xobject.items():\n",
    "        resolved = _resolve(obj)\n",
    "        if not isinstance(resolved, DictionaryObject):\n",
    "            continue\n",
    "        subtype = resolved.get(\"/Subtype\")\n",
    "        if subtype == \"/Image\":\n",
    "            images.append(\n",
    "                {\n",
    "                    \"name\": str(name),\n",
    "                    \"width\": resolved.get(\"/Width\"),\n",
    "                    \"height\": resolved.get(\"/Height\"),\n",
    "                    \"color_space\": resolved.get(\"/ColorSpace\"),\n",
    "                }\n",
    "            )\n",
    "    return images\n",
    "\n",
    "\n",
    "def pdf_to_document_payload(\n",
    "    pdf_path: Path,\n",
    "    *,\n",
    "    doc_id: str,\n",
    "    title: str,\n",
    "    metadata: dict[str, Any],\n",
    ") -> DocumentPayload:\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    sections: list[SectionPayload] = []\n",
    "    all_paragraph_texts: list[str] = []\n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        raw_text = page.extract_text() or \"\"\n",
    "        paragraphs = []\n",
    "        for paragraph_text in split_paragraphs(raw_text):\n",
    "            sentences = [\n",
    "                SentencePayload(text=sentence)\n",
    "                for sentence in split_sentences(paragraph_text)\n",
    "            ]\n",
    "            paragraphs.append(\n",
    "                ParagraphPayload(\n",
    "                    text=paragraph_text,\n",
    "                    sentences=sentences or None,\n",
    "                    metadata={\"page\": page_num},\n",
    "                )\n",
    "            )\n",
    "            all_paragraph_texts.append(paragraph_text)\n",
    "        images = _extract_images(page)\n",
    "        for idx, info in enumerate(images, start=1):\n",
    "            caption = (\n",
    "                f\"[Image page={page_num} idx={idx}] Placeholder for \"\n",
    "                f\"{info.get('width')}x{info.get('height')} {info.get('color_space')} graphic.\"\n",
    "            )\n",
    "            sentences = [SentencePayload(text=caption)]\n",
    "            paragraphs.append(\n",
    "                ParagraphPayload(\n",
    "                    text=caption,\n",
    "                    sentences=sentences,\n",
    "                    metadata={\n",
    "                        \"page\": page_num,\n",
    "                        \"image_index\": idx,\n",
    "                        \"placeholder\": True,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            all_paragraph_texts.append(caption)\n",
    "        if paragraphs:\n",
    "            sections.append(\n",
    "                SectionPayload(\n",
    "                    title=f\"Page {page_num}\",\n",
    "                    paragraphs=paragraphs,\n",
    "                    metadata={\"page\": page_num},\n",
    "                )\n",
    "            )\n",
    "    combined_text = \"\\n\\n\".join(all_paragraph_texts)\n",
    "    return DocumentPayload(\n",
    "        document_id=doc_id,\n",
    "        title=title,\n",
    "        text=combined_text,\n",
    "        metadata=metadata,\n",
    "        sections=sections,\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_to_markdown(\n",
    "    pdf_path: Path,\n",
    "    *,\n",
    "    doc_id: str,\n",
    "    title: str,\n",
    "    metadata: dict[str, Any],\n",
    ") -> str:\n",
    "    payload = pdf_to_document_payload(\n",
    "        pdf_path, doc_id=doc_id, title=title, metadata=metadata\n",
    "    )\n",
    "    lines = [f\"# {title}\", \"\"]\n",
    "    for section in payload.sections or []:\n",
    "        lines.append(f\"## {section.title}\")\n",
    "        lines.append(\"\")\n",
    "        for paragraph in section.paragraphs:\n",
    "            lines.append(paragraph.text)\n",
    "            lines.append(\"\")\n",
    "    return \"\\n\".join(lines)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Embedding Model (chache)\n",
    "os.environ[\"HF_HUB_ENABLE_HF_CONTRIBUTIONS\"] = \"1\"\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name=\"jinaai/jina-embeddings-v3\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, trust_remote_code=True\n",
    "        )\n",
    "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        # Optional: Move model to GPU if available for faster inference\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.to(\"cuda\")\n",
    "\n",
    "        self.cache_db = Path(\"embed_cache.db\")\n",
    "        self._init_cache()\n",
    "\n",
    "    def _init_cache(self):\n",
    "        conn = sqlite3.connect(self.cache_db)\n",
    "        conn.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS cache (\n",
    "                hash TEXT PRIMARY KEY,\n",
    "                embedding BLOB\n",
    "            )\n",
    "        \"\"\"\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def _hash(self, text: str):\n",
    "        return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "    def embed(self, texts: List[str]):\n",
    "        out = []\n",
    "        conn = sqlite3.connect(self.cache_db)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        for t in texts:\n",
    "            key = self._hash(t)\n",
    "            row = cur.execute(\n",
    "                \"SELECT embedding FROM cache WHERE hash=?\", (key,)\n",
    "            ).fetchone()\n",
    "\n",
    "            if row:  # HIT\n",
    "                emb = pickle.loads(row[0])\n",
    "                out.append(emb)\n",
    "                continue\n",
    "\n",
    "            # MISS ‚Üí compute\n",
    "            # Move input tensors to the same device as the model (CPU or GPU)\n",
    "            tok = self.tokenizer(t, return_tensors=\"pt\", truncation=True)\n",
    "            if torch.cuda.is_available():\n",
    "                tok = {k: v.to(\"cuda\") for k, v in tok.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                h = self.model(**tok).last_hidden_state.mean(dim=1).squeeze()\n",
    "                # Convert to float32 before calling .numpy()\n",
    "                h = h.cpu().to(torch.float32).numpy()\n",
    "\n",
    "            cur.execute(\n",
    "                \"INSERT OR REPLACE INTO cache VALUES (?,?)\", (key, pickle.dumps(h))\n",
    "            )\n",
    "            out.append(h)\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return out\n",
    "\n",
    "\n",
    "class FaissCPUIndex:\n",
    "    def __init__(self, dim=1024, nlist=1, m=8):\n",
    "        \"\"\"\n",
    "        IVF-PQÔºöÈÄÇÁî®‰∫éÂ§ßÈáèÊñáÊ°£Ôºà>1MÔºâÔºåÂ∞èÊï∞ÊçÆÈõÜÊó∂Ëá™Âä®Ë∞ÉÊï¥ nlist\n",
    "        dim: embedding Áª¥Â∫¶\n",
    "        nlist: ËÅöÁ±ªÊï∞Èáè (Â¶ÇÊûúÂ∞è‰∫é 1 ‰ºöËá™Âä®ËÆæ‰∏∫ 1)\n",
    "        m: PQ ÂàÜÊÆµÊï∞ (dim % m == 0)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.nlist = max(1, nlist)\n",
    "        self.m = m\n",
    "\n",
    "        quantizer = faiss.IndexFlatL2(dim)\n",
    "        self.index = faiss.IndexIVFPQ(quantizer, dim, self.nlist, m, 8)\n",
    "\n",
    "        self.is_trained = False\n",
    "        self.doc_texts = []\n",
    "        self.doc_ids = []\n",
    "\n",
    "    def train(self, embeddings: np.ndarray):\n",
    "        if not self.is_trained:\n",
    "            # Faiss IVF ËÆ≠ÁªÉË¶ÅÊ±Ç num_embeddings >= nlist\n",
    "            nlist = min(self.nlist, embeddings.shape[0])\n",
    "            if nlist < self.nlist:\n",
    "                self.index.nlist = nlist\n",
    "            self.index.train(embeddings)\n",
    "            self.is_trained = True\n",
    "\n",
    "    def add(self, embeddings: np.ndarray, doc_ids, raw_texts):\n",
    "        self.train(embeddings)\n",
    "        self.index.add(embeddings)\n",
    "        self.doc_ids.extend(doc_ids)\n",
    "        self.doc_texts.extend(raw_texts)\n",
    "\n",
    "    def search(self, query_emb, topk):\n",
    "        scores, I = self.index.search(query_emb, topk)\n",
    "        return [\n",
    "            (self.doc_ids[i], self.doc_texts[i], float(scores[0][j]))\n",
    "            for j, i in enumerate(I[0])\n",
    "            if i >= 0\n",
    "        ]\n",
    "\n",
    "\n",
    "# BM25 Retriever\n",
    "class BM25Retriever:\n",
    "    def __init__(self, docs: List[str]):\n",
    "        self.docs = docs\n",
    "        self.tok = [d.split() for d in docs]\n",
    "        self.bm25 = BM25Okapi(self.tok)\n",
    "\n",
    "    def search(self, query: str, topk: int):\n",
    "        scores = self.bm25.get_scores(query.split())\n",
    "        idx = scores.argsort()[-topk:][::-1]\n",
    "        return [(i, self.docs[i], float(scores[i])) for i in idx]\n",
    "\n",
    "\n",
    "# Reranker (BGE)\n",
    "class Reranker:\n",
    "    def __init__(self, model_name=\"BAAI/bge-reranker-large\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def rerank(self, query: str, candidates: List[Dict[str, Any]], topk: int):\n",
    "        pairs = [(query, c[\"text\"]) for c in candidates]\n",
    "        tok = self.tokenizer(\n",
    "            [p[0] for p in pairs],\n",
    "            [p[1] for p in pairs],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(**tok).logits.squeeze().numpy()\n",
    "\n",
    "        ranked = sorted(\n",
    "            [\n",
    "                {\"id\": c[\"id\"], \"text\": c[\"text\"], \"score\": float(s)}\n",
    "                for c, s in zip(candidates, scores)\n",
    "            ],\n",
    "            key=lambda x: -x[\"score\"],\n",
    "        )\n",
    "        return ranked[:topk]\n",
    "\n",
    "\n",
    "# LLM Wrapper (OpenAI Compatible)\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, model=\"gpt-4o-mini\", system_prompt=\"\"):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def complete(self, prompt):\n",
    "        msg = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "        return msg.choices[0].message.content\n",
    "\n",
    "\n",
    "# Main RAG Pipeline\n",
    "class RAGPipeline:\n",
    "    def __init__(self, store, embedder, chat_model, bm25=None, reranker=None):\n",
    "        self.store = store  # FaissGPUIndex\n",
    "        self.embedder = embedder  # EmbeddingModel\n",
    "        self.chat = chat_model  # ChatModel\n",
    "        self.bm25 = bm25  # BM25Retriever\n",
    "        self.reranker = reranker  # Reranker\n",
    "\n",
    "    def retrieve(self, question, topk=10):\n",
    "        emb = self.embedder.embed([question])[0]\n",
    "        emb = emb.reshape(1, -1).astype(\"float32\")\n",
    "        # 1) Faiss ANN\n",
    "        ann_hits = self.store.search(emb, topk)\n",
    "        # 2) BM25\n",
    "        bm25_hits = []\n",
    "        if self.bm25:\n",
    "            bm25_hits = self.bm25.search(question, topk)\n",
    "        # ÂêàÂπ∂ + ÂéªÈáç\n",
    "        merged = {}\n",
    "        for did, text, score in ann_hits + bm25_hits:\n",
    "            merged[did] = {\"id\": did, \"text\": text, \"score\": score}\n",
    "        merged_list = list(merged.values())\n",
    "\n",
    "        # 3) Reranker\n",
    "        if self.reranker:\n",
    "            merged_list = self.reranker.rerank(question, merged_list, topk)\n",
    "\n",
    "        return merged_list\n",
    "\n",
    "    def run_qa(self, question, system_prompt, user_template, additional_info, top_k=5):\n",
    "        ctx_docs = self.retrieve(question, top_k)\n",
    "\n",
    "        context = \"\\n\\n\".join([f\"[{c['id']}]\\n{c['text']}\" for c in ctx_docs])\n",
    "\n",
    "        user_prompt = user_template.format(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            additional_info_json=json.dumps(additional_info),\n",
    "        )\n",
    "\n",
    "        raw = self.chat.complete(user_prompt)\n",
    "\n",
    "        try:\n",
    "            structured = json.loads(raw[raw.index(\"{\") : raw.rindex(\"}\") + 1])\n",
    "        except:\n",
    "            structured = {\n",
    "                \"answer\": \"is_blank\",\n",
    "                \"answer_value\": \"is_blank\",\n",
    "                \"ref_id\": \"is_blank\",\n",
    "                \"explanation\": \"is_blank\",\n",
    "            }\n",
    "\n",
    "        # Ê®°Êãü KohakuRAG ÁöÑÊ†áÂáÜËæìÂá∫Ê†ºÂºè\n",
    "        return type(\n",
    "            \"QAResult\",\n",
    "            (),\n",
    "            {\"answer\": structured, \"raw_response\": raw, \"prompt\": user_prompt},\n",
    "        )\n",
    "\n",
    "\n",
    "# pipeline\n",
    "\n",
    "\n",
    "def build_pipeline(docs: List[str], ids: List[str]):\n",
    "    embedder = EmbeddingModel()\n",
    "    embs = embedder.embed(docs)\n",
    "    embs = [e.astype(\"float32\") for e in embs]\n",
    "\n",
    "    embs_np = torch.tensor(embs).numpy()\n",
    "    print(embs_np.shape[1])\n",
    "    faiss_index = FaissCPUIndex(dim=embs_np.shape[1])\n",
    "    faiss_index.add(embs_np, ids, docs)\n",
    "\n",
    "    bm25 = BM25Retriever(docs)\n",
    "    reranker = Reranker()\n",
    "\n",
    "    chat = ChatModel(model=\"gpt-4o-mini\")\n",
    "\n",
    "    pipeline = RAGPipeline(\n",
    "        store=faiss_index,\n",
    "        embedder=embedder,\n",
    "        chat_model=chat,\n",
    "        bm25=bm25,\n",
    "        reranker=reranker,\n",
    "    )\n",
    "    return pipeline"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import faiss\n\nprint(\"FAISS version:\", faiss.__version__)\nprint(\"Has GPU module:\", hasattr(faiss, \"GpuIndexFlatL2\"))\nprint(\"Available attributes:\", [a for a in dir(faiss) if \"Gpu\" in a])",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# buld up these parts and test\n",
    "docs = [\n",
    "    \"Solar panel efficiency depends on irradiance.\",\n",
    "    \"Wind turbines convert kinetic energy to electricity.\",\n",
    "]\n",
    "ids = [\"doc1\", \"doc2\"]\n",
    "\n",
    "pipeline = build_pipeline(docs, ids)\n",
    "qa = pipeline.run_qa(\n",
    "    question=\"What affects solar panel efficiency?\",\n",
    "    system_prompt=\"You are helpful.\",\n",
    "    user_template=\"Q: {question}\\nContext:\\n{context}\\n\\nA:\",\n",
    "    additional_info={\"answer_unit\": \"\"},\n",
    "    top_k=5,\n",
    ")\n",
    "print(qa.answer)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}